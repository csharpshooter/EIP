{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of PersonAttrubutes.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/csharpshooter/EIP/blob/master/A5/Assignment5%20opt1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gyq8CE4ug5BK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# mount gdrive and unzip data\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "!unzip -q \"/content/gdrive/My Drive/EIP/Assignment5Dataset/hvc_data.zip\"\n",
        "# look for `hvc_annotations.csv` file and `resized` dir\n",
        "%ls "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bYbNQzK6kj94",
        "colab_type": "code",
        "outputId": "d0754208-0403-4b04-985a-2ec2c740e725",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "\n",
        "import cv2\n",
        "import json\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from functools import partial\n",
        "from pathlib import Path \n",
        "from tqdm import tqdm\n",
        "\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "\n",
        "\n",
        "from keras.applications import VGG16,VGG19\n",
        "from keras.layers.core import Dropout\n",
        "from keras.layers.core import Flatten\n",
        "from keras.layers.core import Dense\n",
        "from keras.layers import Input, AveragePooling2D, GlobalAveragePooling2D\n",
        "from keras.models import Model\n",
        "from keras.optimizers import SGD\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping\n",
        "from keras.callbacks import ReduceLROnPlateau\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQkbSpLK4sTP",
        "colab_type": "code",
        "outputId": "3d6c74d7-ee77-4740-d8e5-12ea11539a8d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "# load annotations\n",
        "df = pd.read_csv(\"hvc_annotations.csv\")\n",
        "del df[\"filename\"] # remove unwanted column\n",
        "df.head()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>gender</th>\n",
              "      <th>imagequality</th>\n",
              "      <th>age</th>\n",
              "      <th>weight</th>\n",
              "      <th>carryingbag</th>\n",
              "      <th>footwear</th>\n",
              "      <th>emotion</th>\n",
              "      <th>bodypose</th>\n",
              "      <th>image_path</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>male</td>\n",
              "      <td>Average</td>\n",
              "      <td>35-45</td>\n",
              "      <td>normal-healthy</td>\n",
              "      <td>Grocery/Home/Plastic Bag</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>Front-Frontish</td>\n",
              "      <td>resized/1.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>female</td>\n",
              "      <td>Average</td>\n",
              "      <td>35-45</td>\n",
              "      <td>over-weight</td>\n",
              "      <td>None</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Angry/Serious</td>\n",
              "      <td>Front-Frontish</td>\n",
              "      <td>resized/2.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>male</td>\n",
              "      <td>Good</td>\n",
              "      <td>45-55</td>\n",
              "      <td>normal-healthy</td>\n",
              "      <td>Grocery/Home/Plastic Bag</td>\n",
              "      <td>CantSee</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>Front-Frontish</td>\n",
              "      <td>resized/3.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>male</td>\n",
              "      <td>Good</td>\n",
              "      <td>45-55</td>\n",
              "      <td>normal-healthy</td>\n",
              "      <td>Daily/Office/Work Bag</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>Front-Frontish</td>\n",
              "      <td>resized/4.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>female</td>\n",
              "      <td>Good</td>\n",
              "      <td>35-45</td>\n",
              "      <td>slightly-overweight</td>\n",
              "      <td>None</td>\n",
              "      <td>CantSee</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>Front-Frontish</td>\n",
              "      <td>resized/5.jpg</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   gender imagequality    age  ...        emotion        bodypose     image_path\n",
              "0    male      Average  35-45  ...        Neutral  Front-Frontish  resized/1.jpg\n",
              "1  female      Average  35-45  ...  Angry/Serious  Front-Frontish  resized/2.jpg\n",
              "2    male         Good  45-55  ...        Neutral  Front-Frontish  resized/3.jpg\n",
              "3    male         Good  45-55  ...        Neutral  Front-Frontish  resized/4.jpg\n",
              "4  female         Good  35-45  ...        Neutral  Front-Frontish  resized/5.jpg\n",
              "\n",
              "[5 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X_HlH7ktCh3O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_random_eraser(p=0.5, s_l=0.02, s_h=0.4, r_1=0.3, r_2=1/0.3, v_l=0, v_h=255, pixel_level=False):\n",
        "    def eraser(input_img):\n",
        "        img_h, img_w, img_c = input_img.shape\n",
        "        p_1 = np.random.rand()\n",
        "\n",
        "        if p_1 > p:\n",
        "            return input_img\n",
        "\n",
        "        while True:\n",
        "            s = np.random.uniform(s_l, s_h) * img_h * img_w\n",
        "            r = np.random.uniform(r_1, r_2)\n",
        "            w = int(np.sqrt(s / r))\n",
        "            h = int(np.sqrt(s * r))\n",
        "            left = np.random.randint(0, img_w)\n",
        "            top = np.random.randint(0, img_h)\n",
        "\n",
        "            if left + w <= img_w and top + h <= img_h:\n",
        "                break\n",
        "\n",
        "        if pixel_level:\n",
        "            c = np.random.uniform(v_l, v_h, (h, w, img_c))\n",
        "        else:\n",
        "            c = np.random.uniform(v_l, v_h)\n",
        "\n",
        "        input_img[top:top + h, left:left + w, :] = c\n",
        "\n",
        "        return input_img\n",
        "\n",
        "    return eraser"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kg67lTGgCAPw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# print('Using real-time data augmentation.')  \n",
        "# This will do preprocessing and realtime data augmentation:\n",
        "    datagen = ImageDataGenerator(\n",
        "        # set input mean to 0 over the dataset\n",
        "        # featurewise_center=True,\n",
        "        # set each sample mean to 0\n",
        "        # samplewise_center=True,\n",
        "        # divide inputs by std of dataset\n",
        "        # featurewise_std_normalization=True,\n",
        "        # divide each input by its std\n",
        "        # samplewise_std_normalization=True,\n",
        "        # apply ZCA whitening\n",
        "        # zca_whitening=False,\n",
        "        # epsilon for ZCA whitening\n",
        "        # zca_epsilon=1e-06,\n",
        "        # randomly rotate images in the range (deg 0 to 180)\n",
        "        # rotation_range=90,\n",
        "        # randomly shift images horizontally\n",
        "        # width_shift_range=0.1,\n",
        "        # randomly shift images vertically\n",
        "        # height_shift_range=0.1,\n",
        "        # set range for random shear\n",
        "        # shear_range=0.,\n",
        "        # set range for random zoom\n",
        "        # zoom_range=0.,\n",
        "        # set range for random channel shifts\n",
        "        # channel_shift_range=0.,\n",
        "        # set mode for filling points outside the input boundaries\n",
        "        # fill_mode='nearest',\n",
        "        # value used for fill_mode = \"constant\"\n",
        "        # cval=0.,\n",
        "        # randomly flip images\n",
        "        # horizontal_flip=True,\n",
        "        # randomly flip images\n",
        "        # vertical_flip=True,\n",
        "        # set rescaling factor (applied before any other transformation)\n",
        "        # rescale=None,\n",
        "        # set function that will be applied on each input\n",
        "        preprocessing_function=get_random_eraser(v_l=0, v_h=1, pixel_level=False),\n",
        "        # image data format, either \"channels_first\" or \"channels_last\"\n",
        "        #data_format=None,\n",
        "        # fraction of images reserved for validation (strictly between 0 and 1)\n",
        "        #validation_split=0.15\n",
        "        )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "202OJva345WA",
        "colab_type": "code",
        "outputId": "5f7cb878-b02a-4771-f43f-6194734eb0f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 917
        }
      },
      "source": [
        "# one hot encoding of labels\n",
        "\n",
        "one_hot_df = pd.concat([\n",
        "    df[[\"image_path\"]],\n",
        "    pd.get_dummies(df.gender, prefix=\"gender\"),\n",
        "    pd.get_dummies(df.imagequality, prefix=\"imagequality\"),\n",
        "    pd.get_dummies(df.age, prefix=\"age\"),\n",
        "    pd.get_dummies(df.weight, prefix=\"weight\"),\n",
        "    pd.get_dummies(df.carryingbag, prefix=\"carryingbag\"),  \n",
        "    pd.get_dummies(df.footwear, prefix=\"footwear\"),\n",
        "    pd.get_dummies(df.emotion, prefix=\"emotion\"),\n",
        "    pd.get_dummies(df.bodypose, prefix=\"bodypose\"),\n",
        "], axis = 1)\n",
        "\n",
        "one_hot_df.head().T"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>image_path</th>\n",
              "      <td>resized/1.jpg</td>\n",
              "      <td>resized/2.jpg</td>\n",
              "      <td>resized/3.jpg</td>\n",
              "      <td>resized/4.jpg</td>\n",
              "      <td>resized/5.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gender_female</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gender_male</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>imagequality_Average</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>imagequality_Bad</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>imagequality_Good</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>age_15-25</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>age_25-35</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>age_35-45</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>age_45-55</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>age_55+</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>weight_normal-healthy</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>weight_over-weight</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>weight_slightly-overweight</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>weight_underweight</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>carryingbag_Daily/Office/Work Bag</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>carryingbag_Grocery/Home/Plastic Bag</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>carryingbag_None</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>footwear_CantSee</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>footwear_Fancy</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>footwear_Normal</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>emotion_Angry/Serious</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>emotion_Happy</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>emotion_Neutral</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>emotion_Sad</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>bodypose_Back</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>bodypose_Front-Frontish</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>bodypose_Side</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  0  ...              4\n",
              "image_path                            resized/1.jpg  ...  resized/5.jpg\n",
              "gender_female                                     0  ...              1\n",
              "gender_male                                       1  ...              0\n",
              "imagequality_Average                              1  ...              0\n",
              "imagequality_Bad                                  0  ...              0\n",
              "imagequality_Good                                 0  ...              1\n",
              "age_15-25                                         0  ...              0\n",
              "age_25-35                                         0  ...              0\n",
              "age_35-45                                         1  ...              1\n",
              "age_45-55                                         0  ...              0\n",
              "age_55+                                           0  ...              0\n",
              "weight_normal-healthy                             1  ...              0\n",
              "weight_over-weight                                0  ...              0\n",
              "weight_slightly-overweight                        0  ...              1\n",
              "weight_underweight                                0  ...              0\n",
              "carryingbag_Daily/Office/Work Bag                 0  ...              0\n",
              "carryingbag_Grocery/Home/Plastic Bag              1  ...              0\n",
              "carryingbag_None                                  0  ...              1\n",
              "footwear_CantSee                                  0  ...              1\n",
              "footwear_Fancy                                    0  ...              0\n",
              "footwear_Normal                                   1  ...              0\n",
              "emotion_Angry/Serious                             0  ...              0\n",
              "emotion_Happy                                     0  ...              0\n",
              "emotion_Neutral                                   1  ...              1\n",
              "emotion_Sad                                       0  ...              0\n",
              "bodypose_Back                                     0  ...              0\n",
              "bodypose_Front-Frontish                           1  ...              1\n",
              "bodypose_Side                                     0  ...              0\n",
              "\n",
              "[28 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ll94zTv6w5i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras\n",
        "import numpy as np\n",
        "\n",
        "# Label columns per attribute\n",
        "_gender_cols_ = [col for col in one_hot_df.columns if col.startswith(\"gender\")]\n",
        "_imagequality_cols_ = [col for col in one_hot_df.columns if col.startswith(\"imagequality\")]\n",
        "_age_cols_ = [col for col in one_hot_df.columns if col.startswith(\"age\")]\n",
        "_weight_cols_ = [col for col in one_hot_df.columns if col.startswith(\"weight\")]\n",
        "_carryingbag_cols_ = [col for col in one_hot_df.columns if col.startswith(\"carryingbag\")]\n",
        "_footwear_cols_ = [col for col in one_hot_df.columns if col.startswith(\"footwear\")]\n",
        "_emotion_cols_ = [col for col in one_hot_df.columns if col.startswith(\"emotion\")]\n",
        "_bodypose_cols_ = [col for col in one_hot_df.columns if col.startswith(\"bodypose\")]\n",
        "\n",
        "class PersonDataGenerator(keras.utils.Sequence):\n",
        "    \"\"\"Ground truth data generator\"\"\"\n",
        "\n",
        "    \n",
        "    def __init__(self, df, batch_size=32, shuffle=True, augmentation=None):\n",
        "        self.df = df\n",
        "        self.batch_size=batch_size\n",
        "        self.shuffle = shuffle\n",
        "        self.on_epoch_end()\n",
        "        self.augmentation = augmentation\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(np.floor(self.df.shape[0] / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"fetch batched images and targets\"\"\"\n",
        "        batch_slice = slice(index * self.batch_size, (index + 1) * self.batch_size)\n",
        "        items = self.df.iloc[batch_slice]\n",
        "        image = np.stack([cv2.imread(item[\"image_path\"]) for _, item in items.iterrows()])\n",
        "\n",
        "        if self.augmentation is not None:\n",
        "          # self.augmentation.fit(image)\n",
        "          image = self.augmentation.flow(image,shuffle=False).next()\n",
        "\n",
        "        target = {\n",
        "            \"gender_output\": items[_gender_cols_].values,\n",
        "            \"image_quality_output\": items[_imagequality_cols_].values,\n",
        "            \"age_output\": items[_age_cols_].values,\n",
        "            \"weight_output\": items[_weight_cols_].values,\n",
        "            \"bag_output\": items[_carryingbag_cols_].values,\n",
        "            \"pose_output\": items[_bodypose_cols_].values,\n",
        "            \"footwear_output\": items[_footwear_cols_].values,\n",
        "            \"emotion_output\": items[_emotion_cols_].values,\n",
        "        }\n",
        "\n",
        "        return image, target\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        \"\"\"Updates indexes after each epoch\"\"\"\n",
        "        if self.shuffle == True:\n",
        "            self.df = self.df.sample(frac=1).reset_index(drop=True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yVE8-OaZ8J5q",
        "colab_type": "code",
        "outputId": "00d81155-beda-4d03-d9ff-9366222993b9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train_df, val_df = train_test_split(one_hot_df, test_size=0.15, random_state=15)\n",
        "train_df.shape, val_df.shape"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((11537, 28), (2036, 28))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K5m15DLyF2ot",
        "colab_type": "code",
        "outputId": "db056fac-9a38-48a6-c900-1837d58c62af",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        }
      },
      "source": [
        "train_df.head()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image_path</th>\n",
              "      <th>gender_female</th>\n",
              "      <th>gender_male</th>\n",
              "      <th>imagequality_Average</th>\n",
              "      <th>imagequality_Bad</th>\n",
              "      <th>imagequality_Good</th>\n",
              "      <th>age_15-25</th>\n",
              "      <th>age_25-35</th>\n",
              "      <th>age_35-45</th>\n",
              "      <th>age_45-55</th>\n",
              "      <th>age_55+</th>\n",
              "      <th>weight_normal-healthy</th>\n",
              "      <th>weight_over-weight</th>\n",
              "      <th>weight_slightly-overweight</th>\n",
              "      <th>weight_underweight</th>\n",
              "      <th>carryingbag_Daily/Office/Work Bag</th>\n",
              "      <th>carryingbag_Grocery/Home/Plastic Bag</th>\n",
              "      <th>carryingbag_None</th>\n",
              "      <th>footwear_CantSee</th>\n",
              "      <th>footwear_Fancy</th>\n",
              "      <th>footwear_Normal</th>\n",
              "      <th>emotion_Angry/Serious</th>\n",
              "      <th>emotion_Happy</th>\n",
              "      <th>emotion_Neutral</th>\n",
              "      <th>emotion_Sad</th>\n",
              "      <th>bodypose_Back</th>\n",
              "      <th>bodypose_Front-Frontish</th>\n",
              "      <th>bodypose_Side</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3968</th>\n",
              "      <td>resized/3969.jpg</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11444</th>\n",
              "      <td>resized/11446.jpg</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1197</th>\n",
              "      <td>resized/1198.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11569</th>\n",
              "      <td>resized/11571.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9626</th>\n",
              "      <td>resized/9627.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              image_path  gender_female  ...  bodypose_Front-Frontish  bodypose_Side\n",
              "3968    resized/3969.jpg              1  ...                        1              0\n",
              "11444  resized/11446.jpg              1  ...                        0              0\n",
              "1197    resized/1198.jpg              0  ...                        1              0\n",
              "11569  resized/11571.jpg              0  ...                        1              0\n",
              "9626    resized/9627.jpg              0  ...                        0              0\n",
              "\n",
              "[5 rows x 28 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oTiOi5tVBnhS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create train and validation data generators\n",
        "train_gen = PersonDataGenerator(train_df, batch_size=32, shuffle=False, augmentation=datagen)\n",
        "valid_gen = PersonDataGenerator(val_df, batch_size=32, shuffle=False, augmentation=None)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2pMDGat-Ghow",
        "colab_type": "code",
        "outputId": "4a1ebbf7-0fd6-48ef-88cf-468bdaad4a8a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 161
        }
      },
      "source": [
        "# get number of output units from data\n",
        "images, targets = next(iter(train_gen))\n",
        "num_units = { k.split(\"_output\")[0]:v.shape[1] for k, v in targets.items()}\n",
        "num_units"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'age': 5,\n",
              " 'bag': 3,\n",
              " 'emotion': 4,\n",
              " 'footwear': 3,\n",
              " 'gender': 2,\n",
              " 'image_quality': 3,\n",
              " 'pose': 3,\n",
              " 'weight': 4}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "03W8Pagg_Ppp",
        "colab_type": "code",
        "outputId": "46e0a17c-8d70-4cd7-e011-5cf690ba0085",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 251
        }
      },
      "source": [
        "backbone = VGG16(\n",
        "    weights=None, \n",
        "    include_top=False, \n",
        "    input_tensor=Input(shape=(224, 224, 3))\n",
        ")\n",
        "\n",
        "neck = backbone.output\n",
        "neck = Flatten(name=\"flatten\")(neck)\n",
        "neck = Dense(512, activation=\"relu\")(neck)\n",
        "\n",
        "\n",
        "def build_tower(in_layer):\n",
        "    neck = Dropout(0.2)(in_layer)\n",
        "    neck = Dense(128, activation=\"relu\")(neck)\n",
        "    neck = Dropout(0.3)(neck)\n",
        "    neck = Dense(128, activation=\"relu\")(neck)\n",
        "    return neck\n",
        "\n",
        "\n",
        "# def build_tower_emotion(in_layer):\n",
        "#     neck = Dropout(0.4)(in_layer)\n",
        "#     neck = Dense(256, activation=\"relu\")(neck)\n",
        "#     neck = Dropout(0.4)(neck)\n",
        "#     neck = Dense(256, activation=\"relu\")(neck)\n",
        "#     return neck\n",
        "\n",
        "def build_head(name, in_layer):\n",
        "    return Dense(\n",
        "        num_units[name], activation=\"softmax\", name=f\"{name}_output\"\n",
        "    )(in_layer)\n",
        "\n",
        "# heads\n",
        "gender = build_head(\"gender\", build_tower(neck))\n",
        "image_quality = build_head(\"image_quality\", build_tower(neck))\n",
        "age = build_head(\"age\", build_tower(neck))\n",
        "weight = build_head(\"weight\", build_tower(neck))\n",
        "bag = build_head(\"bag\", build_tower(neck))\n",
        "footwear = build_head(\"footwear\", build_tower(neck))\n",
        "emotion = build_head(\"emotion\", build_tower(neck))\n",
        "pose = build_head(\"pose\", build_tower(neck))\n",
        "\n",
        "\n",
        "model = Model(\n",
        "    inputs=backbone.input, \n",
        "    outputs=[gender, image_quality, age, weight, bag, footwear, pose, emotion]\n",
        ")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SxWVxcbi_y6V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def lr_schedule(epoch):\n",
        "    \"\"\"Learning Rate Schedule\n",
        "\n",
        "    Learning rate is scheduled to be reduced after 80, 120, 160, 180 epochs.\n",
        "    Called automatically every epoch as part of callbacks during training.\n",
        "\n",
        "    # Arguments\n",
        "        epoch (int): The number of epochs\n",
        "\n",
        "    # Returns\n",
        "        lr (float32): learning rate\n",
        "    \"\"\"\n",
        "    lr = 1e-3\n",
        "    if epoch > 17:\n",
        "        lr *= 0.5e-3\n",
        "    elif epoch > 13:\n",
        "        lr *= 1e-3\n",
        "    elif epoch > 9:\n",
        "        lr *= 1e-2\n",
        "    elif epoch > 5:\n",
        "        lr *= 1e-1\n",
        "    print('Learning rate: ', lr)\n",
        "    return lr\n",
        "\n",
        "    # lr = 0.5\n",
        "    # if epoch > 45:\n",
        "    #     lr *= 0.1\n",
        "    # elif epoch > 40:\n",
        "    #     lr *= 0.2\n",
        "    # elif epoch > 30:\n",
        "    #     lr *= 0.3\n",
        "    # elif epoch > 20:\n",
        "    #     lr *= 0.4\n",
        "    # print('Learning rate: ', lr)\n",
        "    # return lr"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RfPG9C2eA1zn",
        "colab_type": "code",
        "outputId": "c0d4bcad-6188-479a-8152-3b98271b57d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        }
      },
      "source": [
        "losses = {\n",
        " \t\"gender_output\": \"binary_crossentropy\",\n",
        " \t\"image_quality_output\": \"categorical_crossentropy\",\n",
        " \t\"age_output\": \"categorical_crossentropy\",\n",
        " \t\"weight_output\": \"categorical_crossentropy\",\n",
        "  \"bag_output\": \"categorical_crossentropy\",\n",
        " \t\"footwear_output\": \"categorical_crossentropy\",\n",
        " \t\"pose_output\": \"categorical_crossentropy\",\n",
        "  \"emotion_output\": \"categorical_crossentropy\",\n",
        " }\n",
        "\n",
        "loss_weights = {\"gender_output\": 1.0, \"image_quality_output\": 1.0, \"age_output\": 1.0,\"weight_output\": 1.0,\"bag_output\": 1.0,\"footwear_output\": 1.0,\"pose_output\": 1.0,\"emotion_output\": 1.0}\n",
        "\n",
        "opt = SGD(lr=0.001, momentum=0.9)\n",
        "model.compile(\n",
        "    optimizer=opt,\n",
        "    loss=losses, \n",
        "    loss_weights=loss_weights, \n",
        "    metrics=[\"accuracy\"]\n",
        ")"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3657: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zw2ZRIQ7BW-Q",
        "colab_type": "code",
        "outputId": "3f98ae91-a031-42e6-b2fc-0aa4944ef8b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# model.fit(X_train, y_train, validation_data=(X_valid, y_valid), batch_size=32, epochs=10)\n",
        "model.summary()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 224, 224, 3)  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "block1_conv1 (Conv2D)           (None, 224, 224, 64) 1792        input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "block1_conv2 (Conv2D)           (None, 224, 224, 64) 36928       block1_conv1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block1_pool (MaxPooling2D)      (None, 112, 112, 64) 0           block1_conv2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block2_conv1 (Conv2D)           (None, 112, 112, 128 73856       block1_pool[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "block2_conv2 (Conv2D)           (None, 112, 112, 128 147584      block2_conv1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block2_pool (MaxPooling2D)      (None, 56, 56, 128)  0           block2_conv2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block3_conv1 (Conv2D)           (None, 56, 56, 256)  295168      block2_pool[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "block3_conv2 (Conv2D)           (None, 56, 56, 256)  590080      block3_conv1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block3_conv3 (Conv2D)           (None, 56, 56, 256)  590080      block3_conv2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block3_pool (MaxPooling2D)      (None, 28, 28, 256)  0           block3_conv3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block4_conv1 (Conv2D)           (None, 28, 28, 512)  1180160     block3_pool[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "block4_conv2 (Conv2D)           (None, 28, 28, 512)  2359808     block4_conv1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block4_conv3 (Conv2D)           (None, 28, 28, 512)  2359808     block4_conv2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block4_pool (MaxPooling2D)      (None, 14, 14, 512)  0           block4_conv3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block5_conv1 (Conv2D)           (None, 14, 14, 512)  2359808     block4_pool[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "block5_conv2 (Conv2D)           (None, 14, 14, 512)  2359808     block5_conv1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block5_conv3 (Conv2D)           (None, 14, 14, 512)  2359808     block5_conv2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block5_pool (MaxPooling2D)      (None, 7, 7, 512)    0           block5_conv3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "flatten (Flatten)               (None, 25088)        0           block5_pool[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 512)          12845568    flatten[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 512)          0           dense_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_3 (Dropout)             (None, 512)          0           dense_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_5 (Dropout)             (None, 512)          0           dense_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_7 (Dropout)             (None, 512)          0           dense_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_9 (Dropout)             (None, 512)          0           dense_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_11 (Dropout)            (None, 512)          0           dense_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_15 (Dropout)            (None, 512)          0           dense_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_13 (Dropout)            (None, 512)          0           dense_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 128)          65664       dropout_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_4 (Dense)                 (None, 128)          65664       dropout_3[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_6 (Dense)                 (None, 128)          65664       dropout_5[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_8 (Dense)                 (None, 128)          65664       dropout_7[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_10 (Dense)                (None, 128)          65664       dropout_9[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_12 (Dense)                (None, 128)          65664       dropout_11[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dense_16 (Dense)                (None, 128)          65664       dropout_15[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dense_14 (Dense)                (None, 128)          65664       dropout_13[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_2 (Dropout)             (None, 128)          0           dense_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_4 (Dropout)             (None, 128)          0           dense_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_6 (Dropout)             (None, 128)          0           dense_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_8 (Dropout)             (None, 128)          0           dense_8[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_10 (Dropout)            (None, 128)          0           dense_10[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_12 (Dropout)            (None, 128)          0           dense_12[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_16 (Dropout)            (None, 128)          0           dense_16[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_14 (Dropout)            (None, 128)          0           dense_14[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_3 (Dense)                 (None, 128)          16512       dropout_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_5 (Dense)                 (None, 128)          16512       dropout_4[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_7 (Dense)                 (None, 128)          16512       dropout_6[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_9 (Dense)                 (None, 128)          16512       dropout_8[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_11 (Dense)                (None, 128)          16512       dropout_10[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dense_13 (Dense)                (None, 128)          16512       dropout_12[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dense_17 (Dense)                (None, 128)          16512       dropout_16[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dense_15 (Dense)                (None, 128)          16512       dropout_14[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "gender_output (Dense)           (None, 2)            258         dense_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "image_quality_output (Dense)    (None, 3)            387         dense_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "age_output (Dense)              (None, 5)            645         dense_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "weight_output (Dense)           (None, 4)            516         dense_9[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "bag_output (Dense)              (None, 3)            387         dense_11[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "footwear_output (Dense)         (None, 3)            387         dense_13[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "pose_output (Dense)             (None, 3)            387         dense_17[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "emotion_output (Dense)          (None, 4)            516         dense_15[0][0]                   \n",
            "==================================================================================================\n",
            "Total params: 28,221,147\n",
            "Trainable params: 28,221,147\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sxkrEvAUDS-o",
        "colab_type": "code",
        "outputId": "4a83b5d0-6b88-4f36-cfc1-698bb3042f4e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "import os\n",
        "import datetime\n",
        "# Prepare model model saving directory.\n",
        "# save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
        "t = datetime.datetime.today()\n",
        "print(str(datetime.datetime.today()))\n",
        "save_dir = \"/content/gdrive/My Drive/EIP/Assignment5Dataset/saved_models/\"+str(t)\n",
        "model_name = 'A5_model.{epoch:03d}.h5'\n",
        "if not os.path.isdir(save_dir):\n",
        "    os.makedirs(save_dir)\n",
        "filepath = os.path.join(save_dir, model_name)\n",
        "filepath"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-12-29 10:35:46.773873\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/gdrive/My Drive/EIP/Assignment5Dataset/saved_models/2019-12-29 10:35:46.773811/A5_model.{epoch:03d}.h5'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I3lvRFJBDM5Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Prepare callbacks for model saving and for learning rate adjustment.\n",
        "checkpoint = ModelCheckpoint(filepath=filepath,\n",
        "                             monitor='val_loss',\n",
        "                             verbose=1,\n",
        "                             save_best_only=True)\n",
        "\n",
        "\n",
        "es = EarlyStopping(monitor='val_loss', verbose=1, patience=5, mode='min')\n",
        "\n",
        "lr_scheduler = LearningRateScheduler(lr_schedule, verbose=1)\n",
        "\n",
        "# lr_reducer = ReduceLROnPlateau(factor=np.sqrt(0.1),\n",
        "#                                cooldown=0,\n",
        "#                                patience=5,\n",
        "#                                min_lr=0.5e-6)\n",
        "\n",
        "# callbacks = [checkpoint, lr_reducer, lr_scheduler]\n",
        "callbacks = [checkpoint,es,lr_scheduler]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rpxv41EyNmN4",
        "colab_type": "code",
        "outputId": "b5780d6d-56df-4d51-d47e-2815d4ede304",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model_output = model.fit_generator(\n",
        "    generator=train_gen,\n",
        "    validation_data=valid_gen,\n",
        "    use_multiprocessing=True,\n",
        "    workers=6, \n",
        "    epochs=50,\n",
        "    verbose=1,\n",
        "    callbacks=callbacks\n",
        ")"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Epoch 1/50\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "Learning rate:  0.001\n",
            "\n",
            "Epoch 00001: LearningRateScheduler setting learning rate to 0.001.\n",
            "360/360 [==============================] - 112s 312ms/step - loss: 7.9796 - gender_output_loss: 0.6897 - image_quality_output_loss: 0.9959 - age_output_loss: 1.4504 - weight_output_loss: 1.0159 - bag_output_loss: 0.9426 - footwear_output_loss: 1.0052 - pose_output_loss: 0.9400 - emotion_output_loss: 0.9398 - gender_output_acc: 0.5497 - image_quality_output_acc: 0.5452 - age_output_acc: 0.3900 - weight_output_acc: 0.6312 - bag_output_acc: 0.5495 - footwear_output_acc: 0.5153 - pose_output_acc: 0.6163 - emotion_output_acc: 0.7114 - val_loss: 7.7982 - val_gender_output_loss: 0.6824 - val_image_quality_output_loss: 0.9603 - val_age_output_loss: 1.4320 - val_weight_output_loss: 0.9609 - val_bag_output_loss: 0.9179 - val_footwear_output_loss: 0.9681 - val_pose_output_loss: 0.9386 - val_emotion_output_loss: 0.9381 - val_gender_output_acc: 0.5680 - val_image_quality_output_acc: 0.5714 - val_age_output_acc: 0.3914 - val_weight_output_acc: 0.6533 - val_bag_output_acc: 0.5724 - val_footwear_output_acc: 0.5402 - val_pose_output_acc: 0.6091 - val_emotion_output_acc: 0.6994\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 7.79821, saving model to /content/gdrive/My Drive/EIP/Assignment5Dataset/saved_models/2019-12-29 10:35:46.773811/A5_model.001.h5\n",
            "Epoch 2/50\n",
            "Learning rate:  0.001\n",
            "\n",
            "Epoch 00002: LearningRateScheduler setting learning rate to 0.001.\n",
            "360/360 [==============================] - 104s 289ms/step - loss: 7.8373 - gender_output_loss: 0.6863 - image_quality_output_loss: 0.9857 - age_output_loss: 1.4331 - weight_output_loss: 0.9944 - bag_output_loss: 0.9248 - footwear_output_loss: 0.9657 - pose_output_loss: 0.9297 - emotion_output_loss: 0.9175 - gender_output_acc: 0.5581 - image_quality_output_acc: 0.5503 - age_output_acc: 0.3998 - weight_output_acc: 0.6326 - bag_output_acc: 0.5615 - footwear_output_acc: 0.5497 - pose_output_acc: 0.6193 - emotion_output_acc: 0.7137 - val_loss: 7.7661 - val_gender_output_loss: 0.6841 - val_image_quality_output_loss: 0.9607 - val_age_output_loss: 1.4317 - val_weight_output_loss: 0.9600 - val_bag_output_loss: 0.9176 - val_footwear_output_loss: 0.9452 - val_pose_output_loss: 0.9342 - val_emotion_output_loss: 0.9326 - val_gender_output_acc: 0.5680 - val_image_quality_output_acc: 0.5714 - val_age_output_acc: 0.3914 - val_weight_output_acc: 0.6533 - val_bag_output_acc: 0.5724 - val_footwear_output_acc: 0.5615 - val_pose_output_acc: 0.6091 - val_emotion_output_acc: 0.6994\n",
            "\n",
            "Epoch 00002: val_loss improved from 7.79821 to 7.76612, saving model to /content/gdrive/My Drive/EIP/Assignment5Dataset/saved_models/2019-12-29 10:35:46.773811/A5_model.002.h5\n",
            "Epoch 3/50\n",
            "Learning rate:  0.001\n",
            "\n",
            "Epoch 00003: LearningRateScheduler setting learning rate to 0.001.\n",
            "360/360 [==============================] - 104s 288ms/step - loss: 7.7777 - gender_output_loss: 0.6798 - image_quality_output_loss: 0.9839 - age_output_loss: 1.4263 - weight_output_loss: 0.9891 - bag_output_loss: 0.9189 - footwear_output_loss: 0.9443 - pose_output_loss: 0.9245 - emotion_output_loss: 0.9109 - gender_output_acc: 0.5640 - image_quality_output_acc: 0.5506 - age_output_acc: 0.4006 - weight_output_acc: 0.6326 - bag_output_acc: 0.5612 - footwear_output_acc: 0.5620 - pose_output_acc: 0.6193 - emotion_output_acc: 0.7137 - val_loss: 7.6507 - val_gender_output_loss: 0.6518 - val_image_quality_output_loss: 0.9557 - val_age_output_loss: 1.4180 - val_weight_output_loss: 0.9523 - val_bag_output_loss: 0.9041 - val_footwear_output_loss: 0.9051 - val_pose_output_loss: 0.9268 - val_emotion_output_loss: 0.9367 - val_gender_output_acc: 0.6071 - val_image_quality_output_acc: 0.5714 - val_age_output_acc: 0.3914 - val_weight_output_acc: 0.6533 - val_bag_output_acc: 0.5724 - val_footwear_output_acc: 0.5908 - val_pose_output_acc: 0.6091 - val_emotion_output_acc: 0.6994\n",
            "\n",
            "Epoch 00003: val_loss improved from 7.76612 to 7.65069, saving model to /content/gdrive/My Drive/EIP/Assignment5Dataset/saved_models/2019-12-29 10:35:46.773811/A5_model.003.h5\n",
            "360/360 [==============================] - 104s 289ms/step - loss: 7.8373 - gender_output_loss: 0.6863 - image_quality_output_loss: 0.9857 - age_output_loss: 1.4331 - weight_output_loss: 0.9944 - bag_output_loss: 0.9248 - footwear_output_loss: 0.9657 - pose_output_loss: 0.9297 - emotion_output_loss: 0.9175 - gender_output_acc: 0.5581 - image_quality_output_acc: 0.5503 - age_output_acc: 0.3998 - weight_output_acc: 0.6326 - bag_output_acc: 0.5615 - footwear_output_acc: 0.5497 - pose_output_acc: 0.6193 - emotion_output_acc: 0.7137 - val_loss: 7.7661 - val_gender_output_loss: 0.6841 - val_image_quality_output_loss: 0.9607 - val_age_output_loss: 1.4317 - val_weight_output_loss: 0.9600 - val_bag_output_loss: 0.9176 - val_footwear_output_loss: 0.9452 - val_pose_output_loss: 0.9342 - val_emotion_output_loss: 0.9326 - val_gender_output_acc: 0.5680 - val_image_quality_output_acc: 0.5714 - val_age_output_acc: 0.3914 - val_weight_output_acc: 0.6533 - val_bag_output_acc: 0.5724 - val_footwear_output_acc: 0.5615 - val_pose_output_acc: 0.6091 - val_emotion_output_acc: 0.6994\n",
            "Epoch 4/50\n",
            "Learning rate:  0.001\n",
            "\n",
            "Epoch 00004: LearningRateScheduler setting learning rate to 0.001.\n",
            "360/360 [==============================] - 104s 289ms/step - loss: 7.6905 - gender_output_loss: 0.6530 - image_quality_output_loss: 0.9809 - age_output_loss: 1.4186 - weight_output_loss: 0.9865 - bag_output_loss: 0.9104 - footwear_output_loss: 0.9132 - pose_output_loss: 0.9182 - emotion_output_loss: 0.9096 - gender_output_acc: 0.6046 - image_quality_output_acc: 0.5496 - age_output_acc: 0.3991 - weight_output_acc: 0.6326 - bag_output_acc: 0.5649 - footwear_output_acc: 0.5817 - pose_output_acc: 0.6193 - emotion_output_acc: 0.7137 - val_loss: 7.5694 - val_gender_output_loss: 0.6361 - val_image_quality_output_loss: 0.9591 - val_age_output_loss: 1.4067 - val_weight_output_loss: 0.9492 - val_bag_output_loss: 0.9034 - val_footwear_output_loss: 0.8778 - val_pose_output_loss: 0.9104 - val_emotion_output_loss: 0.9267 - val_gender_output_acc: 0.6215 - val_image_quality_output_acc: 0.5719 - val_age_output_acc: 0.3938 - val_weight_output_acc: 0.6533 - val_bag_output_acc: 0.5759 - val_footwear_output_acc: 0.6012 - val_pose_output_acc: 0.6091 - val_emotion_output_acc: 0.6994\n",
            "\n",
            "Epoch 00004: val_loss improved from 7.65069 to 7.56939, saving model to /content/gdrive/My Drive/EIP/Assignment5Dataset/saved_models/2019-12-29 10:35:46.773811/A5_model.004.h5\n",
            "Epoch 5/50\n",
            "Learning rate:  0.001\n",
            "\n",
            "Epoch 00005: LearningRateScheduler setting learning rate to 0.001.\n",
            "360/360 [==============================] - 104s 288ms/step - loss: 7.6055 - gender_output_loss: 0.6342 - image_quality_output_loss: 0.9799 - age_output_loss: 1.4134 - weight_output_loss: 0.9842 - bag_output_loss: 0.9007 - footwear_output_loss: 0.8842 - pose_output_loss: 0.9014 - emotion_output_loss: 0.9076 - gender_output_acc: 0.6325 - image_quality_output_acc: 0.5487 - age_output_acc: 0.4010 - weight_output_acc: 0.6326 - bag_output_acc: 0.5650 - footwear_output_acc: 0.6012 - pose_output_acc: 0.6194 - emotion_output_acc: 0.7137 - val_loss: 7.4259 - val_gender_output_loss: 0.6000 - val_image_quality_output_loss: 0.9481 - val_age_output_loss: 1.4051 - val_weight_output_loss: 0.9494 - val_bag_output_loss: 0.8913 - val_footwear_output_loss: 0.8289 - val_pose_output_loss: 0.8801 - val_emotion_output_loss: 0.9230 - val_gender_output_acc: 0.6716 - val_image_quality_output_acc: 0.5709 - val_age_output_acc: 0.3953 - val_weight_output_acc: 0.6533 - val_bag_output_acc: 0.5868 - val_footwear_output_acc: 0.6235 - val_pose_output_acc: 0.6096 - val_emotion_output_acc: 0.6994\n",
            "\n",
            "Epoch 00005: val_loss improved from 7.56939 to 7.42589, saving model to /content/gdrive/My Drive/EIP/Assignment5Dataset/saved_models/2019-12-29 10:35:46.773811/A5_model.005.h5\n",
            "Epoch 6/50\n",
            "Learning rate:  0.001\n",
            "\n",
            "Epoch 00006: LearningRateScheduler setting learning rate to 0.001.\n",
            "360/360 [==============================] - 104s 289ms/step - loss: 7.5059 - gender_output_loss: 0.6151 - image_quality_output_loss: 0.9771 - age_output_loss: 1.4074 - weight_output_loss: 0.9791 - bag_output_loss: 0.8923 - footwear_output_loss: 0.8574 - pose_output_loss: 0.8736 - emotion_output_loss: 0.9039 - gender_output_acc: 0.6577 - image_quality_output_acc: 0.5503 - age_output_acc: 0.3963 - weight_output_acc: 0.6325 - bag_output_acc: 0.5734 - footwear_output_acc: 0.6190 - pose_output_acc: 0.6220 - emotion_output_acc: 0.7137 - val_loss: 7.3078 - val_gender_output_loss: 0.5753 - val_image_quality_output_loss: 0.9473 - val_age_output_loss: 1.4005 - val_weight_output_loss: 0.9468 - val_bag_output_loss: 0.8797 - val_footwear_output_loss: 0.8054 - val_pose_output_loss: 0.8335 - val_emotion_output_loss: 0.9193 - val_gender_output_acc: 0.6949 - val_image_quality_output_acc: 0.5709 - val_age_output_acc: 0.3948 - val_weight_output_acc: 0.6533 - val_bag_output_acc: 0.5942 - val_footwear_output_acc: 0.6384 - val_pose_output_acc: 0.6354 - val_emotion_output_acc: 0.6994\n",
            "\n",
            "Epoch 00006: val_loss improved from 7.42589 to 7.30785, saving model to /content/gdrive/My Drive/EIP/Assignment5Dataset/saved_models/2019-12-29 10:35:46.773811/A5_model.006.h5\n",
            "Epoch 7/50\n",
            "Learning rate:  0.0001\n",
            "\n",
            "Epoch 00007: LearningRateScheduler setting learning rate to 0.0001.\n",
            "360/360 [==============================] - 104s 288ms/step - loss: 7.3756 - gender_output_loss: 0.5890 - image_quality_output_loss: 0.9727 - age_output_loss: 1.3994 - weight_output_loss: 0.9752 - bag_output_loss: 0.8824 - footwear_output_loss: 0.8307 - pose_output_loss: 0.8313 - emotion_output_loss: 0.8950 - gender_output_acc: 0.6835 - image_quality_output_acc: 0.5490 - age_output_acc: 0.4006 - weight_output_acc: 0.6326 - bag_output_acc: 0.5842 - footwear_output_acc: 0.6279 - pose_output_acc: 0.6307 - emotion_output_acc: 0.7137 - val_loss: 7.2395 - val_gender_output_loss: 0.5671 - val_image_quality_output_loss: 0.9450 - val_age_output_loss: 1.3983 - val_weight_output_loss: 0.9384 - val_bag_output_loss: 0.8728 - val_footwear_output_loss: 0.8025 - val_pose_output_loss: 0.7998 - val_emotion_output_loss: 0.9157 - val_gender_output_acc: 0.7063 - val_image_quality_output_acc: 0.5734 - val_age_output_acc: 0.3953 - val_weight_output_acc: 0.6533 - val_bag_output_acc: 0.5962 - val_footwear_output_acc: 0.6429 - val_pose_output_acc: 0.6414 - val_emotion_output_acc: 0.6994\n",
            "\n",
            "Epoch 00007: val_loss improved from 7.30785 to 7.23955, saving model to /content/gdrive/My Drive/EIP/Assignment5Dataset/saved_models/2019-12-29 10:35:46.773811/A5_model.007.h5\n",
            "Epoch 8/50\n",
            "Learning rate:  0.0001\n",
            "\n",
            "Epoch 00008: LearningRateScheduler setting learning rate to 0.0001.\n",
            "360/360 [==============================] - 103s 287ms/step - loss: 7.3225 - gender_output_loss: 0.5789 - image_quality_output_loss: 0.9714 - age_output_loss: 1.3963 - weight_output_loss: 0.9718 - bag_output_loss: 0.8770 - footwear_output_loss: 0.8170 - pose_output_loss: 0.8155 - emotion_output_loss: 0.8946 - gender_output_acc: 0.6946 - image_quality_output_acc: 0.5511 - age_output_acc: 0.3981 - weight_output_acc: 0.6327 - bag_output_acc: 0.5866 - footwear_output_acc: 0.6350 - pose_output_acc: 0.6363 - emotion_output_acc: 0.7137 - val_loss: 7.2042 - val_gender_output_loss: 0.5587 - val_image_quality_output_loss: 0.9434 - val_age_output_loss: 1.3960 - val_weight_output_loss: 0.9370 - val_bag_output_loss: 0.8705 - val_footwear_output_loss: 0.7987 - val_pose_output_loss: 0.7844 - val_emotion_output_loss: 0.9155 - val_gender_output_acc: 0.7088 - val_image_quality_output_acc: 0.5719 - val_age_output_acc: 0.3948 - val_weight_output_acc: 0.6533 - val_bag_output_acc: 0.6027 - val_footwear_output_acc: 0.6438 - val_pose_output_acc: 0.6518 - val_emotion_output_acc: 0.6994\n",
            "\n",
            "Epoch 00008: val_loss improved from 7.23955 to 7.20416, saving model to /content/gdrive/My Drive/EIP/Assignment5Dataset/saved_models/2019-12-29 10:35:46.773811/A5_model.008.h5\n",
            "Epoch 9/50\n",
            "Learning rate:  0.0001\n",
            "\n",
            "Epoch 00009: LearningRateScheduler setting learning rate to 0.0001.\n",
            "360/360 [==============================] - 104s 290ms/step - loss: 7.2850 - gender_output_loss: 0.5703 - image_quality_output_loss: 0.9696 - age_output_loss: 1.3955 - weight_output_loss: 0.9700 - bag_output_loss: 0.8721 - footwear_output_loss: 0.8174 - pose_output_loss: 0.7993 - emotion_output_loss: 0.8908 - gender_output_acc: 0.6994 - image_quality_output_acc: 0.5538 - age_output_acc: 0.3988 - weight_output_acc: 0.6328 - bag_output_acc: 0.5911 - footwear_output_acc: 0.6381 - pose_output_acc: 0.6435 - emotion_output_acc: 0.7137 - val_loss: 7.1652 - val_gender_output_loss: 0.5473 - val_image_quality_output_loss: 0.9417 - val_age_output_loss: 1.3954 - val_weight_output_loss: 0.9356 - val_bag_output_loss: 0.8665 - val_footwear_output_loss: 0.7959 - val_pose_output_loss: 0.7694 - val_emotion_output_loss: 0.9135 - val_gender_output_acc: 0.7207 - val_image_quality_output_acc: 0.5759 - val_age_output_acc: 0.3978 - val_weight_output_acc: 0.6538 - val_bag_output_acc: 0.6042 - val_footwear_output_acc: 0.6424 - val_pose_output_acc: 0.6622 - val_emotion_output_acc: 0.6994\n",
            "\n",
            "Epoch 00009: val_loss improved from 7.20416 to 7.16523, saving model to /content/gdrive/My Drive/EIP/Assignment5Dataset/saved_models/2019-12-29 10:35:46.773811/A5_model.009.h5\n",
            "Epoch 10/50\n",
            "Learning rate:  0.0001\n",
            "\n",
            "Epoch 00010: LearningRateScheduler setting learning rate to 0.0001.\n",
            "360/360 [==============================] - 104s 289ms/step - loss: 7.2587 - gender_output_loss: 0.5672 - image_quality_output_loss: 0.9668 - age_output_loss: 1.3928 - weight_output_loss: 0.9698 - bag_output_loss: 0.8722 - footwear_output_loss: 0.8100 - pose_output_loss: 0.7876 - emotion_output_loss: 0.8924 - gender_output_acc: 0.7016 - image_quality_output_acc: 0.5511 - age_output_acc: 0.4039 - weight_output_acc: 0.6326 - bag_output_acc: 0.5928 - footwear_output_acc: 0.6417 - pose_output_acc: 0.6497 - emotion_output_acc: 0.7137 - val_loss: 7.1341 - val_gender_output_loss: 0.5390 - val_image_quality_output_loss: 0.9397 - val_age_output_loss: 1.3953 - val_weight_output_loss: 0.9376 - val_bag_output_loss: 0.8630 - val_footwear_output_loss: 0.7914 - val_pose_output_loss: 0.7556 - val_emotion_output_loss: 0.9126 - val_gender_output_acc: 0.7247 - val_image_quality_output_acc: 0.5734 - val_age_output_acc: 0.3968 - val_weight_output_acc: 0.6533 - val_bag_output_acc: 0.6076 - val_footwear_output_acc: 0.6443 - val_pose_output_acc: 0.6706 - val_emotion_output_acc: 0.6994\n",
            "\n",
            "Epoch 00010: val_loss improved from 7.16523 to 7.13408, saving model to /content/gdrive/My Drive/EIP/Assignment5Dataset/saved_models/2019-12-29 10:35:46.773811/A5_model.010.h5\n",
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bEpoch 11/50\n",
            "Learning rate:  1e-05\n",
            "\n",
            "Epoch 00011: LearningRateScheduler setting learning rate to 1e-05.\n",
            "360/360 [==============================] - 104s 290ms/step - loss: 7.2185 - gender_output_loss: 0.5553 - image_quality_output_loss: 0.9683 - age_output_loss: 1.3921 - weight_output_loss: 0.9693 - bag_output_loss: 0.8657 - footwear_output_loss: 0.8048 - pose_output_loss: 0.7757 - emotion_output_loss: 0.8874 - gender_output_acc: 0.7150 - image_quality_output_acc: 0.5513 - age_output_acc: 0.3993 - weight_output_acc: 0.6331 - bag_output_acc: 0.5970 - footwear_output_acc: 0.6440 - pose_output_acc: 0.6575 - emotion_output_acc: 0.7137 - val_loss: 7.1226 - val_gender_output_loss: 0.5380 - val_image_quality_output_loss: 0.9386 - val_age_output_loss: 1.3937 - val_weight_output_loss: 0.9364 - val_bag_output_loss: 0.8633 - val_footwear_output_loss: 0.7897 - val_pose_output_loss: 0.7506 - val_emotion_output_loss: 0.9123 - val_gender_output_acc: 0.7262 - val_image_quality_output_acc: 0.5734 - val_age_output_acc: 0.3963 - val_weight_output_acc: 0.6533 - val_bag_output_acc: 0.6076 - val_footwear_output_acc: 0.6458 - val_pose_output_acc: 0.6726 - val_emotion_output_acc: 0.6994\n",
            "\n",
            "Epoch 00011: val_loss improved from 7.13408 to 7.12260, saving model to /content/gdrive/My Drive/EIP/Assignment5Dataset/saved_models/2019-12-29 10:35:46.773811/A5_model.011.h5\n",
            "Epoch 12/50\n",
            "Learning rate:  1e-05\n",
            "\n",
            "Epoch 00012: LearningRateScheduler setting learning rate to 1e-05.\n",
            "360/360 [==============================] - 104s 289ms/step - loss: 7.2040 - gender_output_loss: 0.5541 - image_quality_output_loss: 0.9645 - age_output_loss: 1.3902 - weight_output_loss: 0.9688 - bag_output_loss: 0.8639 - footwear_output_loss: 0.8019 - pose_output_loss: 0.7713 - emotion_output_loss: 0.8893 - gender_output_acc: 0.7130 - image_quality_output_acc: 0.5514 - age_output_acc: 0.4033 - weight_output_acc: 0.6339 - bag_output_acc: 0.5958 - footwear_output_acc: 0.6451 - pose_output_acc: 0.6588 - emotion_output_acc: 0.7137 - val_loss: 7.1163 - val_gender_output_loss: 0.5358 - val_image_quality_output_loss: 0.9379 - val_age_output_loss: 1.3939 - val_weight_output_loss: 0.9371 - val_bag_output_loss: 0.8629 - val_footwear_output_loss: 0.7890 - val_pose_output_loss: 0.7480 - val_emotion_output_loss: 0.9116 - val_gender_output_acc: 0.7297 - val_image_quality_output_acc: 0.5744 - val_age_output_acc: 0.3968 - val_weight_output_acc: 0.6533 - val_bag_output_acc: 0.6062 - val_footwear_output_acc: 0.6458 - val_pose_output_acc: 0.6736 - val_emotion_output_acc: 0.6994\n",
            "\n",
            "Epoch 00012: val_loss improved from 7.12260 to 7.11629, saving model to /content/gdrive/My Drive/EIP/Assignment5Dataset/saved_models/2019-12-29 10:35:46.773811/A5_model.012.h5\n",
            "Epoch 13/50\n",
            "Learning rate:  1e-05\n",
            "\n",
            "Epoch 00013: LearningRateScheduler setting learning rate to 1e-05.\n",
            "360/360 [==============================] - 104s 288ms/step - loss: 7.2021 - gender_output_loss: 0.5538 - image_quality_output_loss: 0.9670 - age_output_loss: 1.3900 - weight_output_loss: 0.9646 - bag_output_loss: 0.8674 - footwear_output_loss: 0.8005 - pose_output_loss: 0.7693 - emotion_output_loss: 0.8894 - gender_output_acc: 0.7137 - image_quality_output_acc: 0.5483 - age_output_acc: 0.4011 - weight_output_acc: 0.6334 - bag_output_acc: 0.5931 - footwear_output_acc: 0.6442 - pose_output_acc: 0.6632 - emotion_output_acc: 0.7137 - val_loss: 7.1127 - val_gender_output_loss: 0.5347 - val_image_quality_output_loss: 0.9378 - val_age_output_loss: 1.3933 - val_weight_output_loss: 0.9368 - val_bag_output_loss: 0.8634 - val_footwear_output_loss: 0.7891 - val_pose_output_loss: 0.7460 - val_emotion_output_loss: 0.9116 - val_gender_output_acc: 0.7302 - val_image_quality_output_acc: 0.5744 - val_age_output_acc: 0.3978 - val_weight_output_acc: 0.6533 - val_bag_output_acc: 0.6101 - val_footwear_output_acc: 0.6463 - val_pose_output_acc: 0.6741 - val_emotion_output_acc: 0.6994\n",
            "\n",
            "Epoch 00013: val_loss improved from 7.11629 to 7.11267, saving model to /content/gdrive/My Drive/EIP/Assignment5Dataset/saved_models/2019-12-29 10:35:46.773811/A5_model.013.h5\n",
            "Epoch 14/50\n",
            "Learning rate:  1e-05\n",
            "\n",
            "Epoch 00014: LearningRateScheduler setting learning rate to 1e-05.\n",
            "360/360 [==============================] - 104s 289ms/step - loss: 7.1967 - gender_output_loss: 0.5508 - image_quality_output_loss: 0.9645 - age_output_loss: 1.3904 - weight_output_loss: 0.9687 - bag_output_loss: 0.8622 - footwear_output_loss: 0.8020 - pose_output_loss: 0.7700 - emotion_output_loss: 0.8879 - gender_output_acc: 0.7196 - image_quality_output_acc: 0.5527 - age_output_acc: 0.4007 - weight_output_acc: 0.6330 - bag_output_acc: 0.6012 - footwear_output_acc: 0.6442 - pose_output_acc: 0.6597 - emotion_output_acc: 0.7137 - val_loss: 7.1068 - val_gender_output_loss: 0.5327 - val_image_quality_output_loss: 0.9371 - val_age_output_loss: 1.3937 - val_weight_output_loss: 0.9371 - val_bag_output_loss: 0.8627 - val_footwear_output_loss: 0.7885 - val_pose_output_loss: 0.7436 - val_emotion_output_loss: 0.9113 - val_gender_output_acc: 0.7321 - val_image_quality_output_acc: 0.5749 - val_age_output_acc: 0.3978 - val_weight_output_acc: 0.6538 - val_bag_output_acc: 0.6091 - val_footwear_output_acc: 0.6463 - val_pose_output_acc: 0.6766 - val_emotion_output_acc: 0.6994\n",
            "\n",
            "Epoch 00014: val_loss improved from 7.11267 to 7.10677, saving model to /content/gdrive/My Drive/EIP/Assignment5Dataset/saved_models/2019-12-29 10:35:46.773811/A5_model.014.h5\n",
            "Epoch 15/50\n",
            "Learning rate:  1e-06\n",
            "\n",
            "Epoch 00015: LearningRateScheduler setting learning rate to 1e-06.\n",
            "360/360 [==============================] - 104s 289ms/step - loss: 7.1858 - gender_output_loss: 0.5500 - image_quality_output_loss: 0.9633 - age_output_loss: 1.3899 - weight_output_loss: 0.9664 - bag_output_loss: 0.8668 - footwear_output_loss: 0.7998 - pose_output_loss: 0.7640 - emotion_output_loss: 0.8855 - gender_output_acc: 0.7175 - image_quality_output_acc: 0.5530 - age_output_acc: 0.4015 - weight_output_acc: 0.6327 - bag_output_acc: 0.5999 - footwear_output_acc: 0.6438 - pose_output_acc: 0.6615 - emotion_output_acc: 0.7137 - val_loss: 7.1058 - val_gender_output_loss: 0.5325 - val_image_quality_output_loss: 0.9371 - val_age_output_loss: 1.3935 - val_weight_output_loss: 0.9365 - val_bag_output_loss: 0.8628 - val_footwear_output_loss: 0.7887 - val_pose_output_loss: 0.7433 - val_emotion_output_loss: 0.9113 - val_gender_output_acc: 0.7331 - val_image_quality_output_acc: 0.5754 - val_age_output_acc: 0.3973 - val_weight_output_acc: 0.6538 - val_bag_output_acc: 0.6081 - val_footwear_output_acc: 0.6473 - val_pose_output_acc: 0.6756 - val_emotion_output_acc: 0.6994\n",
            "\n",
            "Epoch 00015: val_loss improved from 7.10677 to 7.10578, saving model to /content/gdrive/My Drive/EIP/Assignment5Dataset/saved_models/2019-12-29 10:35:46.773811/A5_model.015.h5\n",
            "Epoch 16/50\n",
            "Learning rate:  1e-06\n",
            "\n",
            "Epoch 00016: LearningRateScheduler setting learning rate to 1e-06.\n",
            "360/360 [==============================] - 104s 289ms/step - loss: 7.1805 - gender_output_loss: 0.5470 - image_quality_output_loss: 0.9639 - age_output_loss: 1.3884 - weight_output_loss: 0.9697 - bag_output_loss: 0.8653 - footwear_output_loss: 0.7953 - pose_output_loss: 0.7644 - emotion_output_loss: 0.8866 - gender_output_acc: 0.7201 - image_quality_output_acc: 0.5523 - age_output_acc: 0.3997 - weight_output_acc: 0.6325 - bag_output_acc: 0.5937 - footwear_output_acc: 0.6488 - pose_output_acc: 0.6641 - emotion_output_acc: 0.7137 - val_loss: 7.1049 - val_gender_output_loss: 0.5325 - val_image_quality_output_loss: 0.9371 - val_age_output_loss: 1.3934 - val_weight_output_loss: 0.9365 - val_bag_output_loss: 0.8627 - val_footwear_output_loss: 0.7884 - val_pose_output_loss: 0.7430 - val_emotion_output_loss: 0.9113 - val_gender_output_acc: 0.7316 - val_image_quality_output_acc: 0.5749 - val_age_output_acc: 0.3968 - val_weight_output_acc: 0.6538 - val_bag_output_acc: 0.6111 - val_footwear_output_acc: 0.6463 - val_pose_output_acc: 0.6756 - val_emotion_output_acc: 0.6994\n",
            "\n",
            "Epoch 00016: val_loss improved from 7.10578 to 7.10493, saving model to /content/gdrive/My Drive/EIP/Assignment5Dataset/saved_models/2019-12-29 10:35:46.773811/A5_model.016.h5\n",
            "\n",
            "Epoch 00015: val_loss improved from 7.10677 to 7.10578, saving model to /content/gdrive/My Drive/EIP/Assignment5Dataset/saved_models/2019-12-29 10:35:46.773811/A5_model.015.h5\n",
            "Epoch 17/50\n",
            "Learning rate:  1e-06\n",
            "\n",
            "Epoch 00017: LearningRateScheduler setting learning rate to 1e-06.\n",
            "360/360 [==============================] - 104s 289ms/step - loss: 7.1840 - gender_output_loss: 0.5486 - image_quality_output_loss: 0.9640 - age_output_loss: 1.3897 - weight_output_loss: 0.9664 - bag_output_loss: 0.8639 - footwear_output_loss: 0.7997 - pose_output_loss: 0.7645 - emotion_output_loss: 0.8872 - gender_output_acc: 0.7225 - image_quality_output_acc: 0.5531 - age_output_acc: 0.3988 - weight_output_acc: 0.6322 - bag_output_acc: 0.5991 - footwear_output_acc: 0.6467 - pose_output_acc: 0.6618 - emotion_output_acc: 0.7137 - val_loss: 7.1045 - val_gender_output_loss: 0.5324 - val_image_quality_output_loss: 0.9371 - val_age_output_loss: 1.3933 - val_weight_output_loss: 0.9363 - val_bag_output_loss: 0.8627 - val_footwear_output_loss: 0.7885 - val_pose_output_loss: 0.7428 - val_emotion_output_loss: 0.9114 - val_gender_output_acc: 0.7326 - val_image_quality_output_acc: 0.5744 - val_age_output_acc: 0.3968 - val_weight_output_acc: 0.6538 - val_bag_output_acc: 0.6096 - val_footwear_output_acc: 0.6463 - val_pose_output_acc: 0.6761 - val_emotion_output_acc: 0.6994\n",
            "\n",
            "Epoch 00017: val_loss improved from 7.10493 to 7.10448, saving model to /content/gdrive/My Drive/EIP/Assignment5Dataset/saved_models/2019-12-29 10:35:46.773811/A5_model.017.h5\n",
            "Epoch 18/50\n",
            "Learning rate:  1e-06\n",
            "\n",
            "Epoch 00018: LearningRateScheduler setting learning rate to 1e-06.\n",
            "360/360 [==============================] - 105s 291ms/step - loss: 7.1852 - gender_output_loss: 0.5482 - image_quality_output_loss: 0.9649 - age_output_loss: 1.3902 - weight_output_loss: 0.9667 - bag_output_loss: 0.8625 - footwear_output_loss: 0.8012 - pose_output_loss: 0.7656 - emotion_output_loss: 0.8858 - gender_output_acc: 0.7189 - image_quality_output_acc: 0.5520 - age_output_acc: 0.4004 - weight_output_acc: 0.6330 - bag_output_acc: 0.5988 - footwear_output_acc: 0.6456 - pose_output_acc: 0.6620 - emotion_output_acc: 0.7137 - val_loss: 7.1041 - val_gender_output_loss: 0.5322 - val_image_quality_output_loss: 0.9371 - val_age_output_loss: 1.3934 - val_weight_output_loss: 0.9364 - val_bag_output_loss: 0.8626 - val_footwear_output_loss: 0.7885 - val_pose_output_loss: 0.7427 - val_emotion_output_loss: 0.9113 - val_gender_output_acc: 0.7316 - val_image_quality_output_acc: 0.5744 - val_age_output_acc: 0.3978 - val_weight_output_acc: 0.6538 - val_bag_output_acc: 0.6096 - val_footwear_output_acc: 0.6463 - val_pose_output_acc: 0.6761 - val_emotion_output_acc: 0.6994\n",
            "\n",
            "Epoch 00018: val_loss improved from 7.10448 to 7.10412, saving model to /content/gdrive/My Drive/EIP/Assignment5Dataset/saved_models/2019-12-29 10:35:46.773811/A5_model.018.h5\n",
            "Epoch 19/50\n",
            "Learning rate:  5e-07\n",
            "\n",
            "Epoch 00019: LearningRateScheduler setting learning rate to 5e-07.\n",
            "360/360 [==============================] - 104s 289ms/step - loss: 7.1875 - gender_output_loss: 0.5514 - image_quality_output_loss: 0.9636 - age_output_loss: 1.3896 - weight_output_loss: 0.9675 - bag_output_loss: 0.8659 - footwear_output_loss: 0.8007 - pose_output_loss: 0.7617 - emotion_output_loss: 0.8871 - gender_output_acc: 0.7155 - image_quality_output_acc: 0.5510 - age_output_acc: 0.4022 - weight_output_acc: 0.6323 - bag_output_acc: 0.5941 - footwear_output_acc: 0.6503 - pose_output_acc: 0.6638 - emotion_output_acc: 0.7137 - val_loss: 7.1038 - val_gender_output_loss: 0.5320 - val_image_quality_output_loss: 0.9371 - val_age_output_loss: 1.3934 - val_weight_output_loss: 0.9365 - val_bag_output_loss: 0.8626 - val_footwear_output_loss: 0.7884 - val_pose_output_loss: 0.7426 - val_emotion_output_loss: 0.9113 - val_gender_output_acc: 0.7316 - val_image_quality_output_acc: 0.5749 - val_age_output_acc: 0.3978 - val_weight_output_acc: 0.6538 - val_bag_output_acc: 0.6096 - val_footwear_output_acc: 0.6468 - val_pose_output_acc: 0.6756 - val_emotion_output_acc: 0.6994\n",
            "\n",
            "Epoch 00019: val_loss improved from 7.10412 to 7.10383, saving model to /content/gdrive/My Drive/EIP/Assignment5Dataset/saved_models/2019-12-29 10:35:46.773811/A5_model.019.h5\n",
            "\n",
            "Epoch 20/50\n",
            "Learning rate:  5e-07\n",
            "\n",
            "Epoch 00020: LearningRateScheduler setting learning rate to 5e-07.\n",
            "360/360 [==============================] - 104s 288ms/step - loss: 7.1803 - gender_output_loss: 0.5468 - image_quality_output_loss: 0.9654 - age_output_loss: 1.3885 - weight_output_loss: 0.9660 - bag_output_loss: 0.8639 - footwear_output_loss: 0.7987 - pose_output_loss: 0.7638 - emotion_output_loss: 0.8871 - gender_output_acc: 0.7183 - image_quality_output_acc: 0.5530 - age_output_acc: 0.4023 - weight_output_acc: 0.6332 - bag_output_acc: 0.6005 - footwear_output_acc: 0.6473 - pose_output_acc: 0.6613 - emotion_output_acc: 0.7137 - val_loss: 7.1035 - val_gender_output_loss: 0.5320 - val_image_quality_output_loss: 0.9371 - val_age_output_loss: 1.3934 - val_weight_output_loss: 0.9365 - val_bag_output_loss: 0.8624 - val_footwear_output_loss: 0.7883 - val_pose_output_loss: 0.7425 - val_emotion_output_loss: 0.9113 - val_gender_output_acc: 0.7312 - val_image_quality_output_acc: 0.5739 - val_age_output_acc: 0.3978 - val_weight_output_acc: 0.6538 - val_bag_output_acc: 0.6106 - val_footwear_output_acc: 0.6463 - val_pose_output_acc: 0.6761 - val_emotion_output_acc: 0.6994\n",
            "\n",
            "Epoch 00020: val_loss improved from 7.10383 to 7.10348, saving model to /content/gdrive/My Drive/EIP/Assignment5Dataset/saved_models/2019-12-29 10:35:46.773811/A5_model.020.h5\n",
            "Epoch 21/50\n",
            "Learning rate:  5e-07\n",
            "\n",
            "Epoch 00021: LearningRateScheduler setting learning rate to 5e-07.\n",
            "360/360 [==============================]\n",
            " - 104s 289ms/step - loss: 7.1923 - gender_output_loss: 0.5505 - image_quality_output_loss: 0.9649 - age_output_loss: 1.3889 - weight_output_loss: 0.9668 - bag_output_loss: 0.8650 - footwear_output_loss: 0.7998 - pose_output_loss: 0.7681 - emotion_output_loss: 0.8882 - gender_output_acc: 0.7173 - image_quality_output_acc: 0.5512 - age_output_acc: 0.4011 - weight_output_acc: 0.6334 - bag_output_acc: 0.5976 - footwear_output_acc: 0.6459 - pose_output_acc: 0.6615 - emotion_output_acc: 0.7137 - val_loss: 7.1032 - val_gender_output_loss: 0.5319 - val_image_quality_output_loss: 0.9371 - val_age_output_loss: 1.3934 - val_weight_output_loss: 0.9365 - val_bag_output_loss: 0.8624 - val_footwear_output_loss: 0.7882 - val_pose_output_loss: 0.7425 - val_emotion_output_loss: 0.9113 - val_gender_output_acc: 0.7326 - val_image_quality_output_acc: 0.5739 - val_age_output_acc: 0.3978 - val_weight_output_acc: 0.6538 - val_bag_output_acc: 0.6101 - val_footwear_output_acc: 0.6463 - val_pose_output_acc: 0.6761 - val_emotion_output_acc: 0.6994\n",
            "\n",
            "Epoch 00021: val_loss improved from 7.10348 to 7.10317, saving model to /content/gdrive/My Drive/EIP/Assignment5Dataset/saved_models/2019-12-29 10:35:46.773811/A5_model.021.h5\n",
            "Epoch 22/50\n",
            "Learning rate:  5e-07\n",
            "\n",
            "Epoch 00022: LearningRateScheduler setting learning rate to 5e-07.\n",
            "360/360 [==============================] - 104s 289ms/step - loss: 7.1842 - gender_output_loss: 0.5473 - image_quality_output_loss: 0.9651 - age_output_loss: 1.3893 - weight_output_loss: 0.9669 - bag_output_loss: 0.8656 - footwear_output_loss: 0.7986 - pose_output_loss: 0.7625 - emotion_output_loss: 0.8889 - gender_output_acc: 0.7162 - image_quality_output_acc: 0.5524 - age_output_acc: 0.3992 - weight_output_acc: 0.6326 - bag_output_acc: 0.5984 - footwear_output_acc: 0.6474 - pose_output_acc: 0.6654 - emotion_output_acc: 0.7137 - val_loss: 7.1030 - val_gender_output_loss: 0.5318 - val_image_quality_output_loss: 0.9371 - val_age_output_loss: 1.3934 - val_weight_output_loss: 0.9364 - val_bag_output_loss: 0.8624 - val_footwear_output_loss: 0.7882 - val_pose_output_loss: 0.7425 - val_emotion_output_loss: 0.9113 - val_gender_output_acc: 0.7316 - val_image_quality_output_acc: 0.5749 - val_age_output_acc: 0.3973 - val_weight_output_acc: 0.6538 - val_bag_output_acc: 0.6106 - val_footwear_output_acc: 0.6463 - val_pose_output_acc: 0.6766 - val_emotion_output_acc: 0.6994\n",
            "\n",
            "Epoch 00022: val_loss improved from 7.10317 to 7.10303, saving model to /content/gdrive/My Drive/EIP/Assignment5Dataset/saved_models/2019-12-29 10:35:46.773811/A5_model.022.h5\n",
            "\n",
            "Epoch 00021: val_loss improved from 7.10348 to 7.10317, saving model to /content/gdrive/My Drive/EIP/Assignment5Dataset/saved_models/2019-12-29 10:35:46.773811/A5_model.021.h5\n",
            "Epoch 23/50\n",
            "Learning rate:  5e-07\n",
            "\n",
            "Epoch 00023: LearningRateScheduler setting learning rate to 5e-07.\n",
            "360/360 [==============================] - 105s 291ms/step - loss: 7.1880 - gender_output_loss: 0.5475 - image_quality_output_loss: 0.9643 - age_output_loss: 1.3898 - weight_output_loss: 0.9663 - bag_output_loss: 0.8642 - footwear_output_loss: 0.8028 - pose_output_loss: 0.7639 - emotion_output_loss: 0.8892 - gender_output_acc: 0.7214 - image_quality_output_acc: 0.5530 - age_output_acc: 0.4036 - weight_output_acc: 0.6332 - bag_output_acc: 0.6003 - footwear_output_acc: 0.6467 - pose_output_acc: 0.6636 - emotion_output_acc: 0.7137 - val_loss: 7.1026 - val_gender_output_loss: 0.5317 - val_image_quality_output_loss: 0.9371 - val_age_output_loss: 1.3934 - val_weight_output_loss: 0.9364 - val_bag_output_loss: 0.8623 - val_footwear_output_loss: 0.7881 - val_pose_output_loss: 0.7423 - val_emotion_output_loss: 0.9113 - val_gender_output_acc: 0.7326 - val_image_quality_output_acc: 0.5739 - val_age_output_acc: 0.3973 - val_weight_output_acc: 0.6538 - val_bag_output_acc: 0.6101 - val_footwear_output_acc: 0.6468 - val_pose_output_acc: 0.6776 - val_emotion_output_acc: 0.6994\n",
            "\n",
            "Epoch 00023: val_loss improved from 7.10303 to 7.10263, saving model to /content/gdrive/My Drive/EIP/Assignment5Dataset/saved_models/2019-12-29 10:35:46.773811/A5_model.023.h5\n",
            "Epoch 24/50\n",
            "Learning rate:  5e-07\n",
            "\n",
            "Epoch 00024: LearningRateScheduler setting learning rate to 5e-07.\n",
            "360/360 [==============================] - 105s 290ms/step - loss: 7.1940 - gender_output_loss: 0.5505 - image_quality_output_loss: 0.9649 - age_output_loss: 1.3901 - weight_output_loss: 0.9664 - bag_output_loss: 0.8658 - footwear_output_loss: 0.8022 - pose_output_loss: 0.7653 - emotion_output_loss: 0.8889 - gender_output_acc: 0.7182 - image_quality_output_acc: 0.5503 - age_output_acc: 0.3987 - weight_output_acc: 0.6327 - bag_output_acc: 0.5974 - footwear_output_acc: 0.6459 - pose_output_acc: 0.6624 - emotion_output_acc: 0.7137 - val_loss: 7.1023 - val_gender_output_loss: 0.5316 - val_image_quality_output_loss: 0.9371 - val_age_output_loss: 1.3934 - val_weight_output_loss: 0.9364 - val_bag_output_loss: 0.8623 - val_footwear_output_loss: 0.7881 - val_pose_output_loss: 0.7422 - val_emotion_output_loss: 0.9113 - val_gender_output_acc: 0.7326 - val_image_quality_output_acc: 0.5744 - val_age_output_acc: 0.3973 - val_weight_output_acc: 0.6538 - val_bag_output_acc: 0.6101 - val_footwear_output_acc: 0.6463 - val_pose_output_acc: 0.6781 - val_emotion_output_acc: 0.6994\n",
            "\n",
            "Epoch 00024: val_loss improved from 7.10263 to 7.10231, saving model to /content/gdrive/My Drive/EIP/Assignment5Dataset/saved_models/2019-12-29 10:35:46.773811/A5_model.024.h5\n",
            "Epoch 25/50\n",
            "Learning rate:  5e-07\n",
            "\n",
            "Epoch 00025: LearningRateScheduler setting learning rate to 5e-07.\n",
            "360/360 [==============================] - 104s 289ms/step - loss: 7.1779 - gender_output_loss: 0.5458 - image_quality_output_loss: 0.9627 - age_output_loss: 1.3882 - weight_output_loss: 0.9674 - bag_output_loss: 0.8637 - footwear_output_loss: 0.7985 - pose_output_loss: 0.7629 - emotion_output_loss: 0.8887 - gender_output_acc: 0.7185 - image_quality_output_acc: 0.5516 - age_output_acc: 0.4009 - weight_output_acc: 0.6334 - bag_output_acc: 0.5987 - footwear_output_acc: 0.6484 - pose_output_acc: 0.6625 - emotion_output_acc: 0.7137 - val_loss: 7.1021 - val_gender_output_loss: 0.5316 - val_image_quality_output_loss: 0.9370 - val_age_output_loss: 1.3934 - val_weight_output_loss: 0.9364 - val_bag_output_loss: 0.8623 - val_footwear_output_loss: 0.7881 - val_pose_output_loss: 0.7421 - val_emotion_output_loss: 0.9113 - val_gender_output_acc: 0.7326 - val_image_quality_output_acc: 0.5744 - val_age_output_acc: 0.3973 - val_weight_output_acc: 0.6538 - val_bag_output_acc: 0.6111 - val_footwear_output_acc: 0.6463 - val_pose_output_acc: 0.6771 - val_emotion_output_acc: 0.6994\n",
            "\n",
            "Epoch 00025: val_loss improved from 7.10231 to 7.10212, saving model to /content/gdrive/My Drive/EIP/Assignment5Dataset/saved_models/2019-12-29 10:35:46.773811/A5_model.025.h5\n",
            "Epoch 26/50\n",
            "Learning rate:  5e-07\n",
            "\n",
            "Epoch 00026: LearningRateScheduler setting learning rate to 5e-07.\n",
            "360/360 [==============================] - 104s 290ms/step - loss: 7.1815 - gender_output_loss: 0.5465 - image_quality_output_loss: 0.9670 - age_output_loss: 1.3890 - weight_output_loss: 0.9666 - bag_output_loss: 0.8666 - footwear_output_loss: 0.7977 - pose_output_loss: 0.7621 - emotion_output_loss: 0.8860 - gender_output_acc: 0.7196 - image_quality_output_acc: 0.5517 - age_output_acc: 0.4010 - weight_output_acc: 0.6326 - bag_output_acc: 0.5971 - footwear_output_acc: 0.6461 - pose_output_acc: 0.6632 - emotion_output_acc: 0.7136 - val_loss: 7.1019 - val_gender_output_loss: 0.5315 - val_image_quality_output_loss: 0.9370 - val_age_output_loss: 1.3934 - val_weight_output_loss: 0.9364 - val_bag_output_loss: 0.8623 - val_footwear_output_loss: 0.7880 - val_pose_output_loss: 0.7420 - val_emotion_output_loss: 0.9112 - val_gender_output_acc: 0.7326 - val_image_quality_output_acc: 0.5739 - val_age_output_acc: 0.3973 - val_weight_output_acc: 0.6538 - val_bag_output_acc: 0.6106 - val_footwear_output_acc: 0.6453 - val_pose_output_acc: 0.6781 - val_emotion_output_acc: 0.6994\n",
            "\n",
            "Epoch 00026: val_loss improved from 7.10212 to 7.10191, saving model to /content/gdrive/My Drive/EIP/Assignment5Dataset/saved_models/2019-12-29 10:35:46.773811/A5_model.026.h5\n",
            "Epoch 27/50\n",
            "Learning rate:  5e-07\n",
            "\n",
            "Epoch 00027: LearningRateScheduler setting learning rate to 5e-07.\n",
            "360/360 [==============================] - 104s 290ms/step - loss: 7.1805 - gender_output_loss: 0.5491 - image_quality_output_loss: 0.9654 - age_output_loss: 1.3899 - weight_output_loss: 0.9648 - bag_output_loss: 0.8640 - footwear_output_loss: 0.7997 - pose_output_loss: 0.7604 - emotion_output_loss: 0.8871 - gender_output_acc: 0.7207 - image_quality_output_acc: 0.5527 - age_output_acc: 0.3967 - weight_output_acc: 0.6333 - bag_output_acc: 0.5982 - footwear_output_acc: 0.6453 - pose_output_acc: 0.6658 - emotion_output_acc: 0.7137 - val_loss: 7.1017 - val_gender_output_loss: 0.5315 - val_image_quality_output_loss: 0.9370 - val_age_output_loss: 1.3934 - val_weight_output_loss: 0.9365 - val_bag_output_loss: 0.8622 - val_footwear_output_loss: 0.7880 - val_pose_output_loss: 0.7418 - val_emotion_output_loss: 0.9112 - val_gender_output_acc: 0.7307 - val_image_quality_output_acc: 0.5739 - val_age_output_acc: 0.3973 - val_weight_output_acc: 0.6538 - val_bag_output_acc: 0.6116 - val_footwear_output_acc: 0.6453 - val_pose_output_acc: 0.6766 - val_emotion_output_acc: 0.6994\n",
            "\n",
            "Epoch 00027: val_loss improved from 7.10191 to 7.10167, saving model to /content/gdrive/My Drive/EIP/Assignment5Dataset/saved_models/2019-12-29 10:35:46.773811/A5_model.027.h5\n",
            "Epoch 28/50\n",
            "Learning rate:  5e-07\n",
            "\n",
            "Epoch 00028: LearningRateScheduler setting learning rate to 5e-07.\n",
            "360/360 [==============================] - 104s 289ms/step - loss: 7.1731 - gender_output_loss: 0.5464 - image_quality_output_loss: 0.9649 - age_output_loss: 1.3902 - weight_output_loss: 0.9672 - bag_output_loss: 0.8598 - footwear_output_loss: 0.7987 - pose_output_loss: 0.7605 - emotion_output_loss: 0.8855 - gender_output_acc: 0.7170 - image_quality_output_acc: 0.5520 - age_output_acc: 0.3997 - weight_output_acc: 0.6330 - bag_output_acc: 0.5995 - footwear_output_acc: 0.6478 - pose_output_acc: 0.6644 - emotion_output_acc: 0.7137 - val_loss: 7.1014 - val_gender_output_loss: 0.5315 - val_image_quality_output_loss: 0.9370 - val_age_output_loss: 1.3933 - val_weight_output_loss: 0.9363 - val_bag_output_loss: 0.8623 - val_footwear_output_loss: 0.7881 - val_pose_output_loss: 0.7417 - val_emotion_output_loss: 0.9112 - val_gender_output_acc: 0.7326 - val_image_quality_output_acc: 0.5744 - val_age_output_acc: 0.3968 - val_weight_output_acc: 0.6538 - val_bag_output_acc: 0.6116 - val_footwear_output_acc: 0.6458 - val_pose_output_acc: 0.6771 - val_emotion_output_acc: 0.6994\n",
            "\n",
            "Epoch 00028: val_loss improved from 7.10167 to 7.10139, saving model to /content/gdrive/My Drive/EIP/Assignment5Dataset/saved_models/2019-12-29 10:35:46.773811/A5_model.028.h5\n",
            "Epoch 29/50\n",
            "Learning rate:  5e-07\n",
            "\n",
            "Epoch 00029: LearningRateScheduler setting learning rate to 5e-07.\n",
            "360/360 [==============================] - 104s 290ms/step - loss: 7.1827 - gender_output_loss: 0.5475 - image_quality_output_loss: 0.9637 - age_output_loss: 1.3904 - weight_output_loss: 0.9665 - bag_output_loss: 0.8634 - footwear_output_loss: 0.8022 - pose_output_loss: 0.7621 - emotion_output_loss: 0.8871 - gender_output_acc: 0.7190 - image_quality_output_acc: 0.5519 - age_output_acc: 0.4013 - weight_output_acc: 0.6337 - bag_output_acc: 0.5998 - footwear_output_acc: 0.6476 - pose_output_acc: 0.6654 - emotion_output_acc: 0.7137 - val_loss: 7.1009 - val_gender_output_loss: 0.5313 - val_image_quality_output_loss: 0.9370 - val_age_output_loss: 1.3934 - val_weight_output_loss: 0.9363 - val_bag_output_loss: 0.8623 - val_footwear_output_loss: 0.7880 - val_pose_output_loss: 0.7415 - val_emotion_output_loss: 0.9112 - val_gender_output_acc: 0.7316 - val_image_quality_output_acc: 0.5744 - val_age_output_acc: 0.3973 - val_weight_output_acc: 0.6538 - val_bag_output_acc: 0.6111 - val_footwear_output_acc: 0.6458 - val_pose_output_acc: 0.6776 - val_emotion_output_acc: 0.6994\n",
            "\n",
            "Epoch 00029: val_loss improved from 7.10139 to 7.10094, saving model to /content/gdrive/My Drive/EIP/Assignment5Dataset/saved_models/2019-12-29 10:35:46.773811/A5_model.029.h5\n",
            "Epoch 30/50\n",
            "Learning rate:  5e-07\n",
            "\n",
            "Epoch 00030: LearningRateScheduler setting learning rate to 5e-07.\n",
            "360/360 [==============================] - 104s 289ms/step - loss: 7.1667 - gender_output_loss: 0.5451 - image_quality_output_loss: 0.9629 - age_output_loss: 1.3863 - weight_output_loss: 0.9641 - bag_output_loss: 0.8629 - footwear_output_loss: 0.7985 - pose_output_loss: 0.7596 - emotion_output_loss: 0.8871 - gender_output_acc: 0.7195 - image_quality_output_acc: 0.5513 - age_output_acc: 0.4030 - weight_output_acc: 0.6335 - bag_output_acc: 0.6020 - footwear_output_acc: 0.6465 - pose_output_acc: 0.6677 - emotion_output_acc: 0.7137 - val_loss: 7.1007 - val_gender_output_loss: 0.5313 - val_image_quality_output_loss: 0.9369 - val_age_output_loss: 1.3934 - val_weight_output_loss: 0.9363 - val_bag_output_loss: 0.8622 - val_footwear_output_loss: 0.7880 - val_pose_output_loss: 0.7414 - val_emotion_output_loss: 0.9112 - val_gender_output_acc: 0.7316 - val_image_quality_output_acc: 0.5744 - val_age_output_acc: 0.3973 - val_weight_output_acc: 0.6538 - val_bag_output_acc: 0.6116 - val_footwear_output_acc: 0.6458 - val_pose_output_acc: 0.6771 - val_emotion_output_acc: 0.6994\n",
            "\n",
            "Epoch 00030: val_loss improved from 7.10094 to 7.10068, saving model to /content/gdrive/My Drive/EIP/Assignment5Dataset/saved_models/2019-12-29 10:35:46.773811/A5_model.030.h5\n",
            "Epoch 31/50\n",
            "Learning rate:  5e-07\n",
            "\n",
            "Epoch 00031: LearningRateScheduler setting learning rate to 5e-07.\n",
            "359/360 [============================>.] - ETA: 0s - loss: 7.1764 - gender_output_loss: 0.5460 - image_quality_output_loss: 0.9634 - age_output_loss: 1.3881 - weight_output_loss: 0.9674 - bag_output_loss: 0.8611 - footwear_output_loss: 0.7973 - pose_output_loss: 0.7654 - emotion_output_loss: 0.8878 - gender_output_acc: 0.7213 - image_quality_output_acc: 0.5528 - age_output_acc: 0.4032 - weight_output_acc: 0.6328 - bag_output_acc: 0.6050 - footwear_output_acc: 0.6476 - pose_output_acc: 0.6628 - emotion_output_acc: 0.7136\n",
            "360/360 [==============================] - 104s 290ms/step - loss: 7.1765 - gender_output_loss: 0.5464 - image_quality_output_loss: 0.9635 - age_output_loss: 1.3880 - weight_output_loss: 0.9674 - bag_output_loss: 0.8613 - footwear_output_loss: 0.7968 - pose_output_loss: 0.7656 - emotion_output_loss: 0.8875 - gender_output_acc: 0.7207 - image_quality_output_acc: 0.5526 - age_output_acc: 0.4032 - weight_output_acc: 0.6330 - bag_output_acc: 0.6051 - footwear_output_acc: 0.6477 - pose_output_acc: 0.6624 - emotion_output_acc: 0.7137 - val_loss: 7.1003 - val_gender_output_loss: 0.5310 - val_image_quality_output_loss: 0.9369 - val_age_output_loss: 1.3934 - val_weight_output_loss: 0.9362 - val_bag_output_loss: 0.8622 - val_footwear_output_loss: 0.7881 - val_pose_output_loss: 0.7413 - val_emotion_output_loss: 0.9112 - val_gender_output_acc: 0.7312 - val_image_quality_output_acc: 0.5744 - val_age_output_acc: 0.3973 - val_weight_output_acc: 0.6538 - val_bag_output_acc: 0.6116 - val_footwear_output_acc: 0.6458 - val_pose_output_acc: 0.6771 - val_emotion_output_acc: 0.6994\n",
            "\n",
            "Epoch 00031: val_loss improved from 7.10068 to 7.10029, saving model to /content/gdrive/My Drive/EIP/Assignment5Dataset/saved_models/2019-12-29 10:35:46.773811/A5_model.031.h5\n",
            "Epoch 32/50\n",
            "Learning rate:  5e-07\n",
            "\n",
            "Epoch 00032: LearningRateScheduler setting learning rate to 5e-07.\n",
            "360/360 [==============================] - 105s 290ms/step - loss: 7.1714 - gender_output_loss: 0.5445 - image_quality_output_loss: 0.9645 - age_output_loss: 1.3875 - weight_output_loss: 0.9662 - bag_output_loss: 0.8631 - footwear_output_loss: 0.7980 - pose_output_loss: 0.7608 - emotion_output_loss: 0.8869 - gender_output_acc: 0.7221 - image_quality_output_acc: 0.5526 - age_output_acc: 0.4004 - weight_output_acc: 0.6326 - bag_output_acc: 0.5960 - footwear_output_acc: 0.6480 - pose_output_acc: 0.6657 - emotion_output_acc: 0.7137 - val_loss: 7.1001 - val_gender_output_loss: 0.5310 - val_image_quality_output_loss: 0.9369 - val_age_output_loss: 1.3934 - val_weight_output_loss: 0.9362 - val_bag_output_loss: 0.8622 - val_footwear_output_loss: 0.7881 - val_pose_output_loss: 0.7412 - val_emotion_output_loss: 0.9112 - val_gender_output_acc: 0.7312 - val_image_quality_output_acc: 0.5754 - val_age_output_acc: 0.3973 - val_weight_output_acc: 0.6538 - val_bag_output_acc: 0.6116 - val_footwear_output_acc: 0.6458 - val_pose_output_acc: 0.6771 - val_emotion_output_acc: 0.6994\n",
            "\n",
            "Epoch 00032: val_loss improved from 7.10029 to 7.10006, saving model to /content/gdrive/My Drive/EIP/Assignment5Dataset/saved_models/2019-12-29 10:35:46.773811/A5_model.032.h5\n",
            "Epoch 33/50\n",
            "Learning rate:  5e-07\n",
            "\n",
            "Epoch 00033: LearningRateScheduler setting learning rate to 5e-07.\n",
            "359/360 [============================>.] - ETA: 0s - loss: 7.1761 - gender_output_loss: 0.5446 - image_quality_output_loss: 0.9657 - age_output_loss: 1.3870 - weight_output_loss: 0.9654 - bag_output_loss: 0.8630 - footwear_output_loss: 0.8010 - pose_output_loss: 0.7619 - emotion_output_loss: 0.8875 - gender_output_acc: 0.7210 - image_quality_output_acc: 0.5527 - age_output_acc: 0.4043 - weight_output_acc: 0.6338 - bag_output_acc: 0.6022 - footwear_output_acc: 0.6468 - pose_output_acc: 0.6663 - emotion_output_acc: 0.7137\n",
            "Epoch 00032: val_loss improved from 7.10029 to 7.10006, saving model to /content/gdrive/My Drive/EIP/Assignment5Dataset/saved_models/2019-12-29 10:35:46.773811/A5_model.032.h5\n",
            "360/360 [==============================] - 105s 291ms/step - loss: 7.1764 - gender_output_loss: 0.5447 - image_quality_output_loss: 0.9658 - age_output_loss: 1.3872 - weight_output_loss: 0.9658 - bag_output_loss: 0.8629 - footwear_output_loss: 0.8006 - pose_output_loss: 0.7620 - emotion_output_loss: 0.8875 - gender_output_acc: 0.7211 - image_quality_output_acc: 0.5525 - age_output_acc: 0.4043 - weight_output_acc: 0.6333 - bag_output_acc: 0.6021 - footwear_output_acc: 0.6470 - pose_output_acc: 0.6659 - emotion_output_acc: 0.7137 - val_loss: 7.0997 - val_gender_output_loss: 0.5310 - val_image_quality_output_loss: 0.9369 - val_age_output_loss: 1.3934 - val_weight_output_loss: 0.9363 - val_bag_output_loss: 0.8621 - val_footwear_output_loss: 0.7880 - val_pose_output_loss: 0.7410 - val_emotion_output_loss: 0.9111 - val_gender_output_acc: 0.7312 - val_image_quality_output_acc: 0.5744 - val_age_output_acc: 0.3973 - val_weight_output_acc: 0.6538 - val_bag_output_acc: 0.6116 - val_footwear_output_acc: 0.6453 - val_pose_output_acc: 0.6771 - val_emotion_output_acc: 0.6994\n",
            "\n",
            "Epoch 00033: val_loss improved from 7.10006 to 7.09974, saving model to /content/gdrive/My Drive/EIP/Assignment5Dataset/saved_models/2019-12-29 10:35:46.773811/A5_model.033.h5\n",
            "Epoch 34/50\n",
            "Learning rate:  5e-07\n",
            "\n",
            "Epoch 00034: LearningRateScheduler setting learning rate to 5e-07.\n",
            "360/360 [==============================] - 104s 289ms/step - loss: 7.1827 - gender_output_loss: 0.5451 - image_quality_output_loss: 0.9649 - age_output_loss: 1.3906 - weight_output_loss: 0.9638 - bag_output_loss: 0.8662 - footwear_output_loss: 0.7977 - pose_output_loss: 0.7663 - emotion_output_loss: 0.8881 - gender_output_acc: 0.7227 - image_quality_output_acc: 0.5518 - age_output_acc: 0.4006 - weight_output_acc: 0.6331 - bag_output_acc: 0.5983 - footwear_output_acc: 0.6482 - pose_output_acc: 0.6646 - emotion_output_acc: 0.7137 - val_loss: 7.0995 - val_gender_output_loss: 0.5308 - val_image_quality_output_loss: 0.9369 - val_age_output_loss: 1.3933 - val_weight_output_loss: 0.9362 - val_bag_output_loss: 0.8622 - val_footwear_output_loss: 0.7880 - val_pose_output_loss: 0.7410 - val_emotion_output_loss: 0.9111 - val_gender_output_acc: 0.7316 - val_image_quality_output_acc: 0.5749 - val_age_output_acc: 0.3968 - val_weight_output_acc: 0.6538 - val_bag_output_acc: 0.6111 - val_footwear_output_acc: 0.6458 - val_pose_output_acc: 0.6781 - val_emotion_output_acc: 0.6994\n",
            "\n",
            "Epoch 00034: val_loss improved from 7.09974 to 7.09953, saving model to /content/gdrive/My Drive/EIP/Assignment5Dataset/saved_models/2019-12-29 10:35:46.773811/A5_model.034.h5\n",
            "\n",
            "Epoch 00033: val_loss improved from 7.10006 to 7.09974, saving model to /content/gdrive/My Drive/EIP/Assignment5Dataset/saved_models/2019-12-29 10:35:46.773811/A5_model.033.h5\n",
            "Epoch 35/50\n",
            "Learning rate:  5e-07\n",
            "\n",
            "Epoch 00035: LearningRateScheduler setting learning rate to 5e-07.\n",
            "360/360 [==============================] - 105s 290ms/step - loss: 7.1673 - gender_output_loss: 0.5455 - image_quality_output_loss: 0.9645 - age_output_loss: 1.3890 - weight_output_loss: 0.9654 - bag_output_loss: 0.8623 - footwear_output_loss: 0.7954 - pose_output_loss: 0.7583 - emotion_output_loss: 0.8870 - gender_output_acc: 0.7208 - image_quality_output_acc: 0.5510 - age_output_acc: 0.3997 - weight_output_acc: 0.6330 - bag_output_acc: 0.6009 - footwear_output_acc: 0.6491 - pose_output_acc: 0.6646 - emotion_output_acc: 0.7137 - val_loss: 7.0995 - val_gender_output_loss: 0.5307 - val_image_quality_output_loss: 0.9368 - val_age_output_loss: 1.3934 - val_weight_output_loss: 0.9363 - val_bag_output_loss: 0.8622 - val_footwear_output_loss: 0.7881 - val_pose_output_loss: 0.7409 - val_emotion_output_loss: 0.9111 - val_gender_output_acc: 0.7312 - val_image_quality_output_acc: 0.5754 - val_age_output_acc: 0.3973 - val_weight_output_acc: 0.6538 - val_bag_output_acc: 0.6106 - val_footwear_output_acc: 0.6458 - val_pose_output_acc: 0.6791 - val_emotion_output_acc: 0.6994\n",
            "\n",
            "Epoch 00035: val_loss improved from 7.09953 to 7.09946, saving model to /content/gdrive/My Drive/EIP/Assignment5Dataset/saved_models/2019-12-29 10:35:46.773811/A5_model.035.h5\n",
            "Epoch 36/50\n",
            "Learning rate:  5e-07\n",
            "\n",
            "Epoch 00036: LearningRateScheduler setting learning rate to 5e-07.\n",
            "360/360 [==============================] - 105s 291ms/step - loss: 7.1754 - gender_output_loss: 0.5466 - image_quality_output_loss: 0.9660 - age_output_loss: 1.3876 - weight_output_loss: 0.9658 - bag_output_loss: 0.8627 - footwear_output_loss: 0.7983 - pose_output_loss: 0.7609 - emotion_output_loss: 0.8875 - gender_output_acc: 0.7200 - image_quality_output_acc: 0.5509 - age_output_acc: 0.3991 - weight_output_acc: 0.6326 - bag_output_acc: 0.6007 - footwear_output_acc: 0.6473 - pose_output_acc: 0.6636 - emotion_output_acc: 0.7137 - val_loss: 7.0992 - val_gender_output_loss: 0.5307 - val_image_quality_output_loss: 0.9368 - val_age_output_loss: 1.3933 - val_weight_output_loss: 0.9362 - val_bag_output_loss: 0.8621 - val_footwear_output_loss: 0.7881 - val_pose_output_loss: 0.7408 - val_emotion_output_loss: 0.9111 - val_gender_output_acc: 0.7316 - val_image_quality_output_acc: 0.5754 - val_age_output_acc: 0.3968 - val_weight_output_acc: 0.6538 - val_bag_output_acc: 0.6116 - val_footwear_output_acc: 0.6458 - val_pose_output_acc: 0.6781 - val_emotion_output_acc: 0.6994\n",
            "\n",
            "Epoch 00036: val_loss improved from 7.09946 to 7.09919, saving model to /content/gdrive/My Drive/EIP/Assignment5Dataset/saved_models/2019-12-29 10:35:46.773811/A5_model.036.h5\n",
            "Epoch 37/50\n",
            "Learning rate:  5e-07\n",
            "\n",
            "Epoch 00037: LearningRateScheduler setting learning rate to 5e-07.\n",
            "360/360 [==============================] - 104s 289ms/step - loss: 7.1739 - gender_output_loss: 0.5454 - image_quality_output_loss: 0.9634 - age_output_loss: 1.3901 - weight_output_loss: 0.9654 - bag_output_loss: 0.8621 - footwear_output_loss: 0.8003 - pose_output_loss: 0.7623 - emotion_output_loss: 0.8848 - gender_output_acc: 0.7200 - image_quality_output_acc: 0.5510 - age_output_acc: 0.4026 - weight_output_acc: 0.6323 - bag_output_acc: 0.6004 - footwear_output_acc: 0.6463 - pose_output_acc: 0.6627 - emotion_output_acc: 0.7137 - val_loss: 7.0990 - val_gender_output_loss: 0.5307 - val_image_quality_output_loss: 0.9368 - val_age_output_loss: 1.3933 - val_weight_output_loss: 0.9362 - val_bag_output_loss: 0.8621 - val_footwear_output_loss: 0.7880 - val_pose_output_loss: 0.7408 - val_emotion_output_loss: 0.9111 - val_gender_output_acc: 0.7316 - val_image_quality_output_acc: 0.5754 - val_age_output_acc: 0.3973 - val_weight_output_acc: 0.6538 - val_bag_output_acc: 0.6116 - val_footwear_output_acc: 0.6458 - val_pose_output_acc: 0.6786 - val_emotion_output_acc: 0.6994\n",
            "\n",
            "Epoch 00037: val_loss improved from 7.09919 to 7.09904, saving model to /content/gdrive/My Drive/EIP/Assignment5Dataset/saved_models/2019-12-29 10:35:46.773811/A5_model.037.h5\n",
            "Epoch 38/50\n",
            "Learning rate:  5e-07\n",
            "\n",
            "Epoch 00038: LearningRateScheduler setting learning rate to 5e-07.\n",
            "360/360 [==============================] - 104s 288ms/step - loss: 7.1787 - gender_output_loss: 0.5452 - image_quality_output_loss: 0.9629 - age_output_loss: 1.3902 - weight_output_loss: 0.9681 - bag_output_loss: 0.8654 - footwear_output_loss: 0.7979 - pose_output_loss: 0.7629 - emotion_output_loss: 0.8861 - gender_output_acc: 0.7231 - image_quality_output_acc: 0.5547 - age_output_acc: 0.3970 - weight_output_acc: 0.6332 - bag_output_acc: 0.5960 - footwear_output_acc: 0.6461 - pose_output_acc: 0.6629 - emotion_output_acc: 0.7137 - val_loss: 7.0987 - val_gender_output_loss: 0.5306 - val_image_quality_output_loss: 0.9368 - val_age_output_loss: 1.3933 - val_weight_output_loss: 0.9363 - val_bag_output_loss: 0.8620 - val_footwear_output_loss: 0.7879 - val_pose_output_loss: 0.7407 - val_emotion_output_loss: 0.9111 - val_gender_output_acc: 0.7316 - val_image_quality_output_acc: 0.5754 - val_age_output_acc: 0.3973 - val_weight_output_acc: 0.6538 - val_bag_output_acc: 0.6121 - val_footwear_output_acc: 0.6463 - val_pose_output_acc: 0.6766 - val_emotion_output_acc: 0.6994\n",
            "\n",
            "Epoch 00038: val_loss improved from 7.09904 to 7.09872, saving model to /content/gdrive/My Drive/EIP/Assignment5Dataset/saved_models/2019-12-29 10:35:46.773811/A5_model.038.h5\n",
            "Epoch 39/50\n",
            "Learning rate:  5e-07\n",
            "\n",
            "Epoch 00039: LearningRateScheduler setting learning rate to 5e-07.\n",
            "360/360 [==============================] - 104s 289ms/step - loss: 7.1753 - gender_output_loss: 0.5442 - image_quality_output_loss: 0.9653 - age_output_loss: 1.3892 - weight_output_loss: 0.9692 - bag_output_loss: 0.8622 - footwear_output_loss: 0.7972 - pose_output_loss: 0.7596 - emotion_output_loss: 0.8885 - gender_output_acc: 0.7215 - image_quality_output_acc: 0.5511 - age_output_acc: 0.4026 - weight_output_acc: 0.6321 - bag_output_acc: 0.5975 - footwear_output_acc: 0.6471 - pose_output_acc: 0.6666 - emotion_output_acc: 0.7137 - val_loss: 7.0983 - val_gender_output_loss: 0.5304 - val_image_quality_output_loss: 0.9368 - val_age_output_loss: 1.3933 - val_weight_output_loss: 0.9362 - val_bag_output_loss: 0.8620 - val_footwear_output_loss: 0.7879 - val_pose_output_loss: 0.7407 - val_emotion_output_loss: 0.9111 - val_gender_output_acc: 0.7321 - val_image_quality_output_acc: 0.5749 - val_age_output_acc: 0.3968 - val_weight_output_acc: 0.6538 - val_bag_output_acc: 0.6111 - val_footwear_output_acc: 0.6463 - val_pose_output_acc: 0.6781 - val_emotion_output_acc: 0.6994\n",
            "\n",
            "Epoch 00039: val_loss improved from 7.09872 to 7.09830, saving model to /content/gdrive/My Drive/EIP/Assignment5Dataset/saved_models/2019-12-29 10:35:46.773811/A5_model.039.h5\n",
            "Epoch 40/50\n",
            "Learning rate:  5e-07\n",
            "\n",
            "Epoch 00040: LearningRateScheduler setting learning rate to 5e-07.\n",
            "360/360 [==============================] - 105s 290ms/step - loss: 7.1845 - gender_output_loss: 0.5499 - image_quality_output_loss: 0.9640 - age_output_loss: 1.3902 - weight_output_loss: 0.9682 - bag_output_loss: 0.8635 - footwear_output_loss: 0.7977 - pose_output_loss: 0.7642 - emotion_output_loss: 0.8869 - gender_output_acc: 0.7178 - image_quality_output_acc: 0.5503 - age_output_acc: 0.3995 - weight_output_acc: 0.6340 - bag_output_acc: 0.5977 - footwear_output_acc: 0.6484 - pose_output_acc: 0.6652 - emotion_output_acc: 0.7137 - val_loss: 7.0980 - val_gender_output_loss: 0.5304 - val_image_quality_output_loss: 0.9368 - val_age_output_loss: 1.3933 - val_weight_output_loss: 0.9363 - val_bag_output_loss: 0.8620 - val_footwear_output_loss: 0.7878 - val_pose_output_loss: 0.7405 - val_emotion_output_loss: 0.9111 - val_gender_output_acc: 0.7316 - val_image_quality_output_acc: 0.5744 - val_age_output_acc: 0.3973 - val_weight_output_acc: 0.6538 - val_bag_output_acc: 0.6121 - val_footwear_output_acc: 0.6463 - val_pose_output_acc: 0.6771 - val_emotion_output_acc: 0.6994\n",
            "\n",
            "Epoch 00040: val_loss improved from 7.09830 to 7.09801, saving model to /content/gdrive/My Drive/EIP/Assignment5Dataset/saved_models/2019-12-29 10:35:46.773811/A5_model.040.h5\n",
            "Epoch 41/50\n",
            "Learning rate:  5e-07\n",
            "\n",
            "Epoch 00041: LearningRateScheduler setting learning rate to 5e-07.\n",
            "360/360 [==============================] - 104s 289ms/step - loss: 7.1743 - gender_output_loss: 0.5480 - image_quality_output_loss: 0.9639 - age_output_loss: 1.3861 - weight_output_loss: 0.9664 - bag_output_loss: 0.8649 - footwear_output_loss: 0.7965 - pose_output_loss: 0.7639 - emotion_output_loss: 0.8846 - gender_output_acc: 0.7202 - image_quality_output_acc: 0.5506 - age_output_acc: 0.4014 - weight_output_acc: 0.6332 - bag_output_acc: 0.5980 - footwear_output_acc: 0.6489 - pose_output_acc: 0.6635 - emotion_output_acc: 0.7137 - val_loss: 7.0980 - val_gender_output_loss: 0.5304 - val_image_quality_output_loss: 0.9367 - val_age_output_loss: 1.3934 - val_weight_output_loss: 0.9364 - val_bag_output_loss: 0.8618 - val_footwear_output_loss: 0.7878 - val_pose_output_loss: 0.7403 - val_emotion_output_loss: 0.9110 - val_gender_output_acc: 0.7321 - val_image_quality_output_acc: 0.5754 - val_age_output_acc: 0.3973 - val_weight_output_acc: 0.6538 - val_bag_output_acc: 0.6126 - val_footwear_output_acc: 0.6458 - val_pose_output_acc: 0.6781 - val_emotion_output_acc: 0.6994\n",
            "\n",
            "Epoch 00041: val_loss improved from 7.09801 to 7.09795, saving model to /content/gdrive/My Drive/EIP/Assignment5Dataset/saved_models/2019-12-29 10:35:46.773811/A5_model.041.h5\n",
            "Epoch 42/50\n",
            "Learning rate:  5e-07\n",
            "\n",
            "Epoch 00042: LearningRateScheduler setting learning rate to 5e-07.\n",
            "360/360 [==============================] - 104s 288ms/step - loss: 7.1865 - gender_output_loss: 0.5460 - image_quality_output_loss: 0.9642 - age_output_loss: 1.3906 - weight_output_loss: 0.9678 - bag_output_loss: 0.8651 - footwear_output_loss: 0.7997 - pose_output_loss: 0.7628 - emotion_output_loss: 0.8902 - gender_output_acc: 0.7197 - image_quality_output_acc: 0.5532 - age_output_acc: 0.4000 - weight_output_acc: 0.6328 - bag_output_acc: 0.5988 - footwear_output_acc: 0.6477 - pose_output_acc: 0.6658 - emotion_output_acc: 0.7137 - val_loss: 7.0975 - val_gender_output_loss: 0.5302 - val_image_quality_output_loss: 0.9367 - val_age_output_loss: 1.3933 - val_weight_output_loss: 0.9362 - val_bag_output_loss: 0.8620 - val_footwear_output_loss: 0.7878 - val_pose_output_loss: 0.7403 - val_emotion_output_loss: 0.9111 - val_gender_output_acc: 0.7321 - val_image_quality_output_acc: 0.5744 - val_age_output_acc: 0.3968 - val_weight_output_acc: 0.6538 - val_bag_output_acc: 0.6111 - val_footwear_output_acc: 0.6463 - val_pose_output_acc: 0.6781 - val_emotion_output_acc: 0.6994\n",
            "\n",
            "Epoch 00042: val_loss improved from 7.09795 to 7.09749, saving model to /content/gdrive/My Drive/EIP/Assignment5Dataset/saved_models/2019-12-29 10:35:46.773811/A5_model.042.h5\n",
            "Epoch 43/50\n",
            "Learning rate:  5e-07\n",
            "\n",
            "Epoch 00043: LearningRateScheduler setting learning rate to 5e-07.\n",
            "360/360 [==============================] - 104s 289ms/step - loss: 7.1680 - gender_output_loss: 0.5453 - image_quality_output_loss: 0.9637 - age_output_loss: 1.3890 - weight_output_loss: 0.9656 - bag_output_loss: 0.8647 - footwear_output_loss: 0.7934 - pose_output_loss: 0.7615 - emotion_output_loss: 0.8849 - gender_output_acc: 0.7242 - image_quality_output_acc: 0.5523 - age_output_acc: 0.3992 - weight_output_acc: 0.6330 - bag_output_acc: 0.5994 - footwear_output_acc: 0.6482 - pose_output_acc: 0.6661 - emotion_output_acc: 0.7137 - val_loss: 7.0973 - val_gender_output_loss: 0.5301 - val_image_quality_output_loss: 0.9367 - val_age_output_loss: 1.3934 - val_weight_output_loss: 0.9362 - val_bag_output_loss: 0.8619 - val_footwear_output_loss: 0.7879 - val_pose_output_loss: 0.7401 - val_emotion_output_loss: 0.9110 - val_gender_output_acc: 0.7316 - val_image_quality_output_acc: 0.5754 - val_age_output_acc: 0.3968 - val_weight_output_acc: 0.6538 - val_bag_output_acc: 0.6116 - val_footwear_output_acc: 0.6458 - val_pose_output_acc: 0.6796 - val_emotion_output_acc: 0.6994\n",
            "\n",
            "Epoch 00043: val_loss improved from 7.09749 to 7.09732, saving model to /content/gdrive/My Drive/EIP/Assignment5Dataset/saved_models/2019-12-29 10:35:46.773811/A5_model.043.h5\n",
            "Epoch 44/50\n",
            "Learning rate:  5e-07\n",
            "\n",
            "Epoch 00044: LearningRateScheduler setting learning rate to 5e-07.\n",
            "360/360 [==============================] - 104s 289ms/step - loss: 7.1742 - gender_output_loss: 0.5459 - image_quality_output_loss: 0.9638 - age_output_loss: 1.3890 - weight_output_loss: 0.9654 - bag_output_loss: 0.8609 - footwear_output_loss: 0.7995 - pose_output_loss: 0.7618 - emotion_output_loss: 0.8879 - gender_output_acc: 0.7219 - image_quality_output_acc: 0.5521 - age_output_acc: 0.4016 - weight_output_acc: 0.6331 - bag_output_acc: 0.5990 - footwear_output_acc: 0.6494 - pose_output_acc: 0.6633 - emotion_output_acc: 0.7137 - val_loss: 7.0972 - val_gender_output_loss: 0.5299 - val_image_quality_output_loss: 0.9367 - val_age_output_loss: 1.3933 - val_weight_output_loss: 0.9361 - val_bag_output_loss: 0.8619 - val_footwear_output_loss: 0.7880 - val_pose_output_loss: 0.7401 - val_emotion_output_loss: 0.9110 - val_gender_output_acc: 0.7312 - val_image_quality_output_acc: 0.5754 - val_age_output_acc: 0.3968 - val_weight_output_acc: 0.6538 - val_bag_output_acc: 0.6106 - val_footwear_output_acc: 0.6453 - val_pose_output_acc: 0.6791 - val_emotion_output_acc: 0.6994\n",
            "\n",
            "Epoch 00044: val_loss improved from 7.09732 to 7.09716, saving model to /content/gdrive/My Drive/EIP/Assignment5Dataset/saved_models/2019-12-29 10:35:46.773811/A5_model.044.h5\n",
            "\n",
            "Epoch 45/50\n",
            "Learning rate:  5e-07\n",
            "\n",
            "Epoch 00045: LearningRateScheduler setting learning rate to 5e-07.\n",
            "360/360 [==============================] - 104s 290ms/step - loss: 7.1791 - gender_output_loss: 0.5469 - image_quality_output_loss: 0.9656 - age_output_loss: 1.3899 - weight_output_loss: 0.9672 - bag_output_loss: 0.8636 - footwear_output_loss: 0.7988 - pose_output_loss: 0.7614 - emotion_output_loss: 0.8858 - gender_output_acc: 0.7189 - image_quality_output_acc: 0.5531 - age_output_acc: 0.4001 - weight_output_acc: 0.6322 - bag_output_acc: 0.5964 - footwear_output_acc: 0.6454 - pose_output_acc: 0.6650 - emotion_output_acc: 0.7137 - val_loss: 7.0966 - val_gender_output_loss: 0.5299 - val_image_quality_output_loss: 0.9367 - val_age_output_loss: 1.3933 - val_weight_output_loss: 0.9361 - val_bag_output_loss: 0.8618 - val_footwear_output_loss: 0.7878 - val_pose_output_loss: 0.7400 - val_emotion_output_loss: 0.9110 - val_gender_output_acc: 0.7312 - val_image_quality_output_acc: 0.5744 - val_age_output_acc: 0.3968 - val_weight_output_acc: 0.6538 - val_bag_output_acc: 0.6116 - val_footwear_output_acc: 0.6463 - val_pose_output_acc: 0.6796 - val_emotion_output_acc: 0.6994\n",
            "\n",
            "Epoch 00045: val_loss improved from 7.09716 to 7.09662, saving model to /content/gdrive/My Drive/EIP/Assignment5Dataset/saved_models/2019-12-29 10:35:46.773811/A5_model.045.h5\n",
            "Epoch 46/50\n",
            "Learning rate:  5e-07\n",
            "\n",
            "Epoch 00046: LearningRateScheduler setting learning rate to 5e-07.\n",
            "360/360 [==============================] - 105s 291ms/step - loss: 7.1744 - gender_output_loss: 0.5456 - image_quality_output_loss: 0.9636 - age_output_loss: 1.3877 - weight_output_loss: 0.9663 - bag_output_loss: 0.8622 - footwear_output_loss: 0.7985 - pose_output_loss: 0.7643 - emotion_output_loss: 0.8862 - gender_output_acc: 0.7236 - image_quality_output_acc: 0.5527 - age_output_acc: 0.4040 - weight_output_acc: 0.6332 - bag_output_acc: 0.6005 - footwear_output_acc: 0.6457 - pose_output_acc: 0.6646 - emotion_output_acc: 0.7137 - val_loss: 7.0965 - val_gender_output_loss: 0.5297 - val_image_quality_output_loss: 0.9367 - val_age_output_loss: 1.3934 - val_weight_output_loss: 0.9362 - val_bag_output_loss: 0.8618 - val_footwear_output_loss: 0.7879 - val_pose_output_loss: 0.7399 - val_emotion_output_loss: 0.9110 - val_gender_output_acc: 0.7307 - val_image_quality_output_acc: 0.5759 - val_age_output_acc: 0.3968 - val_weight_output_acc: 0.6538 - val_bag_output_acc: 0.6116 - val_footwear_output_acc: 0.6463 - val_pose_output_acc: 0.6801 - val_emotion_output_acc: 0.6994\n",
            "\n",
            "Epoch 00046: val_loss improved from 7.09662 to 7.09654, saving model to /content/gdrive/My Drive/EIP/Assignment5Dataset/saved_models/2019-12-29 10:35:46.773811/A5_model.046.h5\n",
            "Epoch 47/50\n",
            "Learning rate:  5e-07\n",
            "\n",
            "Epoch 00047: LearningRateScheduler setting learning rate to 5e-07.\n",
            "360/360 [==============================] - 104s 290ms/step - loss: 7.1759 - gender_output_loss: 0.5462 - image_quality_output_loss: 0.9650 - age_output_loss: 1.3889 - weight_output_loss: 0.9690 - bag_output_loss: 0.8622 - footwear_output_loss: 0.7970 - pose_output_loss: 0.7600 - emotion_output_loss: 0.8876 - gender_output_acc: 0.7181 - image_quality_output_acc: 0.5503 - age_output_acc: 0.4009 - weight_output_acc: 0.6328 - bag_output_acc: 0.5996 - footwear_output_acc: 0.6483 - pose_output_acc: 0.6630 - emotion_output_acc: 0.7137 - val_loss: 7.0962 - val_gender_output_loss: 0.5297 - val_image_quality_output_loss: 0.9367 - val_age_output_loss: 1.3934 - val_weight_output_loss: 0.9362 - val_bag_output_loss: 0.8617 - val_footwear_output_loss: 0.7878 - val_pose_output_loss: 0.7398 - val_emotion_output_loss: 0.9110 - val_gender_output_acc: 0.7312 - val_image_quality_output_acc: 0.5759 - val_age_output_acc: 0.3968 - val_weight_output_acc: 0.6538 - val_bag_output_acc: 0.6121 - val_footwear_output_acc: 0.6453 - val_pose_output_acc: 0.6806 - val_emotion_output_acc: 0.6994\n",
            "\n",
            "Epoch 00047: val_loss improved from 7.09654 to 7.09624, saving model to /content/gdrive/My Drive/EIP/Assignment5Dataset/saved_models/2019-12-29 10:35:46.773811/A5_model.047.h5\n",
            "Epoch 48/50\n",
            "Learning rate:  5e-07\n",
            "\n",
            "Epoch 00048: LearningRateScheduler setting learning rate to 5e-07.\n",
            "360/360 [==============================] - 105s 290ms/step - loss: 7.1714 - gender_output_loss: 0.5471 - image_quality_output_loss: 0.9635 - age_output_loss: 1.3899 - weight_output_loss: 0.9637 - bag_output_loss: 0.8646 - footwear_output_loss: 0.7974 - pose_output_loss: 0.7590 - emotion_output_loss: 0.8860 - gender_output_acc: 0.7196 - image_quality_output_acc: 0.5532 - age_output_acc: 0.4030 - weight_output_acc: 0.6328 - bag_output_acc: 0.6005 - footwear_output_acc: 0.6487 - pose_output_acc: 0.6616 - emotion_output_acc: 0.7137 - val_loss: 7.0960 - val_gender_output_loss: 0.5297 - val_image_quality_output_loss: 0.9367 - val_age_output_loss: 1.3934 - val_weight_output_loss: 0.9362 - val_bag_output_loss: 0.8617 - val_footwear_output_loss: 0.7877 - val_pose_output_loss: 0.7397 - val_emotion_output_loss: 0.9110 - val_gender_output_acc: 0.7316 - val_image_quality_output_acc: 0.5749 - val_age_output_acc: 0.3968 - val_weight_output_acc: 0.6538 - val_bag_output_acc: 0.6121 - val_footwear_output_acc: 0.6463 - val_pose_output_acc: 0.6806 - val_emotion_output_acc: 0.6994\n",
            "\n",
            "Epoch 00048: val_loss improved from 7.09624 to 7.09602, saving model to /content/gdrive/My Drive/EIP/Assignment5Dataset/saved_models/2019-12-29 10:35:46.773811/A5_model.048.h5\n",
            "Epoch 49/50\n",
            "Learning rate:  5e-07\n",
            "\n",
            "Epoch 00049: LearningRateScheduler setting learning rate to 5e-07.\n",
            "359/360 [============================>.] - ETA: 0s - loss: 7.1750 - gender_output_loss: 0.5457 - image_quality_output_loss: 0.9626 - age_output_loss: 1.3875 - weight_output_loss: 0.9688 - bag_output_loss: 0.8648 - footwear_output_loss: 0.7993 - pose_output_loss: 0.7599 - emotion_output_loss: 0.8865 - gender_output_acc: 0.7176 - image_quality_output_acc: 0.5502 - age_output_acc: 0.3998 - weight_output_acc: 0.6338 - bag_output_acc: 0.6014 - footwear_output_acc: 0.6500 - pose_output_acc: 0.6651 - emotion_output_acc: 0.7134\n",
            "360/360 [==============================] - 105s 290ms/step - loss: 7.1747 - gender_output_loss: 0.5456 - image_quality_output_loss: 0.9626 - age_output_loss: 1.3874 - weight_output_loss: 0.9691 - bag_output_loss: 0.8646 - footwear_output_loss: 0.7990 - pose_output_loss: 0.7604 - emotion_output_loss: 0.8859 - gender_output_acc: 0.7177 - image_quality_output_acc: 0.5503 - age_output_acc: 0.3997 - weight_output_acc: 0.6336 - bag_output_acc: 0.6013 - footwear_output_acc: 0.6502 - pose_output_acc: 0.6648 - emotion_output_acc: 0.7137 - val_loss: 7.0958 - val_gender_output_loss: 0.5297 - val_image_quality_output_loss: 0.9366 - val_age_output_loss: 1.3933 - val_weight_output_loss: 0.9361 - val_bag_output_loss: 0.8618 - val_footwear_output_loss: 0.7877 - val_pose_output_loss: 0.7396 - val_emotion_output_loss: 0.9110 - val_gender_output_acc: 0.7312 - val_image_quality_output_acc: 0.5764 - val_age_output_acc: 0.3968 - val_weight_output_acc: 0.6538 - val_bag_output_acc: 0.6116 - val_footwear_output_acc: 0.6453 - val_pose_output_acc: 0.6801 - val_emotion_output_acc: 0.6994\n",
            "\n",
            "Epoch 00049: val_loss improved from 7.09602 to 7.09582, saving model to /content/gdrive/My Drive/EIP/Assignment5Dataset/saved_models/2019-12-29 10:35:46.773811/A5_model.049.h5\n",
            "Epoch 50/50\n",
            "Learning rate:  5e-07\n",
            "\n",
            "Epoch 00050: LearningRateScheduler setting learning rate to 5e-07.\n",
            "359/360 [============================>.] - ETA: 0s - loss: 7.1714 - gender_output_loss: 0.5472 - image_quality_output_loss: 0.9632 - age_output_loss: 1.3882 - weight_output_loss: 0.9665 - bag_output_loss: 0.8630 - footwear_output_loss: 0.7941 - pose_output_loss: 0.7623 - emotion_output_loss: 0.8869 - gender_output_acc: 0.7202 - image_quality_output_acc: 0.5502 - age_output_acc: 0.3995 - weight_output_acc: 0.6328 - bag_output_acc: 0.6013 - footwear_output_acc: 0.6521 - pose_output_acc: 0.6654 - emotion_output_acc: 0.7139\n",
            "360/360 [==============================] - 105s 291ms/step - loss: 7.1703 - gender_output_loss: 0.5470 - image_quality_output_loss: 0.9631 - age_output_loss: 1.3879 - weight_output_loss: 0.9661 - bag_output_loss: 0.8631 - footwear_output_loss: 0.7939 - pose_output_loss: 0.7619 - emotion_output_loss: 0.8873 - gender_output_acc: 0.7205 - image_quality_output_acc: 0.5500 - age_output_acc: 0.3997 - weight_output_acc: 0.6331 - bag_output_acc: 0.6012 - footwear_output_acc: 0.6522 - pose_output_acc: 0.6656 - emotion_output_acc: 0.7137 - val_loss: 7.0955 - val_gender_output_loss: 0.5295 - val_image_quality_output_loss: 0.9366 - val_age_output_loss: 1.3933 - val_weight_output_loss: 0.9361 - val_bag_output_loss: 0.8617 - val_footwear_output_loss: 0.7877 - val_pose_output_loss: 0.7395 - val_emotion_output_loss: 0.9110 - val_gender_output_acc: 0.7312 - val_image_quality_output_acc: 0.5764 - val_age_output_acc: 0.3968 - val_weight_output_acc: 0.6538 - val_bag_output_acc: 0.6121 - val_footwear_output_acc: 0.6453 - val_pose_output_acc: 0.6806 - val_emotion_output_acc: 0.6994\n",
            "\n",
            "Epoch 00050: val_loss improved from 7.09582 to 7.09550, saving model to /content/gdrive/My Drive/EIP/Assignment5Dataset/saved_models/2019-12-29 10:35:46.773811/A5_model.050.h5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zI1hJb4qM6OH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from keras.utils import plot_model\n",
        "# plot_model(model, to_file='/content/gdrive/My Drive/EIP/Assignment5Dataset/model.png')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HKtaMfcHMj66",
        "colab_type": "code",
        "outputId": "f5bbce79-f663-46b5-f9c7-388fa0040034",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 341
        }
      },
      "source": [
        "results = model.evaluate_generator(valid_gen, verbose=1)\n",
        "dict(zip(model.metrics_names, results))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "63/63 [==============================] - 6s 90ms/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'age_output_acc': 0.3968253968253968,\n",
              " 'age_output_loss': 1.3933049402539692,\n",
              " 'bag_output_acc': 0.6121031746031746,\n",
              " 'bag_output_loss': 0.8617341367025224,\n",
              " 'emotion_output_acc': 0.6994047619047619,\n",
              " 'emotion_output_loss': 0.9109562058297415,\n",
              " 'footwear_output_acc': 0.6453373015873016,\n",
              " 'footwear_output_loss': 0.7877351707882352,\n",
              " 'gender_output_acc': 0.7311507936507936,\n",
              " 'gender_output_loss': 0.5295270146831633,\n",
              " 'image_quality_output_acc': 0.5763888888888888,\n",
              " 'image_quality_output_loss': 0.9365993785479713,\n",
              " 'loss': 7.095501112559485,\n",
              " 'pose_output_acc': 0.6805555555555556,\n",
              " 'pose_output_loss': 0.7395308117071787,\n",
              " 'weight_output_acc': 0.6537698412698413,\n",
              " 'weight_output_loss': 0.9361135183818756}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "18C_lZn8mTeE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "fe88eda1-e773-4d86-8a39-4c6fe7fb42c6"
      },
      "source": [
        "import os\n",
        "import datetime\n",
        "# Prepare model model saving directory.\n",
        "# save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
        "t = datetime.datetime.today()\n",
        "print(str(datetime.datetime.today()))\n",
        "save_dir = \"/content/gdrive/My Drive/EIP/Assignment5Dataset/saved_models/\"+str(t)\n",
        "model_name = 'A5_model.{epoch:03d}.h5'\n",
        "if not os.path.isdir(save_dir):\n",
        "    os.makedirs(save_dir)\n",
        "filepath = os.path.join(save_dir, model_name)\n",
        "filepath"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-12-29 14:28:42.929818\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/gdrive/My Drive/EIP/Assignment5Dataset/saved_models/2019-12-29 14:28:42.929758/A5_model.{epoch:03d}.h5'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kmcrm3ZAl7_m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = keras.models.load_model(\"/content/gdrive/My Drive/EIP/Assignment5Dataset/saved_models/2019-12-29 10:35:46.773811/A5_model.050.h5\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RLkKkW4ck7XO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def lr_schedule(epoch):\n",
        "    \"\"\"Learning Rate Schedule\n",
        "\n",
        "    Learning rate is scheduled to be reduced after 80, 120, 160, 180 epochs.\n",
        "    Called automatically every epoch as part of callbacks during training.\n",
        "\n",
        "    # Arguments\n",
        "        epoch (int): The number of epochs\n",
        "\n",
        "    # Returns\n",
        "        lr (float32): learning rate\n",
        "    \"\"\"\n",
        "    return 0.01\n",
        "\n",
        "    # lr = 1e-2\n",
        "    # if epoch > 17:\n",
        "    #     lr *= 0.2e-3\n",
        "    # elif epoch > 13:\n",
        "    #     lr *= 1e-2\n",
        "    # elif epoch > 9:\n",
        "    #     lr *= 1e-1\n",
        "    # elif epoch > 5:\n",
        "    #     lr *= 1e-0\n",
        "    # print('Learning rate: ', lr)\n",
        "    # return lr\n",
        "\n",
        "    # lr = 0.5\n",
        "    # if epoch > 45:\n",
        "    #     lr *= 0.1\n",
        "    # elif epoch > 40:\n",
        "    #     lr *= 0.2\n",
        "    # elif epoch > 30:\n",
        "    #     lr *= 0.3\n",
        "    # elif epoch > 20:\n",
        "    #     lr *= 0.4\n",
        "    # print('Learning rate: ', lr)\n",
        "    # return lr"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "afh4FcIOlEKL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "losses = {\n",
        " \t\"gender_output\": \"binary_crossentropy\",\n",
        " \t\"image_quality_output\": \"categorical_crossentropy\",\n",
        " \t\"age_output\": \"categorical_crossentropy\",\n",
        " \t\"weight_output\": \"categorical_crossentropy\",\n",
        "  \"bag_output\": \"categorical_crossentropy\",\n",
        " \t\"footwear_output\": \"categorical_crossentropy\",\n",
        " \t\"pose_output\": \"categorical_crossentropy\",\n",
        "  \"emotion_output\": \"categorical_crossentropy\",\n",
        " }\n",
        "\n",
        "loss_weights = {\"gender_output\": 1.0, \"image_quality_output\": 1.0, \"age_output\": 1.0,\"weight_output\": 1.0,\"bag_output\": 1.0,\"footwear_output\": 1.0,\"pose_output\": 1.0,\"emotion_output\": 1.0}\n",
        "\n",
        "opt = SGD(lr=0.01, momentum=0.9)\n",
        "model.compile(\n",
        "    optimizer=opt,\n",
        "    loss=losses, \n",
        "    loss_weights=loss_weights, \n",
        "    metrics=[\"accuracy\"]\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fOPXI9CFlG-b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Prepare callbacks for model saving and for learning rate adjustment.\n",
        "checkpoint = ModelCheckpoint(filepath=filepath,\n",
        "                             monitor='val_loss',\n",
        "                             verbose=1,\n",
        "                             save_best_only=True)\n",
        "\n",
        "\n",
        "es = EarlyStopping(monitor='val_loss', verbose=1, patience=5, mode='min')\n",
        "\n",
        "lr_scheduler = LearningRateScheduler(lr_schedule, verbose=1)\n",
        "\n",
        "# lr_reducer = ReduceLROnPlateau(factor=np.sqrt(0.1),\n",
        "#                                cooldown=0,\n",
        "#                                patience=5,\n",
        "#                                min_lr=0.5e-6)\n",
        "\n",
        "# callbacks = [checkpoint, lr_reducer, lr_scheduler]\n",
        "callbacks = [checkpoint,es,lr_scheduler]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "En32UI09lU0b",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "57e6d0fb-ca84-41a8-8940-1c16d8453833"
      },
      "source": [
        "model_output = model.fit_generator(\n",
        "    generator=train_gen,\n",
        "    validation_data=valid_gen,\n",
        "    use_multiprocessing=True,\n",
        "    workers=6, \n",
        "    epochs=20,\n",
        "    verbose=1,\n",
        "    callbacks=callbacks\n",
        ")"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "\n",
            "Epoch 00001: LearningRateScheduler setting learning rate to 0.01.\n",
            "360/360 [==============================] - 123s 343ms/step - loss: 7.8573 - gender_output_loss: 0.6877 - image_quality_output_loss: 0.9863 - age_output_loss: 1.4327 - weight_output_loss: 0.9897 - bag_output_loss: 0.9219 - footwear_output_loss: 1.0033 - pose_output_loss: 0.9282 - emotion_output_loss: 0.9077 - gender_output_acc: 0.5594 - image_quality_output_acc: 0.5503 - age_output_acc: 0.3994 - weight_output_acc: 0.6326 - bag_output_acc: 0.5611 - footwear_output_acc: 0.5037 - pose_output_acc: 0.6193 - emotion_output_acc: 0.7137 - val_loss: 7.7761 - val_gender_output_loss: 0.6848 - val_image_quality_output_loss: 0.9629 - val_age_output_loss: 1.4298 - val_weight_output_loss: 0.9542 - val_bag_output_loss: 0.9141 - val_footwear_output_loss: 0.9647 - val_pose_output_loss: 0.9336 - val_emotion_output_loss: 0.9319 - val_gender_output_acc: 0.5680 - val_image_quality_output_acc: 0.5714 - val_age_output_acc: 0.3914 - val_weight_output_acc: 0.6533 - val_bag_output_acc: 0.5724 - val_footwear_output_acc: 0.5456 - val_pose_output_acc: 0.6091 - val_emotion_output_acc: 0.6994\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 7.77608, saving model to /content/gdrive/My Drive/EIP/Assignment5Dataset/saved_models/2019-12-29 14:28:42.929758/A5_model.001.h5\n",
            "Epoch 1/20\n",
            "Epoch 2/20\n",
            "\n",
            "Epoch 00002: LearningRateScheduler setting learning rate to 0.01.\n",
            "360/360 [==============================] - 106s 294ms/step - loss: 7.7625 - gender_output_loss: 0.6819 - image_quality_output_loss: 0.9826 - age_output_loss: 1.4231 - weight_output_loss: 0.9849 - bag_output_loss: 0.9159 - footwear_output_loss: 0.9462 - pose_output_loss: 0.9240 - emotion_output_loss: 0.9038 - gender_output_acc: 0.5618 - image_quality_output_acc: 0.5502 - age_output_acc: 0.3993 - weight_output_acc: 0.6326 - bag_output_acc: 0.5613 - footwear_output_acc: 0.5653 - pose_output_acc: 0.6193 - emotion_output_acc: 0.7137 - val_loss: 7.6624 - val_gender_output_loss: 0.6780 - val_image_quality_output_loss: 0.9603 - val_age_output_loss: 1.4265 - val_weight_output_loss: 0.9546 - val_bag_output_loss: 0.9118 - val_footwear_output_loss: 0.8761 - val_pose_output_loss: 0.9305 - val_emotion_output_loss: 0.9246 - val_gender_output_acc: 0.5630 - val_image_quality_output_acc: 0.5714 - val_age_output_acc: 0.3914 - val_weight_output_acc: 0.6533 - val_bag_output_acc: 0.5724 - val_footwear_output_acc: 0.6066 - val_pose_output_acc: 0.6091 - val_emotion_output_acc: 0.6994\n",
            "\n",
            "Epoch 00002: val_loss improved from 7.77608 to 7.66237, saving model to /content/gdrive/My Drive/EIP/Assignment5Dataset/saved_models/2019-12-29 14:28:42.929758/A5_model.002.h5\n",
            "Epoch 3/20\n",
            "\n",
            "Epoch 00003: LearningRateScheduler setting learning rate to 0.01.\n",
            "360/360 [==============================] - 107s 297ms/step - loss: 7.6867 - gender_output_loss: 0.6679 - image_quality_output_loss: 0.9827 - age_output_loss: 1.4174 - weight_output_loss: 0.9852 - bag_output_loss: 0.9117 - footwear_output_loss: 0.8996 - pose_output_loss: 0.9207 - emotion_output_loss: 0.9016 - gender_output_acc: 0.5863 - image_quality_output_acc: 0.5495 - age_output_acc: 0.3981 - weight_output_acc: 0.6326 - bag_output_acc: 0.5598 - footwear_output_acc: 0.5872 - pose_output_acc: 0.6193 - emotion_output_acc: 0.7137 - val_loss: 7.6288 - val_gender_output_loss: 0.6514 - val_image_quality_output_loss: 0.9642 - val_age_output_loss: 1.4237 - val_weight_output_loss: 0.9518 - val_bag_output_loss: 0.9108 - val_footwear_output_loss: 0.8832 - val_pose_output_loss: 0.9206 - val_emotion_output_loss: 0.9231 - val_gender_output_acc: 0.6106 - val_image_quality_output_acc: 0.5724 - val_age_output_acc: 0.3914 - val_weight_output_acc: 0.6533 - val_bag_output_acc: 0.5724 - val_footwear_output_acc: 0.6002 - val_pose_output_acc: 0.6091 - val_emotion_output_acc: 0.6994\n",
            "\n",
            "Epoch 00003: val_loss improved from 7.66237 to 7.62876, saving model to /content/gdrive/My Drive/EIP/Assignment5Dataset/saved_models/2019-12-29 14:28:42.929758/A5_model.003.h5\n",
            "Epoch 4/20\n",
            "\n",
            "Epoch 00004: LearningRateScheduler setting learning rate to 0.01.\n",
            "360/360 [==============================] - 106s 293ms/step - loss: 7.6158 - gender_output_loss: 0.6522 - image_quality_output_loss: 0.9802 - age_output_loss: 1.4141 - weight_output_loss: 0.9813 - bag_output_loss: 0.9098 - footwear_output_loss: 0.8744 - pose_output_loss: 0.9033 - emotion_output_loss: 0.9004 - gender_output_acc: 0.6138 - image_quality_output_acc: 0.5501 - age_output_acc: 0.3990 - weight_output_acc: 0.6326 - bag_output_acc: 0.5612 - footwear_output_acc: 0.6079 - pose_output_acc: 0.6202 - emotion_output_acc: 0.7137 - val_loss: 7.4876 - val_gender_output_loss: 0.6156 - val_image_quality_output_loss: 0.9608 - val_age_output_loss: 1.4127 - val_weight_output_loss: 0.9471 - val_bag_output_loss: 0.9053 - val_footwear_output_loss: 0.8438 - val_pose_output_loss: 0.8800 - val_emotion_output_loss: 0.9222 - val_gender_output_acc: 0.6543 - val_image_quality_output_acc: 0.5714 - val_age_output_acc: 0.3914 - val_weight_output_acc: 0.6533 - val_bag_output_acc: 0.5729 - val_footwear_output_acc: 0.6225 - val_pose_output_acc: 0.6126 - val_emotion_output_acc: 0.6994\n",
            "\n",
            "Epoch 00004: val_loss improved from 7.62876 to 7.48759, saving model to /content/gdrive/My Drive/EIP/Assignment5Dataset/saved_models/2019-12-29 14:28:42.929758/A5_model.004.h5\n",
            "Epoch 5/20\n",
            "\n",
            "Epoch 00005: LearningRateScheduler setting learning rate to 0.01.\n",
            "360/360 [==============================] - 104s 290ms/step - loss: 7.5244 - gender_output_loss: 0.6291 - image_quality_output_loss: 0.9800 - age_output_loss: 1.4101 - weight_output_loss: 0.9797 - bag_output_loss: 0.8995 - footwear_output_loss: 0.8655 - pose_output_loss: 0.8618 - emotion_output_loss: 0.8987 - gender_output_acc: 0.6415 - image_quality_output_acc: 0.5502 - age_output_acc: 0.3987 - weight_output_acc: 0.6328 - bag_output_acc: 0.5642 - footwear_output_acc: 0.6116 - pose_output_acc: 0.6298 - emotion_output_acc: 0.7137 - val_loss: 7.4511 - val_gender_output_loss: 0.6128 - val_image_quality_output_loss: 0.9636 - val_age_output_loss: 1.4101 - val_weight_output_loss: 0.9409 - val_bag_output_loss: 0.9017 - val_footwear_output_loss: 0.8360 - val_pose_output_loss: 0.8664 - val_emotion_output_loss: 0.9197 - val_gender_output_acc: 0.6677 - val_image_quality_output_acc: 0.5714 - val_age_output_acc: 0.3889 - val_weight_output_acc: 0.6533 - val_bag_output_acc: 0.5794 - val_footwear_output_acc: 0.6414 - val_pose_output_acc: 0.6101 - val_emotion_output_acc: 0.6994\n",
            "\n",
            "Epoch 00005: val_loss improved from 7.48759 to 7.45113, saving model to /content/gdrive/My Drive/EIP/Assignment5Dataset/saved_models/2019-12-29 14:28:42.929758/A5_model.005.h5\n",
            "Epoch 6/20\n",
            "\n",
            "Epoch 00006: LearningRateScheduler setting learning rate to 0.01.\n",
            "360/360 [==============================] - 104s 290ms/step - loss: 7.4508 - gender_output_loss: 0.6161 - image_quality_output_loss: 0.9782 - age_output_loss: 1.4049 - weight_output_loss: 0.9772 - bag_output_loss: 0.8953 - footwear_output_loss: 0.8612 - pose_output_loss: 0.8238 - emotion_output_loss: 0.8940 - gender_output_acc: 0.6560 - image_quality_output_acc: 0.5503 - age_output_acc: 0.4006 - weight_output_acc: 0.6323 - bag_output_acc: 0.5667 - footwear_output_acc: 0.6181 - pose_output_acc: 0.6527 - emotion_output_acc: 0.7137 - val_loss: 7.3749 - val_gender_output_loss: 0.5785 - val_image_quality_output_loss: 0.9581 - val_age_output_loss: 1.3984 - val_weight_output_loss: 0.9382 - val_bag_output_loss: 0.8917 - val_footwear_output_loss: 0.8926 - val_pose_output_loss: 0.8023 - val_emotion_output_loss: 0.9153 - val_gender_output_acc: 0.6959 - val_image_quality_output_acc: 0.5714 - val_age_output_acc: 0.3919 - val_weight_output_acc: 0.6533 - val_bag_output_acc: 0.5754 - val_footwear_output_acc: 0.5938 - val_pose_output_acc: 0.6597 - val_emotion_output_acc: 0.6994\n",
            "\n",
            "Epoch 00006: val_loss improved from 7.45113 to 7.37491, saving model to /content/gdrive/My Drive/EIP/Assignment5Dataset/saved_models/2019-12-29 14:28:42.929758/A5_model.006.h5\n",
            "Epoch 7/20\n",
            "\n",
            "Epoch 00007: LearningRateScheduler setting learning rate to 0.01.\n",
            "360/360 [==============================] - 104s 290ms/step - loss: 7.3526 - gender_output_loss: 0.5950 - image_quality_output_loss: 0.9760 - age_output_loss: 1.3986 - weight_output_loss: 0.9730 - bag_output_loss: 0.8891 - footwear_output_loss: 0.8396 - pose_output_loss: 0.7910 - emotion_output_loss: 0.8902 - gender_output_acc: 0.6744 - image_quality_output_acc: 0.5487 - age_output_acc: 0.4025 - weight_output_acc: 0.6326 - bag_output_acc: 0.5727 - footwear_output_acc: 0.6275 - pose_output_acc: 0.6676 - emotion_output_acc: 0.7139 - val_loss: 7.2173 - val_gender_output_loss: 0.5633 - val_image_quality_output_loss: 0.9530 - val_age_output_loss: 1.4069 - val_weight_output_loss: 0.9366 - val_bag_output_loss: 0.8924 - val_footwear_output_loss: 0.8046 - val_pose_output_loss: 0.7484 - val_emotion_output_loss: 0.9122 - val_gender_output_acc: 0.7098 - val_image_quality_output_acc: 0.5714 - val_age_output_acc: 0.3859 - val_weight_output_acc: 0.6562 - val_bag_output_acc: 0.5858 - val_footwear_output_acc: 0.6533 - val_pose_output_acc: 0.6900 - val_emotion_output_acc: 0.6994\n",
            "\n",
            "Epoch 00007: val_loss improved from 7.37491 to 7.21731, saving model to /content/gdrive/My Drive/EIP/Assignment5Dataset/saved_models/2019-12-29 14:28:42.929758/A5_model.007.h5\n",
            "Epoch 8/20\n",
            "\n",
            "Epoch 00008: LearningRateScheduler setting learning rate to 0.01.\n",
            "360/360 [==============================] - 105s 291ms/step - loss: 7.2723 - gender_output_loss: 0.5793 - image_quality_output_loss: 0.9760 - age_output_loss: 1.3960 - weight_output_loss: 0.9697 - bag_output_loss: 0.8814 - footwear_output_loss: 0.8291 - pose_output_loss: 0.7534 - emotion_output_loss: 0.8874 - gender_output_acc: 0.6882 - image_quality_output_acc: 0.5510 - age_output_acc: 0.3973 - weight_output_acc: 0.6319 - bag_output_acc: 0.5799 - footwear_output_acc: 0.6313 - pose_output_acc: 0.6850 - emotion_output_acc: 0.7137 - val_loss: 7.2462 - val_gender_output_loss: 0.5920 - val_image_quality_output_loss: 0.9619 - val_age_output_loss: 1.4127 - val_weight_output_loss: 0.9510 - val_bag_output_loss: 0.8874 - val_footwear_output_loss: 0.8115 - val_pose_output_loss: 0.7143 - val_emotion_output_loss: 0.9154 - val_gender_output_acc: 0.6736 - val_image_quality_output_acc: 0.5714 - val_age_output_acc: 0.3973 - val_weight_output_acc: 0.6533 - val_bag_output_acc: 0.5868 - val_footwear_output_acc: 0.6374 - val_pose_output_acc: 0.7039 - val_emotion_output_acc: 0.6994\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 7.21731\n",
            "Epoch 9/20\n",
            "\n",
            "Epoch 00009: LearningRateScheduler setting learning rate to 0.01.\n",
            "360/360 [==============================] - 105s 292ms/step - loss: 7.2358 - gender_output_loss: 0.5724 - image_quality_output_loss: 0.9734 - age_output_loss: 1.3935 - weight_output_loss: 0.9656 - bag_output_loss: 0.8772 - footwear_output_loss: 0.8262 - pose_output_loss: 0.7411 - emotion_output_loss: 0.8864 - gender_output_acc: 0.6958 - image_quality_output_acc: 0.5516 - age_output_acc: 0.4016 - weight_output_acc: 0.6322 - bag_output_acc: 0.5840 - footwear_output_acc: 0.6306 - pose_output_acc: 0.6907 - emotion_output_acc: 0.7136 - val_loss: 7.0795 - val_gender_output_loss: 0.5513 - val_image_quality_output_loss: 0.9558 - val_age_output_loss: 1.3997 - val_weight_output_loss: 0.9308 - val_bag_output_loss: 0.8763 - val_footwear_output_loss: 0.7918 - val_pose_output_loss: 0.6679 - val_emotion_output_loss: 0.9059 - val_gender_output_acc: 0.7098 - val_image_quality_output_acc: 0.5729 - val_age_output_acc: 0.3934 - val_weight_output_acc: 0.6587 - val_bag_output_acc: 0.5952 - val_footwear_output_acc: 0.6493 - val_pose_output_acc: 0.7222 - val_emotion_output_acc: 0.6994\n",
            "\n",
            "Epoch 00009: val_loss improved from 7.21731 to 7.07950, saving model to /content/gdrive/My Drive/EIP/Assignment5Dataset/saved_models/2019-12-29 14:28:42.929758/A5_model.009.h5\n",
            "Epoch 10/20\n",
            "\n",
            "Epoch 00010: LearningRateScheduler setting learning rate to 0.01.\n",
            "360/360 [==============================] - 104s 290ms/step - loss: 7.1686 - gender_output_loss: 0.5590 - image_quality_output_loss: 0.9714 - age_output_loss: 1.3880 - weight_output_loss: 0.9623 - bag_output_loss: 0.8734 - footwear_output_loss: 0.8145 - pose_output_loss: 0.7163 - emotion_output_loss: 0.8837 - gender_output_acc: 0.7061 - image_quality_output_acc: 0.5510 - age_output_acc: 0.4002 - weight_output_acc: 0.6340 - bag_output_acc: 0.5935 - footwear_output_acc: 0.6411 - pose_output_acc: 0.7001 - emotion_output_acc: 0.7137 - val_loss: 7.2376 - val_gender_output_loss: 0.5683 - val_image_quality_output_loss: 0.9631 - val_age_output_loss: 1.4121 - val_weight_output_loss: 0.9227 - val_bag_output_loss: 0.9051 - val_footwear_output_loss: 0.8272 - val_pose_output_loss: 0.7254 - val_emotion_output_loss: 0.9137 - val_gender_output_acc: 0.7044 - val_image_quality_output_acc: 0.5551 - val_age_output_acc: 0.3968 - val_weight_output_acc: 0.6617 - val_bag_output_acc: 0.5670 - val_footwear_output_acc: 0.6518 - val_pose_output_acc: 0.6905 - val_emotion_output_acc: 0.6999\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 7.07950\n",
            "Epoch 11/20\n",
            "\n",
            "Epoch 00011: LearningRateScheduler setting learning rate to 0.01.\n",
            "\n",
            "360/360 [==============================] - 106s 295ms/step - loss: 7.1155 - gender_output_loss: 0.5498 - image_quality_output_loss: 0.9731 - age_output_loss: 1.3840 - weight_output_loss: 0.9634 - bag_output_loss: 0.8696 - footwear_output_loss: 0.8038 - pose_output_loss: 0.6910 - emotion_output_loss: 0.8808 - gender_output_acc: 0.7157 - image_quality_output_acc: 0.5530 - age_output_acc: 0.4008 - weight_output_acc: 0.6319 - bag_output_acc: 0.5902 - footwear_output_acc: 0.6460 - pose_output_acc: 0.7094 - emotion_output_acc: 0.7134 - val_loss: 7.0488 - val_gender_output_loss: 0.5414 - val_image_quality_output_loss: 0.9549 - val_age_output_loss: 1.3994 - val_weight_output_loss: 0.9251 - val_bag_output_loss: 0.8762 - val_footwear_output_loss: 0.7799 - val_pose_output_loss: 0.6692 - val_emotion_output_loss: 0.9027 - val_gender_output_acc: 0.7272 - val_image_quality_output_acc: 0.5719 - val_age_output_acc: 0.3968 - val_weight_output_acc: 0.6533 - val_bag_output_acc: 0.5813 - val_footwear_output_acc: 0.6682 - val_pose_output_acc: 0.7257 - val_emotion_output_acc: 0.6994\n",
            "\n",
            "Epoch 00011: val_loss improved from 7.07950 to 7.04882, saving model to /content/gdrive/My Drive/EIP/Assignment5Dataset/saved_models/2019-12-29 14:28:42.929758/A5_model.011.h5\n",
            "\n",
            "Epoch 11/20\n",
            "Epoch 12/20\n",
            "\n",
            "Epoch 00012: LearningRateScheduler setting learning rate to 0.01.\n",
            "360/360 [==============================] - 105s 291ms/step - loss: 7.0566 - gender_output_loss: 0.5341 - image_quality_output_loss: 0.9704 - age_output_loss: 1.3819 - weight_output_loss: 0.9569 - bag_output_loss: 0.8631 - footwear_output_loss: 0.7960 - pose_output_loss: 0.6757 - emotion_output_loss: 0.8784 - gender_output_acc: 0.7273 - image_quality_output_acc: 0.5509 - age_output_acc: 0.4041 - weight_output_acc: 0.6333 - bag_output_acc: 0.6003 - footwear_output_acc: 0.6457 - pose_output_acc: 0.7172 - emotion_output_acc: 0.7135 - val_loss: 7.1265 - val_gender_output_loss: 0.5291 - val_image_quality_output_loss: 0.9497 - val_age_output_loss: 1.3962 - val_weight_output_loss: 0.9363 - val_bag_output_loss: 0.8879 - val_footwear_output_loss: 0.8139 - val_pose_output_loss: 0.6894 - val_emotion_output_loss: 0.9241 - val_gender_output_acc: 0.7321 - val_image_quality_output_acc: 0.5665 - val_age_output_acc: 0.4008 - val_weight_output_acc: 0.6503 - val_bag_output_acc: 0.5759 - val_footwear_output_acc: 0.6463 - val_pose_output_acc: 0.7277 - val_emotion_output_acc: 0.6984\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 7.04882\n",
            "Epoch 13/20\n",
            "\n",
            "Epoch 00013: LearningRateScheduler setting learning rate to 0.01.\n",
            "359/360 [============================>.] - ETA: 0s - loss: 6.9994 - gender_output_loss: 0.5235 - image_quality_output_loss: 0.9678 - age_output_loss: 1.3772 - weight_output_loss: 0.9561 - bag_output_loss: 0.8557 - footwear_output_loss: 0.7907 - pose_output_loss: 0.6519 - emotion_output_loss: 0.8765 - gender_output_acc: 0.7306 - image_quality_output_acc: 0.5524 - age_output_acc: 0.4009 - weight_output_acc: 0.6318 - bag_output_acc: 0.6050 - footwear_output_acc: 0.6499 - pose_output_acc: 0.7325 - emotion_output_acc: 0.7134\n",
            "360/360 [==============================] - 106s 295ms/step - loss: 6.9988 - gender_output_loss: 0.5235 - image_quality_output_loss: 0.9681 - age_output_loss: 1.3768 - weight_output_loss: 0.9562 - bag_output_loss: 0.8556 - footwear_output_loss: 0.7910 - pose_output_loss: 0.6518 - emotion_output_loss: 0.8758 - gender_output_acc: 0.7305 - image_quality_output_acc: 0.5521 - age_output_acc: 0.4011 - weight_output_acc: 0.6317 - bag_output_acc: 0.6053 - footwear_output_acc: 0.6499 - pose_output_acc: 0.7326 - emotion_output_acc: 0.7136 - val_loss: 7.0478 - val_gender_output_loss: 0.5246 - val_image_quality_output_loss: 0.9500 - val_age_output_loss: 1.3963 - val_weight_output_loss: 0.9248 - val_bag_output_loss: 0.8735 - val_footwear_output_loss: 0.8306 - val_pose_output_loss: 0.6470 - val_emotion_output_loss: 0.9008 - val_gender_output_acc: 0.7346 - val_image_quality_output_acc: 0.5739 - val_age_output_acc: 0.3988 - val_weight_output_acc: 0.6533 - val_bag_output_acc: 0.5848 - val_footwear_output_acc: 0.6295 - val_pose_output_acc: 0.7440 - val_emotion_output_acc: 0.6994\n",
            "\n",
            "Epoch 00013: val_loss improved from 7.04882 to 7.04776, saving model to /content/gdrive/My Drive/EIP/Assignment5Dataset/saved_models/2019-12-29 14:28:42.929758/A5_model.013.h5\n",
            "Epoch 14/20\n",
            "\n",
            "Epoch 00014: LearningRateScheduler setting learning rate to 0.01.\n",
            "360/360 [==============================] - 105s 290ms/step - loss: 6.9890 - gender_output_loss: 0.5154 - image_quality_output_loss: 0.9682 - age_output_loss: 1.3778 - weight_output_loss: 0.9510 - bag_output_loss: 0.8534 - footwear_output_loss: 0.7946 - pose_output_loss: 0.6524 - emotion_output_loss: 0.8761 - gender_output_acc: 0.7422 - image_quality_output_acc: 0.5508 - age_output_acc: 0.4030 - weight_output_acc: 0.6327 - bag_output_acc: 0.6062 - footwear_output_acc: 0.6451 - pose_output_acc: 0.7282 - emotion_output_acc: 0.7132 - val_loss: 7.0667 - val_gender_output_loss: 0.5199 - val_image_quality_output_loss: 0.9560 - val_age_output_loss: 1.4000 - val_weight_output_loss: 0.9345 - val_bag_output_loss: 0.8695 - val_footwear_output_loss: 0.8204 - val_pose_output_loss: 0.6649 - val_emotion_output_loss: 0.9016 - val_gender_output_acc: 0.7227 - val_image_quality_output_acc: 0.5734 - val_age_output_acc: 0.3924 - val_weight_output_acc: 0.6523 - val_bag_output_acc: 0.5977 - val_footwear_output_acc: 0.6339 - val_pose_output_acc: 0.7282 - val_emotion_output_acc: 0.6994\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 7.04776\n",
            "Epoch 15/20\n",
            "\n",
            "Epoch 00015: LearningRateScheduler setting learning rate to 0.01.\n",
            "360/360 [==============================] - 106s 295ms/step - loss: 6.9988 - gender_output_loss: 0.5235 - image_quality_output_loss: 0.9681 - age_output_loss: 1.3768 - weight_output_loss: 0.9562 - bag_output_loss: 0.8556 - footwear_output_loss: 0.7910 - pose_output_loss: 0.6518 - emotion_output_loss: 0.8758 - gender_output_acc: 0.7305 - image_quality_output_acc: 0.5521 - age_output_acc: 0.4011 - weight_output_acc: 0.6317 - bag_output_acc: 0.6053 - footwear_output_acc: 0.6499 - pose_output_acc: 0.7326 - emotion_output_acc: 0.7136 - val_loss: 7.0478 - val_gender_output_loss: 0.5246 - val_image_quality_output_loss: 0.9500 - val_age_output_loss: 1.3963 - val_weight_output_loss: 0.9248 - val_bag_output_loss: 0.8735 - val_footwear_output_loss: 0.8306 - val_pose_output_loss: 0.6470 - val_emotion_output_loss: 0.9008 - val_gender_output_acc: 0.7346 - val_image_quality_output_acc: 0.5739 - val_age_output_acc: 0.3988 - val_weight_output_acc: 0.6533 - val_bag_output_acc: 0.5848 - val_footwear_output_acc: 0.6295 - val_pose_output_acc: 0.7440 - val_emotion_output_acc: 0.6994\n",
            "360/360 [==============================] - 106s 294ms/step - loss: 6.9675 - gender_output_loss: 0.5234 - image_quality_output_loss: 0.9687 - age_output_loss: 1.3739 - weight_output_loss: 0.9494 - bag_output_loss: 0.8490 - footwear_output_loss: 0.7875 - pose_output_loss: 0.6424 - emotion_output_loss: 0.8733 - gender_output_acc: 0.7401 - image_quality_output_acc: 0.5530 - age_output_acc: 0.4032 - weight_output_acc: 0.6326 - bag_output_acc: 0.6109 - footwear_output_acc: 0.6518 - pose_output_acc: 0.7362 - emotion_output_acc: 0.7137 - val_loss: 7.1795 - val_gender_output_loss: 0.5492 - val_image_quality_output_loss: 0.9494 - val_age_output_loss: 1.4082 - val_weight_output_loss: 0.9300 - val_bag_output_loss: 0.8840 - val_footwear_output_loss: 0.8049 - val_pose_output_loss: 0.7488 - val_emotion_output_loss: 0.9051 - val_gender_output_acc: 0.7197 - val_image_quality_output_acc: 0.5704 - val_age_output_acc: 0.3943 - val_weight_output_acc: 0.6562 - val_bag_output_acc: 0.5774 - val_footwear_output_acc: 0.6543 - val_pose_output_acc: 0.7173 - val_emotion_output_acc: 0.6999\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 7.04776\n",
            "Epoch 16/20\n",
            "\n",
            "Epoch 00016: LearningRateScheduler setting learning rate to 0.01.\n",
            "360/360 [==============================] - 106s 293ms/step - loss: 7.0178 - gender_output_loss: 0.5262 - image_quality_output_loss: 0.9692 - age_output_loss: 1.3770 - weight_output_loss: 0.9488 - bag_output_loss: 0.8548 - footwear_output_loss: 0.8024 - pose_output_loss: 0.6657 - emotion_output_loss: 0.8737 - gender_output_acc: 0.7266 - image_quality_output_acc: 0.5559 - age_output_acc: 0.4038 - weight_output_acc: 0.6329 - bag_output_acc: 0.6026 - footwear_output_acc: 0.6468 - pose_output_acc: 0.7234 - emotion_output_acc: 0.7134 - val_loss: 7.0811 - val_gender_output_loss: 0.5222 - val_image_quality_output_loss: 0.9547 - val_age_output_loss: 1.3987 - val_weight_output_loss: 0.9345 - val_bag_output_loss: 0.8819 - val_footwear_output_loss: 0.7910 - val_pose_output_loss: 0.6915 - val_emotion_output_loss: 0.9065 - val_gender_output_acc: 0.7316 - val_image_quality_output_acc: 0.5655 - val_age_output_acc: 0.4003 - val_weight_output_acc: 0.6533 - val_bag_output_acc: 0.5813 - val_footwear_output_acc: 0.6572 - val_pose_output_acc: 0.7242 - val_emotion_output_acc: 0.6994\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 7.04776\n",
            "Epoch 17/20\n",
            "\n",
            "Epoch 00017: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 16/20\n",
            "360/360 [==============================] - 106s 295ms/step - loss: 6.8562 - gender_output_loss: 0.4934 - image_quality_output_loss: 0.9654 - age_output_loss: 1.3636 - weight_output_loss: 0.9409 - bag_output_loss: 0.8392 - footwear_output_loss: 0.7723 - pose_output_loss: 0.6127 - emotion_output_loss: 0.8687 - gender_output_acc: 0.7598 - image_quality_output_acc: 0.5563 - age_output_acc: 0.4067 - weight_output_acc: 0.6354 - bag_output_acc: 0.6155 - footwear_output_acc: 0.6618 - pose_output_acc: 0.7465 - emotion_output_acc: 0.7137 - val_loss: 7.0912 - val_gender_output_loss: 0.5274 - val_image_quality_output_loss: 0.9537 - val_age_output_loss: 1.4019 - val_weight_output_loss: 0.9301 - val_bag_output_loss: 0.8766 - val_footwear_output_loss: 0.8021 - val_pose_output_loss: 0.6840 - val_emotion_output_loss: 0.9155 - val_gender_output_acc: 0.7341 - val_image_quality_output_acc: 0.5670 - val_age_output_acc: 0.3983 - val_weight_output_acc: 0.6478 - val_bag_output_acc: 0.5918 - val_footwear_output_acc: 0.6379 - val_pose_output_acc: 0.7297 - val_emotion_output_acc: 0.6999\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 7.04776\n",
            "Epoch 18/20\n",
            "\n",
            "Epoch 00018: LearningRateScheduler setting learning rate to 0.01.\n",
            "360/360 [==============================] - 106s 295ms/step - loss: 6.8287 - gender_output_loss: 0.4911 - image_quality_output_loss: 0.9645 - age_output_loss: 1.3552 - weight_output_loss: 0.9339 - bag_output_loss: 0.8343 - footwear_output_loss: 0.7680 - pose_output_loss: 0.6159 - emotion_output_loss: 0.8658 - gender_output_acc: 0.7587 - image_quality_output_acc: 0.5550 - age_output_acc: 0.4069 - weight_output_acc: 0.6354 - bag_output_acc: 0.6243 - footwear_output_acc: 0.6576 - pose_output_acc: 0.7463 - emotion_output_acc: 0.7134 - val_loss: 7.0176 - val_gender_output_loss: 0.5279 - val_image_quality_output_loss: 0.9521 - val_age_output_loss: 1.4135 - val_weight_output_loss: 0.9492 - val_bag_output_loss: 0.8671 - val_footwear_output_loss: 0.7709 - val_pose_output_loss: 0.6291 - val_emotion_output_loss: 0.9078 - val_gender_output_acc: 0.7307 - val_image_quality_output_acc: 0.5635 - val_age_output_acc: 0.3914 - val_weight_output_acc: 0.6538 - val_bag_output_acc: 0.5962 - val_footwear_output_acc: 0.6652 - val_pose_output_acc: 0.7391 - val_emotion_output_acc: 0.6994\n",
            "\n",
            "Epoch 00018: val_loss improved from 7.04776 to 7.01764, saving model to /content/gdrive/My Drive/EIP/Assignment5Dataset/saved_models/2019-12-29 14:28:42.929758/A5_model.018.h5\n",
            "Epoch 19/20\n",
            "\n",
            "Epoch 00019: LearningRateScheduler setting learning rate to 0.01.\n",
            "360/360 [==============================] - 106s 296ms/step - loss: 6.8862 - gender_output_loss: 0.4965 - image_quality_output_loss: 0.9657 - age_output_loss: 1.3627 - weight_output_loss: 0.9415 - bag_output_loss: 0.8335 - footwear_output_loss: 0.7910 - pose_output_loss: 0.6294 - emotion_output_loss: 0.8659 - gender_output_acc: 0.7469 - image_quality_output_acc: 0.5564 - age_output_acc: 0.4108 - weight_output_acc: 0.6339 - bag_output_acc: 0.6243 - footwear_output_acc: 0.6474 - pose_output_acc: 0.7412 - emotion_output_acc: 0.7129 - val_loss: 7.1609 - val_gender_output_loss: 0.5512 - val_image_quality_output_loss: 0.9554 - val_age_output_loss: 1.3985 - val_weight_output_loss: 0.9348 - val_bag_output_loss: 0.8822 - val_footwear_output_loss: 0.8189 - val_pose_output_loss: 0.7037 - val_emotion_output_loss: 0.9162 - val_gender_output_acc: 0.7073 - val_image_quality_output_acc: 0.5694 - val_age_output_acc: 0.3904 - val_weight_output_acc: 0.6562 - val_bag_output_acc: 0.5848 - val_footwear_output_acc: 0.6473 - val_pose_output_acc: 0.7019 - val_emotion_output_acc: 0.6994\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 7.01764\n",
            "Epoch 20/20\n",
            "\n",
            "Epoch 00020: LearningRateScheduler setting learning rate to 0.01.\n",
            "359/360 [============================>.] - ETA: 0s - loss: 6.8167 - gender_output_loss: 0.4852 - image_quality_output_loss: 0.9653 - age_output_loss: 1.3532 - weight_output_loss: 0.9365 - bag_output_loss: 0.8306 - footwear_output_loss: 0.7711 - pose_output_loss: 0.6112 - emotion_output_loss: 0.8636 - gender_output_acc: 0.7611 - image_quality_output_acc: 0.5581 - age_output_acc: 0.4103 - weight_output_acc: 0.6368 - bag_output_acc: 0.6281 - footwear_output_acc: 0.6536 - pose_output_acc: 0.7506 - emotion_output_acc: 0.7134Epoch 20/20\n",
            "360/360 [==============================] - 106s 295ms/step - loss: 6.8152 - gender_output_loss: 0.4851 - image_quality_output_loss: 0.9652 - age_output_loss: 1.3529 - weight_output_loss: 0.9359 - bag_output_loss: 0.8307 - footwear_output_loss: 0.7706 - pose_output_loss: 0.6108 - emotion_output_loss: 0.8639 - gender_output_acc: 0.7610 - image_quality_output_acc: 0.5580 - age_output_acc: 0.4105 - weight_output_acc: 0.6372 - bag_output_acc: 0.6280 - footwear_output_acc: 0.6538 - pose_output_acc: 0.7509 - emotion_output_acc: 0.7133 - val_loss: 7.0832 - val_gender_output_loss: 0.5430 - val_image_quality_output_loss: 0.9587 - val_age_output_loss: 1.4044 - val_weight_output_loss: 0.9359 - val_bag_output_loss: 0.8690 - val_footwear_output_loss: 0.7725 - val_pose_output_loss: 0.6829 - val_emotion_output_loss: 0.9168 - val_gender_output_acc: 0.7148 - val_image_quality_output_acc: 0.5655 - val_age_output_acc: 0.3894 - val_weight_output_acc: 0.6558 - val_bag_output_acc: 0.6027 - val_footwear_output_acc: 0.6652 - val_pose_output_acc: 0.7183 - val_emotion_output_acc: 0.6994\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 7.01764\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XlfsyE_kzz20",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 341
        },
        "outputId": "0a739056-ec81-4a58-9af9-20fcb383d4ed"
      },
      "source": [
        "results = model.evaluate_generator(valid_gen, verbose=1)\n",
        "dict(zip(model.metrics_names, results))"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "63/63 [==============================] - 6s 96ms/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'age_output_acc': 0.38938492063492064,\n",
              " 'age_output_loss': 1.4044006684469799,\n",
              " 'bag_output_acc': 0.6026785714285714,\n",
              " 'bag_output_loss': 0.8689707782533433,\n",
              " 'emotion_output_acc': 0.6994047619047619,\n",
              " 'emotion_output_loss': 0.9168068511145455,\n",
              " 'footwear_output_acc': 0.6651785714285714,\n",
              " 'footwear_output_loss': 0.7724524244429574,\n",
              " 'gender_output_acc': 0.714781746031746,\n",
              " 'gender_output_loss': 0.5429886186879779,\n",
              " 'image_quality_output_acc': 0.5654761904761905,\n",
              " 'image_quality_output_loss': 0.9587341811921861,\n",
              " 'loss': 7.08320264967661,\n",
              " 'pose_output_acc': 0.7182539682539683,\n",
              " 'pose_output_loss': 0.6829349233044518,\n",
              " 'weight_output_acc': 0.6557539682539683,\n",
              " 'weight_output_loss': 0.9359142345095438}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ew6cxrLz-HN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "1bca30ad-f83f-47ef-ce5b-0f4d2e78930d"
      },
      "source": [
        "import os\n",
        "import datetime\n",
        "# Prepare model model saving directory.\n",
        "# save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
        "t = datetime.datetime.today()\n",
        "print(str(datetime.datetime.today()))\n",
        "save_dir = \"/content/gdrive/My Drive/EIP/Assignment5Dataset/saved_models/\"+str(t)\n",
        "model_name = 'A5_model.{epoch:03d}.h5'\n",
        "if not os.path.isdir(save_dir):\n",
        "    os.makedirs(save_dir)\n",
        "filepath = os.path.join(save_dir, model_name)\n",
        "filepath"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-12-29 15:51:15.882370\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/gdrive/My Drive/EIP/Assignment5Dataset/saved_models/2019-12-29 15:51:15.882316/A5_model.{epoch:03d}.h5'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ybF1UbtqznXo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = keras.models.load_model(\"/content/gdrive/My Drive/EIP/Assignment5Dataset/saved_models/2019-12-29 14:28:42.929758/A5_model.018.h5\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yA6yTFB30CPs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def lr_schedule(epoch):\n",
        "    \"\"\"Learning Rate Schedule\n",
        "\n",
        "    Learning rate is scheduled to be reduced after 80, 120, 160, 180 epochs.\n",
        "    Called automatically every epoch as part of callbacks during training.\n",
        "\n",
        "    # Arguments\n",
        "        epoch (int): The number of epochs\n",
        "\n",
        "    # Returns\n",
        "        lr (float32): learning rate\n",
        "    \"\"\"\n",
        "    lr = 1e-3\n",
        "    if epoch > 17:\n",
        "        lr *= 0.5e-3\n",
        "    elif epoch > 13:\n",
        "        lr *= 1e-3\n",
        "    elif epoch > 9:\n",
        "        lr *= 1e-2\n",
        "    elif epoch > 5:\n",
        "        lr *= 1e-1\n",
        "    print('Learning rate: ', lr)\n",
        "    return lr\n",
        "\n",
        "    # lr = 0.5\n",
        "    # if epoch > 45:\n",
        "    #     lr *= 0.1\n",
        "    # elif epoch > 40:\n",
        "    #     lr *= 0.2\n",
        "    # elif epoch > 30:\n",
        "    #     lr *= 0.3\n",
        "    # elif epoch > 20:\n",
        "    #     lr *= 0.4\n",
        "    # print('Learning rate: ', lr)\n",
        "    # return lr"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P3j4Z4Zz0FMV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "losses = {\n",
        " \t\"gender_output\": \"binary_crossentropy\",\n",
        " \t\"image_quality_output\": \"categorical_crossentropy\",\n",
        " \t\"age_output\": \"categorical_crossentropy\",\n",
        " \t\"weight_output\": \"categorical_crossentropy\",\n",
        "  \"bag_output\": \"categorical_crossentropy\",\n",
        " \t\"footwear_output\": \"categorical_crossentropy\",\n",
        " \t\"pose_output\": \"categorical_crossentropy\",\n",
        "  \"emotion_output\": \"categorical_crossentropy\",\n",
        " }\n",
        "\n",
        "loss_weights = {\"gender_output\": 1.0, \"image_quality_output\": 1.0, \"age_output\": 1.0,\"weight_output\": 1.0,\"bag_output\": 1.0,\"footwear_output\": 1.0,\"pose_output\": 1.0,\"emotion_output\": 1.0}\n",
        "\n",
        "opt = SGD(lr=0.001, momentum=0.9)\n",
        "model.compile(\n",
        "    optimizer=opt,\n",
        "    loss=losses, \n",
        "    loss_weights=loss_weights, \n",
        "    metrics=[\"accuracy\"]\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DwNGop8K0Ljd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Prepare callbacks for model saving and for learning rate adjustment.\n",
        "checkpoint = ModelCheckpoint(filepath=filepath,\n",
        "                             monitor='val_loss',\n",
        "                             verbose=1,\n",
        "                             save_best_only=True)\n",
        "\n",
        "\n",
        "es = EarlyStopping(monitor='val_loss', verbose=1, patience=5, mode='min')\n",
        "\n",
        "lr_scheduler = LearningRateScheduler(lr_schedule, verbose=1)\n",
        "\n",
        "# lr_reducer = ReduceLROnPlateau(factor=np.sqrt(0.1),\n",
        "#                                cooldown=0,\n",
        "#                                patience=5,\n",
        "#                                min_lr=0.5e-6)\n",
        "\n",
        "# callbacks = [checkpoint, lr_reducer, lr_scheduler]\n",
        "callbacks = [checkpoint,es,lr_scheduler]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ygRABkj0O3-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 433
        },
        "outputId": "d2c39066-d178-4309-e7f1-372ca00487eb"
      },
      "source": [
        "model_output = model.fit_generator(\n",
        "    generator=train_gen,\n",
        "    validation_data=valid_gen,\n",
        "    use_multiprocessing=True,\n",
        "    workers=6, \n",
        "    epochs=20,\n",
        "    verbose=1,\n",
        "    callbacks=callbacks\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "Learning rate:  0.001\n",
            "\n",
            "Epoch 00001: LearningRateScheduler setting learning rate to 0.001.\n",
            "359/360 [============================>.] - ETA: 0s - loss: 6.2994 - gender_output_loss: 0.3827 - image_quality_output_loss: 0.9497 - age_output_loss: 1.3013 - weight_output_loss: 0.8976 - bag_output_loss: 0.7721 - footwear_output_loss: 0.6949 - pose_output_loss: 0.4574 - emotion_output_loss: 0.8436 - gender_output_acc: 0.8268 - image_quality_output_acc: 0.5615 - age_output_acc: 0.4293 - weight_output_acc: 0.6463 - bag_output_acc: 0.6639 - footwear_output_acc: 0.6952 - pose_output_acc: 0.8172 - emotion_output_acc: 0.7133Epoch 1/20\n",
            "360/360 [==============================] - 131s 363ms/step - loss: 6.2973 - gender_output_loss: 0.3824 - image_quality_output_loss: 0.9491 - age_output_loss: 1.3011 - weight_output_loss: 0.8974 - bag_output_loss: 0.7721 - footwear_output_loss: 0.6953 - pose_output_loss: 0.4569 - emotion_output_loss: 0.8430 - gender_output_acc: 0.8270 - image_quality_output_acc: 0.5622 - age_output_acc: 0.4295 - weight_output_acc: 0.6464 - bag_output_acc: 0.6641 - footwear_output_acc: 0.6951 - pose_output_acc: 0.8174 - emotion_output_acc: 0.7134 - val_loss: 7.0720 - val_gender_output_loss: 0.5273 - val_image_quality_output_loss: 0.9518 - val_age_output_loss: 1.4103 - val_weight_output_loss: 0.9346 - val_bag_output_loss: 0.8720 - val_footwear_output_loss: 0.7792 - val_pose_output_loss: 0.6734 - val_emotion_output_loss: 0.9233 - val_gender_output_acc: 0.7515 - val_image_quality_output_acc: 0.5625 - val_age_output_acc: 0.3909 - val_weight_output_acc: 0.6488 - val_bag_output_acc: 0.6096 - val_footwear_output_acc: 0.6662 - val_pose_output_acc: 0.7485 - val_emotion_output_acc: 0.6984\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 7.07201, saving model to /content/gdrive/My Drive/EIP/Assignment5Dataset/saved_models/2019-12-29 15:51:15.882316/A5_model.001.h5\n",
            "Epoch 2/20\n",
            "Learning rate:  0.001\n",
            "\n",
            "Epoch 00002: LearningRateScheduler setting learning rate to 0.001.\n",
            "360/360 [==============================] - 131s 363ms/step - loss: 6.2973 - gender_output_loss: 0.3824 - image_quality_output_loss: 0.9491 - age_output_loss: 1.3011 - weight_output_loss: 0.8974 - bag_output_loss: 0.7721 - footwear_output_loss: 0.6953 - pose_output_loss: 0.4569 - emotion_output_loss: 0.8430 - gender_output_acc: 0.8270 - image_quality_output_acc: 0.5622 - age_output_acc: 0.4295 - weight_output_acc: 0.6464 - bag_output_acc: 0.6641 - footwear_output_acc: 0.6951 - pose_output_acc: 0.8174 - emotion_output_acc: 0.7134 - val_loss: 7.0720 - val_gender_output_loss: 0.5273 - val_image_quality_output_loss: 0.9518 - val_age_output_loss: 1.4103 - val_weight_output_loss: 0.9346 - val_bag_output_loss: 0.8720 - val_footwear_output_loss: 0.7792 - val_pose_output_loss: 0.6734 - val_emotion_output_loss: 0.9233 - val_gender_output_acc: 0.7515 - val_image_quality_output_acc: 0.5625 - val_age_output_acc: 0.3909 - val_weight_output_acc: 0.6488 - val_bag_output_acc: 0.6096 - val_footwear_output_acc: 0.6662 - val_pose_output_acc: 0.7485 - val_emotion_output_acc: 0.6984\n",
            "\n",
            "360/360 [==============================] - 106s 294ms/step - loss: 6.0644 - gender_output_loss: 0.3336 - image_quality_output_loss: 0.9373 - age_output_loss: 1.2763 - weight_output_loss: 0.8702 - bag_output_loss: 0.7471 - footwear_output_loss: 0.6674 - pose_output_loss: 0.4061 - emotion_output_loss: 0.8264 - gender_output_acc: 0.8537 - image_quality_output_acc: 0.5662 - age_output_acc: 0.4368 - weight_output_acc: 0.6517 - bag_output_acc: 0.6786 - footwear_output_acc: 0.7061 - pose_output_acc: 0.8415 - emotion_output_acc: 0.7145 - val_loss: 7.1665 - val_gender_output_loss: 0.5497 - val_image_quality_output_loss: 0.9541 - val_age_output_loss: 1.4176 - val_weight_output_loss: 0.9396 - val_bag_output_loss: 0.8899 - val_footwear_output_loss: 0.7843 - val_pose_output_loss: 0.7045 - val_emotion_output_loss: 0.9267 - val_gender_output_acc: 0.7629 - val_image_quality_output_acc: 0.5620 - val_age_output_acc: 0.3894 - val_weight_output_acc: 0.6488 - val_bag_output_acc: 0.5982 - val_footwear_output_acc: 0.6647 - val_pose_output_acc: 0.7460 - val_emotion_output_acc: 0.6969\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 7.07201\n",
            "Epoch 3/20\n",
            "Learning rate:  0.001\n",
            "\n",
            "Epoch 00003: LearningRateScheduler setting learning rate to 0.001.\n",
            "140/360 [==========>...................] - ETA: 1:01 - loss: 5.8026 - gender_output_loss: 0.2831 - image_quality_output_loss: 0.9311 - age_output_loss: 1.2326 - weight_output_loss: 0.8463 - bag_output_loss: 0.7114 - footwear_output_loss: 0.6546 - pose_output_loss: 0.3421 - emotion_output_loss: 0.8014 - gender_output_acc: 0.8768 - image_quality_output_acc: 0.5672 - age_output_acc: 0.4554 - weight_output_acc: 0.6589 - bag_output_acc: 0.6964 - footwear_output_acc: 0.7114 - pose_output_acc: 0.8714 - emotion_output_acc: 0.7223"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M5WxGQAX68xC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 341
        },
        "outputId": "199b092f-8984-41fa-f8f6-7b5f1501bc2f"
      },
      "source": [
        "results = model.evaluate_generator(valid_gen, verbose=1)\n",
        "dict(zip(model.metrics_names, results))"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "63/63 [==============================] - 6s 91ms/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'age_output_acc': 0.3898809523809524,\n",
              " 'age_output_loss': 1.3811695935234192,\n",
              " 'bag_output_acc': 0.6428571428571429,\n",
              " 'bag_output_loss': 0.8746008920291114,\n",
              " 'emotion_output_acc': 0.6979166666666666,\n",
              " 'emotion_output_loss': 0.9184170687009418,\n",
              " 'footwear_output_acc': 0.6815476190476191,\n",
              " 'footwear_output_loss': 0.8244341413180033,\n",
              " 'gender_output_acc': 0.8487103174603174,\n",
              " 'gender_output_loss': 0.4635749041206307,\n",
              " 'image_quality_output_acc': 0.5629960317460317,\n",
              " 'image_quality_output_loss': 0.9240117006831698,\n",
              " 'loss': 7.007764097244021,\n",
              " 'pose_output_acc': 0.7872023809523809,\n",
              " 'pose_output_loss': 0.6848407482344007,\n",
              " 'weight_output_acc': 0.6329365079365079,\n",
              " 'weight_output_loss': 0.9367150721095857}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CmoGhaGD7V0a",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "64a52894-200f-4074-ced5-129008b5cc3e"
      },
      "source": [
        "import os\n",
        "import datetime\n",
        "# Prepare model model saving directory.\n",
        "# save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
        "t = datetime.datetime.today()\n",
        "print(str(datetime.datetime.today()))\n",
        "save_dir = \"/content/gdrive/My Drive/EIP/Assignment5Dataset/saved_models/\"+str(t)\n",
        "model_name = 'A5_model.{epoch:03d}.h5'\n",
        "if not os.path.isdir(save_dir):\n",
        "    os.makedirs(save_dir)\n",
        "filepath = os.path.join(save_dir, model_name)\n",
        "filepath"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-12-29 13:53:11.803828\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/gdrive/My Drive/EIP/Assignment5Dataset/saved_models/2019-12-29 13:53:11.803745/A5_model.{epoch:03d}.h5'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dh0H-eG-7W_I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = keras.models.load_model(\"/content/gdrive/My Drive/EIP/Assignment5Dataset/saved_models/2019-12-29 13:19:39.007613/A5_model.005.h5\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-MMjh1AN7XJp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def lr_schedule(epoch):\n",
        "    \"\"\"Learning Rate Schedule\n",
        "\n",
        "    Learning rate is scheduled to be reduced after 80, 120, 160, 180 epochs.\n",
        "    Called automatically every epoch as part of callbacks during training.\n",
        "\n",
        "    # Arguments\n",
        "        epoch (int): The number of epochs\n",
        "\n",
        "    # Returns\n",
        "        lr (float32): learning rate\n",
        "    \"\"\"\n",
        "    lr = 1e-3\n",
        "    if epoch > 17:\n",
        "        lr *= 0.5e-3\n",
        "    elif epoch > 13:\n",
        "        lr *= 1e-3\n",
        "    elif epoch > 9:\n",
        "        lr *= 1e-2\n",
        "    elif epoch > 5:\n",
        "        lr *= 1e-1\n",
        "    print('Learning rate: ', lr)\n",
        "    return lr\n",
        "\n",
        "    # lr = 0.5\n",
        "    # if epoch > 45:\n",
        "    #     lr *= 0.1\n",
        "    # elif epoch > 40:\n",
        "    #     lr *= 0.2\n",
        "    # elif epoch > 30:\n",
        "    #     lr *= 0.3\n",
        "    # elif epoch > 20:\n",
        "    #     lr *= 0.4\n",
        "    # print('Learning rate: ', lr)\n",
        "    # return lr"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UMi2zBQm7XQg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "losses = {\n",
        " \t\"gender_output\": \"binary_crossentropy\",\n",
        " \t\"image_quality_output\": \"categorical_crossentropy\",\n",
        " \t\"age_output\": \"categorical_crossentropy\",\n",
        " \t\"weight_output\": \"categorical_crossentropy\",\n",
        "  \"bag_output\": \"categorical_crossentropy\",\n",
        " \t\"footwear_output\": \"categorical_crossentropy\",\n",
        " \t\"pose_output\": \"categorical_crossentropy\",\n",
        "  \"emotion_output\": \"categorical_crossentropy\",\n",
        " }\n",
        "\n",
        "loss_weights = {\"gender_output\": 1.0, \"image_quality_output\": 1.0, \"age_output\": 1.0,\"weight_output\": 1.0,\"bag_output\": 1.0,\"footwear_output\": 1.0,\"pose_output\": 1.0,\"emotion_output\": 1.0}\n",
        "\n",
        "opt = SGD(lr=0.001, momentum=0.9)\n",
        "model.compile(\n",
        "    optimizer=opt,\n",
        "    loss=losses, \n",
        "    loss_weights=loss_weights, \n",
        "    metrics=[\"accuracy\"]\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8HPxckeh7foE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Prepare callbacks for model saving and for learning rate adjustment.\n",
        "checkpoint = ModelCheckpoint(filepath=filepath,\n",
        "                             monitor='val_loss',\n",
        "                             verbose=1,\n",
        "                             save_best_only=True)\n",
        "\n",
        "\n",
        "es = EarlyStopping(monitor='val_loss', verbose=1, patience=5, mode='min')\n",
        "\n",
        "lr_scheduler = LearningRateScheduler(lr_schedule, verbose=1)\n",
        "\n",
        "# lr_reducer = ReduceLROnPlateau(factor=np.sqrt(0.1),\n",
        "#                                cooldown=0,\n",
        "#                                patience=5,\n",
        "#                                min_lr=0.5e-6)\n",
        "\n",
        "# callbacks = [checkpoint, lr_reducer, lr_scheduler]\n",
        "callbacks = [checkpoint,lr_scheduler]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RBP8zbnt7fwM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3add503a-ad32-417c-f04c-cdd5289f781f"
      },
      "source": [
        "model_output = model.fit_generator(\n",
        "    generator=train_gen,\n",
        "    validation_data=valid_gen,\n",
        "    use_multiprocessing=True,\n",
        "    workers=6, \n",
        "    epochs=20,\n",
        "    verbose=1,\n",
        "    callbacks=callbacks\n",
        ")"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "Learning rate:  0.001\n",
            "\n",
            "Epoch 00001: LearningRateScheduler setting learning rate to 0.001.\n",
            "359/360 [============================>.] - ETA: 0s - loss: 6.1993 - gender_output_loss: 0.3019 - image_quality_output_loss: 0.9195 - age_output_loss: 1.3237 - weight_output_loss: 0.9154 - bag_output_loss: 0.7722 - footwear_output_loss: 0.7011 - pose_output_loss: 0.4265 - emotion_output_loss: 0.8389 - gender_output_acc: 0.8683 - image_quality_output_acc: 0.5606 - age_output_acc: 0.4168 - weight_output_acc: 0.6381 - bag_output_acc: 0.6707 - footwear_output_acc: 0.6896 - pose_output_acc: 0.8290 - emotion_output_acc: 0.7129\n",
            "360/360 [==============================] - 120s 332ms/step - loss: 6.2010 - gender_output_loss: 0.3021 - image_quality_output_loss: 0.9192 - age_output_loss: 1.3245 - weight_output_loss: 0.9158 - bag_output_loss: 0.7723 - footwear_output_loss: 0.7017 - pose_output_loss: 0.4265 - emotion_output_loss: 0.8390 - gender_output_acc: 0.8681 - image_quality_output_acc: 0.5609 - age_output_acc: 0.4163 - weight_output_acc: 0.6379 - bag_output_acc: 0.6707 - footwear_output_acc: 0.6890 - pose_output_acc: 0.8289 - emotion_output_acc: 0.7128 - val_loss: 6.6514 - val_gender_output_loss: 0.3892 - val_image_quality_output_loss: 0.9176 - val_age_output_loss: 1.3769 - val_weight_output_loss: 0.9124 - val_bag_output_loss: 0.8233 - val_footwear_output_loss: 0.7666 - val_pose_output_loss: 0.5736 - val_emotion_output_loss: 0.8918 - val_gender_output_acc: 0.8358 - val_image_quality_output_acc: 0.5724 - val_age_output_acc: 0.4033 - val_weight_output_acc: 0.6533 - val_bag_output_acc: 0.6414 - val_footwear_output_acc: 0.6766 - val_pose_output_acc: 0.7832 - val_emotion_output_acc: 0.6999\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 6.65143, saving model to /content/gdrive/My Drive/EIP/Assignment5Dataset/saved_models/2019-12-29 13:53:11.803745/A5_model.001.h5\n",
            "Epoch 2/20\n",
            "Learning rate:  0.001\n",
            "\n",
            "Epoch 00002: LearningRateScheduler setting learning rate to 0.001.\n",
            "360/360 [==============================] - 105s 292ms/step - loss: 6.0831 - gender_output_loss: 0.2795 - image_quality_output_loss: 0.9076 - age_output_loss: 1.3137 - weight_output_loss: 0.9072 - bag_output_loss: 0.7545 - footwear_output_loss: 0.6865 - pose_output_loss: 0.3992 - emotion_output_loss: 0.8348 - gender_output_acc: 0.8799 - image_quality_output_acc: 0.5700 - age_output_acc: 0.4214 - weight_output_acc: 0.6444 - bag_output_acc: 0.6767 - footwear_output_acc: 0.6987 - pose_output_acc: 0.8427 - emotion_output_acc: 0.7156 - val_loss: 6.7536 - val_gender_output_loss: 0.3997 - val_image_quality_output_loss: 0.9157 - val_age_output_loss: 1.3710 - val_weight_output_loss: 0.9177 - val_bag_output_loss: 0.8360 - val_footwear_output_loss: 0.7713 - val_pose_output_loss: 0.6485 - val_emotion_output_loss: 0.8936 - val_gender_output_acc: 0.8338 - val_image_quality_output_acc: 0.5719 - val_age_output_acc: 0.4033 - val_weight_output_acc: 0.6438 - val_bag_output_acc: 0.6334 - val_footwear_output_acc: 0.6682 - val_pose_output_acc: 0.7768 - val_emotion_output_acc: 0.6989\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 6.65143\n",
            "Epoch 3/20\n",
            "Learning rate:  0.001\n",
            "\n",
            "Epoch 00003: LearningRateScheduler setting learning rate to 0.001.\n",
            "360/360 [==============================] - 105s 293ms/step - loss: 5.9601 - gender_output_loss: 0.2633 - image_quality_output_loss: 0.8996 - age_output_loss: 1.2983 - weight_output_loss: 0.8855 - bag_output_loss: 0.7364 - footwear_output_loss: 0.6793 - pose_output_loss: 0.3727 - emotion_output_loss: 0.8250 - gender_output_acc: 0.8882 - image_quality_output_acc: 0.5760 - age_output_acc: 0.4293 - weight_output_acc: 0.6455 - bag_output_acc: 0.6879 - footwear_output_acc: 0.7034 - pose_output_acc: 0.8575 - emotion_output_acc: 0.7155 - val_loss: 6.7500 - val_gender_output_loss: 0.3933 - val_image_quality_output_loss: 0.9196 - val_age_output_loss: 1.3673 - val_weight_output_loss: 0.9196 - val_bag_output_loss: 0.8773 - val_footwear_output_loss: 0.7634 - val_pose_output_loss: 0.6051 - val_emotion_output_loss: 0.9044 - val_gender_output_acc: 0.8358 - val_image_quality_output_acc: 0.5724 - val_age_output_acc: 0.4062 - val_weight_output_acc: 0.6513 - val_bag_output_acc: 0.6171 - val_footwear_output_acc: 0.6796 - val_pose_output_acc: 0.7812 - val_emotion_output_acc: 0.6994\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 6.65143\n",
            "360/360 [==============================] - 105s 292ms/step - loss: 6.0831 - gender_output_loss: 0.2795 - image_quality_output_loss: 0.9076 - age_output_loss: 1.3137 - weight_output_loss: 0.9072 - bag_output_loss: 0.7545 - footwear_output_loss: 0.6865 - pose_output_loss: 0.3992 - emotion_output_loss: 0.8348 - gender_output_acc: 0.8799 - image_quality_output_acc: 0.5700 - age_output_acc: 0.4214 - weight_output_acc: 0.6444 - bag_output_acc: 0.6767 - footwear_output_acc: 0.6987 - pose_output_acc: 0.8427 - emotion_output_acc: 0.7156 - val_loss: 6.7536 - val_gender_output_loss: 0.3997 - val_image_quality_output_loss: 0.9157 - val_age_output_loss: 1.3710 - val_weight_output_loss: 0.9177 - val_bag_output_loss: 0.8360 - val_footwear_output_loss: 0.7713 - val_pose_output_loss: 0.6485 - val_emotion_output_loss: 0.8936 - val_gender_output_acc: 0.8338 - val_image_quality_output_acc: 0.5719 - val_age_output_acc: 0.4033 - val_weight_output_acc: 0.6438 - val_bag_output_acc: 0.6334 - val_footwear_output_acc: 0.6682 - val_pose_output_acc: 0.7768 - val_emotion_output_acc: 0.6989\n",
            "Epoch 4/20\n",
            "Learning rate:  0.001\n",
            "\n",
            "Epoch 00004: LearningRateScheduler setting learning rate to 0.001.\n",
            "360/360 [==============================] - 106s 295ms/step - loss: 5.8374 - gender_output_loss: 0.2438 - image_quality_output_loss: 0.8909 - age_output_loss: 1.2823 - weight_output_loss: 0.8773 - bag_output_loss: 0.7180 - footwear_output_loss: 0.6553 - pose_output_loss: 0.3535 - emotion_output_loss: 0.8163 - gender_output_acc: 0.9022 - image_quality_output_acc: 0.5780 - age_output_acc: 0.4365 - weight_output_acc: 0.6496 - bag_output_acc: 0.6951 - footwear_output_acc: 0.7130 - pose_output_acc: 0.8602 - emotion_output_acc: 0.7148 - val_loss: 6.7520 - val_gender_output_loss: 0.4133 - val_image_quality_output_loss: 0.9193 - val_age_output_loss: 1.3688 - val_weight_output_loss: 0.9097 - val_bag_output_loss: 0.8370 - val_footwear_output_loss: 0.7977 - val_pose_output_loss: 0.6021 - val_emotion_output_loss: 0.9041 - val_gender_output_acc: 0.8348 - val_image_quality_output_acc: 0.5744 - val_age_output_acc: 0.4058 - val_weight_output_acc: 0.6463 - val_bag_output_acc: 0.6374 - val_footwear_output_acc: 0.6657 - val_pose_output_acc: 0.7783 - val_emotion_output_acc: 0.6984\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 6.65143\n",
            "Epoch 5/20\n",
            "Learning rate:  0.001\n",
            "\n",
            "Epoch 00005: LearningRateScheduler setting learning rate to 0.001.\n",
            "360/360 [==============================] - 106s 293ms/step - loss: 5.6777 - gender_output_loss: 0.2227 - image_quality_output_loss: 0.8758 - age_output_loss: 1.2583 - weight_output_loss: 0.8562 - bag_output_loss: 0.6998 - footwear_output_loss: 0.6360 - pose_output_loss: 0.3244 - emotion_output_loss: 0.8044 - gender_output_acc: 0.9107 - image_quality_output_acc: 0.5849 - age_output_acc: 0.4440 - weight_output_acc: 0.6580 - bag_output_acc: 0.7058 - footwear_output_acc: 0.7197 - pose_output_acc: 0.8753 - emotion_output_acc: 0.7172 - val_loss: 6.9433 - val_gender_output_loss: 0.4387 - val_image_quality_output_loss: 0.9291 - val_age_output_loss: 1.3628 - val_weight_output_loss: 0.9286 - val_bag_output_loss: 0.8679 - val_footwear_output_loss: 0.8354 - val_pose_output_loss: 0.6743 - val_emotion_output_loss: 0.9065 - val_gender_output_acc: 0.8383 - val_image_quality_output_acc: 0.5739 - val_age_output_acc: 0.4082 - val_weight_output_acc: 0.6424 - val_bag_output_acc: 0.6379 - val_footwear_output_acc: 0.6622 - val_pose_output_acc: 0.7664 - val_emotion_output_acc: 0.6959\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 6.65143\n",
            "Epoch 6/20\n",
            "Learning rate:  0.001\n",
            "\n",
            "Epoch 00006: LearningRateScheduler setting learning rate to 0.001.\n",
            "360/360 [==============================] - 106s 294ms/step - loss: 5.5256 - gender_output_loss: 0.2107 - image_quality_output_loss: 0.8662 - age_output_loss: 1.2396 - weight_output_loss: 0.8308 - bag_output_loss: 0.6703 - footwear_output_loss: 0.6153 - pose_output_loss: 0.3048 - emotion_output_loss: 0.7880 - gender_output_acc: 0.9107 - image_quality_output_acc: 0.5891 - age_output_acc: 0.4563 - weight_output_acc: 0.6674 - bag_output_acc: 0.7158 - footwear_output_acc: 0.7352 - pose_output_acc: 0.8811 - emotion_output_acc: 0.7191 - val_loss: 7.0703 - val_gender_output_loss: 0.4971 - val_image_quality_output_loss: 0.9818 - val_age_output_loss: 1.3915 - val_weight_output_loss: 0.9337 - val_bag_output_loss: 0.8806 - val_footwear_output_loss: 0.8097 - val_pose_output_loss: 0.6632 - val_emotion_output_loss: 0.9127 - val_gender_output_acc: 0.8130 - val_image_quality_output_acc: 0.5590 - val_age_output_acc: 0.4018 - val_weight_output_acc: 0.6453 - val_bag_output_acc: 0.6394 - val_footwear_output_acc: 0.6572 - val_pose_output_acc: 0.7778 - val_emotion_output_acc: 0.6939\n",
            "\n",
            "Epoch 00006: LearningRateScheduler setting learning rate to 0.001.\n",
            "Learning rate:  0.001\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 6.65143\n",
            "Epoch 7/20\n",
            "Learning rate:  0.0001\n",
            "\n",
            "Epoch 00007: LearningRateScheduler setting learning rate to 0.0001.\n",
            "360/360 [==============================] - 105s 293ms/step - loss: 4.8851 - gender_output_loss: 0.1437 - image_quality_output_loss: 0.7938 - age_output_loss: 1.1404 - weight_output_loss: 0.7557 - bag_output_loss: 0.5858 - footwear_output_loss: 0.5306 - pose_output_loss: 0.1984 - emotion_output_loss: 0.7365 - gender_output_acc: 0.9431 - image_quality_output_acc: 0.6241 - age_output_acc: 0.4959 - weight_output_acc: 0.6967 - bag_output_acc: 0.7579 - footwear_output_acc: 0.7685 - pose_output_acc: 0.9255 - emotion_output_acc: 0.7300 - val_loss: 7.6060 - val_gender_output_loss: 0.5232 - val_image_quality_output_loss: 0.9734 - val_age_output_loss: 1.4623 - val_weight_output_loss: 0.9975 - val_bag_output_loss: 0.9624 - val_footwear_output_loss: 0.9088 - val_pose_output_loss: 0.8175 - val_emotion_output_loss: 0.9609 - val_gender_output_acc: 0.8477 - val_image_quality_output_acc: 0.5441 - val_age_output_acc: 0.3938 - val_weight_output_acc: 0.6300 - val_bag_output_acc: 0.6364 - val_footwear_output_acc: 0.6682 - val_pose_output_acc: 0.7857 - val_emotion_output_acc: 0.6840\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 6.65143\n",
            "Learning rate:  0.0001\n",
            "\n",
            "Epoch 00007: LearningRateScheduler setting learning rate to 0.0001.\n",
            "\n",
            "Epoch 8/20\n",
            "Learning rate:  0.0001\n",
            "\n",
            "Epoch 00008: LearningRateScheduler setting learning rate to 0.0001.\n",
            "360/360 [==============================] - 105s 292ms/step - loss: 4.4859 - gender_output_loss: 0.1081 - image_quality_output_loss: 0.7531 - age_output_loss: 1.0789 - weight_output_loss: 0.7003 - bag_output_loss: 0.5135 - footwear_output_loss: 0.4731 - pose_output_loss: 0.1503 - emotion_output_loss: 0.7086 - gender_output_acc: 0.9589 - image_quality_output_acc: 0.6481 - age_output_acc: 0.5229 - weight_output_acc: 0.7133 - bag_output_acc: 0.7964 - footwear_output_acc: 0.7953 - pose_output_acc: 0.9457 - emotion_output_acc: 0.7351 - val_loss: 7.8329 - val_gender_output_loss: 0.5551 - val_image_quality_output_loss: 0.9803 - val_age_output_loss: 1.4836 - val_weight_output_loss: 1.0103 - val_bag_output_loss: 0.9939 - val_footwear_output_loss: 0.9573 - val_pose_output_loss: 0.8689 - val_emotion_output_loss: 0.9836 - val_gender_output_acc: 0.8438 - val_image_quality_output_acc: 0.5417 - val_age_output_acc: 0.3715 - val_weight_output_acc: 0.6250 - val_bag_output_acc: 0.6369 - val_footwear_output_acc: 0.6582 - val_pose_output_acc: 0.7946 - val_emotion_output_acc: 0.6865\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 6.65143\n",
            "Epoch 9/20\n",
            "Learning rate: \n",
            "\n",
            "Epoch 00008: LearningRateScheduler setting learning rate to 0.0001.\n",
            " Learning rate:  0.0001\n",
            "0.0001\n",
            "\n",
            "Epoch 00009: LearningRateScheduler setting learning rate to 0.0001.\n",
            "235/360 [==================>...........] - ETA: 34s - loss: 4.2233 - gender_output_loss: 0.0915 - image_quality_output_loss: 0.7143 - age_output_loss: 1.0360 - weight_output_loss: 0.6706 - bag_output_loss: 0.4645 - footwear_output_loss: 0.4489 - pose_output_loss: 0.1259 - emotion_output_loss: 0.6716 - gender_output_acc: 0.9653 - image_quality_output_acc: 0.6713 - age_output_acc: 0.5460 - weight_output_acc: 0.7277 - bag_output_acc: 0.8161 - footwear_output_acc: 0.8036 - pose_output_acc: 0.9549 - emotion_output_acc: 0.7508"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Process ForkPoolWorker-1197:\n",
            "Process ForkPoolWorker-1195:\n",
            "Process ForkPoolWorker-1190:\n",
            "Process ForkPoolWorker-1193:\n",
            "Process ForkPoolWorker-1194:\n",
            "Process ForkPoolWorker-1192:\n",
            "Process ForkPoolWorker-1191:\n",
            "Process ForkPoolWorker-1189:\n",
            "Process ForkPoolWorker-1199:\n",
            "Process ForkPoolWorker-1198:\n",
            "Traceback (most recent call last):\n",
            "Process ForkPoolWorker-1200:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
            "    task = get()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
            "    with self._rlock:\n",
            "Traceback (most recent call last):\n",
            "Process ForkPoolWorker-1196:\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
            "    return self._semlock.__enter__()\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "KeyboardInterrupt\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
            "    task = get()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
            "    task = get()\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
            "    task = get()\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
            "    task = get()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
            "    with self._rlock:\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 335, in get\n",
            "    res = self._reader.recv_bytes()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
            "    task = get()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
            "    with self._rlock:\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
            "    task = get()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
            "    with self._rlock:\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
            "    task = get()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n",
            "    buf = self._recv_bytes(maxlength)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
            "    with self._rlock:\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
            "    return self._semlock.__enter__()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
            "    return self._semlock.__enter__()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
            "    return self._semlock.__enter__()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
            "    buf = self._recv(4)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
            "    with self._rlock:\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
            "    task = get()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
            "    return self._semlock.__enter__()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 335, in get\n",
            "    res = self._reader.recv_bytes()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
            "    task = get()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
            "    task = get()\n",
            "KeyboardInterrupt\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n",
            "    buf = self._recv_bytes(maxlength)\n",
            "KeyboardInterrupt\n",
            "KeyboardInterrupt\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
            "    with self._rlock:\n",
            "KeyboardInterrupt\n",
            "  File \"/usr/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
            "    return self._semlock.__enter__()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
            "    with self._rlock:\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
            "    chunk = read(handle, remaining)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
            "    with self._rlock:\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n",
            "    result = (True, func(*args, **kwds))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 9/20\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  File \"/usr/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
            "    return self._semlock.__enter__()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras/utils/data_utils.py\", line 406, in get_index\n",
            "    return _SHARED_SEQUENCES[uid][i]\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
            "    buf = self._recv(4)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
            "    chunk = read(handle, remaining)\n",
            "KeyboardInterrupt\n",
            "KeyboardInterrupt\n",
            "KeyboardInterrupt\n",
            "  File \"/usr/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
            "    return self._semlock.__enter__()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
            "    return self._semlock.__enter__()\n",
            "  File \"<ipython-input-7-cdff63c65644>\", line 41, in __getitem__\n",
            "    \"age_output\": items[_age_cols_].values,\n",
            "KeyboardInterrupt\n",
            "KeyboardInterrupt\n",
            "KeyboardInterrupt\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py\", line 3001, in __getitem__\n",
            "    indexer = self.loc._convert_to_indexer(key, axis=1, raise_missing=True)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py\", line 1285, in _convert_to_indexer\n",
            "    return self._get_listlike_indexer(obj, axis, **kwargs)[1]\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py\", line 1087, in _get_listlike_indexer\n",
            "    keyarr = ax.reindex(keyarr)[0]\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pandas/core/indexes/base.py\", line 3408, in reindex\n",
            "    target, method=method, limit=limit, tolerance=tolerance\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pandas/core/indexes/base.py\", line 2970, in get_indexer\n",
            "    pself, ptarget = self._maybe_promote(target)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pandas/core/indexes/base.py\", line 4823, in _maybe_promote\n",
            "    from pandas import DatetimeIndex\n",
            "  File \"<frozen importlib._bootstrap>\", line 997, in _handle_fromlist\n",
            "KeyboardInterrupt\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-49-e54003beaa11>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m )\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1656\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1657\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1658\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1659\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1660\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    213\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[1;32m    214\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m                                             class_weight=class_weight)\n\u001b[0m\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1447\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1448\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1449\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1450\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1451\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2977\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2978\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2979\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2980\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2981\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2935\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2936\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2937\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2938\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2939\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1470\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1471\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1472\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1473\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1474\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1bZOo7ak7f3H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "results = model.evaluate_generator(valid_gen, verbose=1)\n",
        "dict(zip(model.metrics_names, results))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}